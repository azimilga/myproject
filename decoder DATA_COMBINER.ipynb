{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBcU-1qrWrEj"
   },
   "source": [
    "**WRITER'S IDENTITY**\n",
    "- AZIMIL GANI ALAM\n",
    "- Ph.D Student\n",
    "- Dept. Energy & Process Tech - NTNU\n",
    "\n",
    "\n",
    "**NOTE :**\n",
    "- This Python File runs in GoogleColab\n",
    "- This work uses **All Data**\n",
    "- we want to combine all data results from CSV models become process-ready for data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mut4f2nkycvj"
   },
   "source": [
    "# **Library Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fjhmov-c_PhT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import statistics\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import psychrolib\n",
    "from pythermalcomfort.models import pmv_ppd\n",
    "from pythermalcomfort.utilities import v_relative, clo_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Dt8m9KN3linE"
   },
   "outputs": [],
   "source": [
    "def datetime_indexer(df):\n",
    "    df['datetime'] = pd.to_datetime((df['date'] + ' ' + df['time']), format='%Y-%m-%d %H:%M')\n",
    "    df.set_index(['datetime'], inplace = True)\n",
    "\n",
    "def datetime_indexer_noindex(df):\n",
    "    df['datetime'] = pd.to_datetime((df['date'] + ' ' + df['time']), format='%Y-%m-%d %H:%M')\n",
    "    #df.set_index(['datetime'], inplace = True)\n",
    "\n",
    "def datedayhour_creator(df) :\n",
    "  df.reset_index( inplace = True)\n",
    "  datechange = pd.to_datetime(df['datetime'])\n",
    "  df['date'] = datechange.dt.strftime('%Y-%m-%d')\n",
    "  df['day']  = datechange.dt.dayofweek\n",
    "  df['hour'] = datechange.dt.hour\n",
    "\n",
    "def datedayhour_creator_noindex(df) :\n",
    "  datechange = pd.to_datetime(df['datetime'])\n",
    "  df['date'] = datechange.dt.strftime('%Y-%m-%d')\n",
    "  df['day']  = datechange.dt.dayofweek\n",
    "  df['hour'] = datechange.dt.hour\n",
    "\n",
    "def date_selector(df,start_date,end_date) :\n",
    "  df.reset_index( inplace = True)\n",
    "  datechange = pd.to_datetime(df['datetime'])\n",
    "  df['date'] = datechange.dt.strftime('%Y-%m-%d')\n",
    "  df         = df[(df['date']>=start_date) & (df['date']<=end_date)]\n",
    "  df         = df.drop('date', axis=1).set_index('datetime')\n",
    "  \n",
    "\n",
    "def pmv_ppd_generator(df):\n",
    "  tdb = df['temp_a'].values\n",
    "  tr = tdb\n",
    "  vel = 0.05\n",
    "  rh = df['rh_a'].values\n",
    "  met = 1.2\n",
    "  clo = 0.67\n",
    "\n",
    "  v_rel = v_relative(vel, met)\n",
    "  clo_d = clo_dynamic(clo, met)\n",
    "  results = pmv_ppd( tdb=tdb, tr=tr ,vr=v_rel, rh= rh, met=met, clo=clo_d,standard= 'iso', units= 'SI')\n",
    "  df['pmv'] = results['pmv']\n",
    "  df['ppd'] = results['ppd']/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantitative_niluapp(DF3):\n",
    "  # DF3['feel_air0'] = 0\n",
    "  # DF3['feel_air1'] = 0\n",
    "  # DF3['feel_air2'] = 0\n",
    "  # DF3['feel_air3'] = 0\n",
    "  # DF3['feel_air4'] = 0\n",
    "  # DF3['feel_temp0'] = 0\n",
    "  # DF3['feel_temp1'] = 0\n",
    "  # DF3['feel_temp2'] = 0\n",
    "  # DF3['feel_temp3'] = 0\n",
    "  # DF3['feel_temp4'] = 0\n",
    "  # DF3['feel_bright0'] = 0\n",
    "  # DF3['feel_bright1'] = 0\n",
    "  # DF3['feel_bright2'] = 0\n",
    "  # DF3['feel_bright3'] = 0\n",
    "  # DF3['feel_bright4'] = 0\n",
    "  # DF3['feel_health0'] = 0\n",
    "  # DF3['feel_health1'] = 0\n",
    "  # DF3['feel_health2'] = 0\n",
    "  # DF3['feel_health3'] = 0\n",
    "  # DF3['feel_health4'] = 0\n",
    "  # DF3['feel_noise0'] = 0\n",
    "  # DF3['feel_noise1'] = 0\n",
    "  # DF3['feel_noise2'] = 0\n",
    "  # DF3['feel_noise3'] = 0\n",
    "  # DF3['feel_noise4'] = 0\n",
    "  DF3.loc[DF3['feel_air'] == 0, 'feel_air0'] = 1\n",
    "  DF3.loc[DF3['feel_air'] == 1, 'feel_air1'] = 1\n",
    "  DF3.loc[DF3['feel_air'] == 2, 'feel_air2'] = 1\n",
    "  DF3.loc[DF3['feel_air'] == 3, 'feel_air3'] = 1\n",
    "  DF3.loc[DF3['feel_air'] == 4, 'feel_air4'] = 1\n",
    "  DF3.loc[DF3['feel_temp'] == 0, 'feel_temp0'] = 1\n",
    "  DF3.loc[DF3['feel_temp'] == 1, 'feel_temp1'] = 1\n",
    "  DF3.loc[DF3['feel_temp'] == 2, 'feel_temp2'] = 1\n",
    "  DF3.loc[DF3['feel_temp'] == 3, 'feel_temp3'] = 1\n",
    "  DF3.loc[DF3['feel_temp'] == 4, 'feel_temp4'] = 1\n",
    "  DF3.loc[DF3['feel_bright'] == 0, 'feel_bright0'] = 1\n",
    "  DF3.loc[DF3['feel_bright'] == 1, 'feel_bright1'] = 1\n",
    "  DF3.loc[DF3['feel_bright'] == 2, 'feel_bright2'] = 1\n",
    "  DF3.loc[DF3['feel_bright'] == 3, 'feel_bright3'] = 1\n",
    "  DF3.loc[DF3['feel_bright'] == 4, 'feel_bright4'] = 1\n",
    "  DF3.loc[DF3['feel_health'] == 0, 'feel_health0'] = 1\n",
    "  DF3.loc[DF3['feel_health'] == 1, 'feel_health1'] = 1\n",
    "  DF3.loc[DF3['feel_health'] == 2, 'feel_health2'] = 1\n",
    "  DF3.loc[DF3['feel_health'] == 3, 'feel_health3'] = 1\n",
    "  DF3.loc[DF3['feel_health'] == 4, 'feel_health4'] = 1\n",
    "  DF3.loc[DF3['feel_noise'] == 0, 'feel_noise0'] = 1\n",
    "  DF3.loc[DF3['feel_noise'] == 1, 'feel_noise1'] = 1\n",
    "  DF3.loc[DF3['feel_noise'] == 2, 'feel_noise2'] = 1\n",
    "  DF3.loc[DF3['feel_noise'] == 3, 'feel_noise3'] = 1\n",
    "  DF3.loc[DF3['feel_noise'] == 4, 'feel_noise4'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k8zB7YMWQH2"
   },
   "source": [
    "# Default  Names & Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder Naming\n",
    "folder_raw     = 'data-raw'\n",
    "folder_ready   = 'data-ready'\n",
    "folder_year    = 'all years'\n",
    "folder_project = 'Project'\n",
    "folder_export  = 'export'\n",
    "folder_is      = 'niluapp'\n",
    "data_for1      = 'combined panel'\n",
    "data_for2      = 'combined longitudinal'\n",
    "data_for3      = 'calibration'\n",
    "platform_directory = 'G:/My Drive/Colab Notebooks'   #Computer NILU\n",
    "# platform_directory = 'C:/Users/azimilga/My Drive/Colab Notebooks'   #laptop NTNU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "AYylrInmSktj"
   },
   "outputs": [],
   "source": [
    "#filtering rows data regarding to desired specific time\n",
    "start_time = '06:00'\n",
    "end_time   = '13:00'\n",
    "start_date = '023-03-06'\n",
    "end_date   =  '2023-05-30'\n",
    "\n",
    "spec_date = pd.to_datetime('today').strftime(\"%Y-%m-%d\")\n",
    "#name_date = pd.to_datetime('today').strftime(\"%y%m%d\")\n",
    "date_format_show =  '%Y-%m-%d'\n",
    "time_format_show =  '%H:%M'\n",
    "start_day  = 0\n",
    "end_day    = 4\n",
    "start_hour = 6\n",
    "end_hour   = 13\n",
    "\n",
    "# tell resample data\n",
    "time_series_resample = '10min'     #for Processor 1\n",
    "survey_resample      = '10min'     #for Processor 2\n",
    "desample_option      = '10min'\n",
    "minutely_resample    = '1min'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aBd2ro3cy7vq"
   },
   "outputs": [],
   "source": [
    "col_header = ['title', 'sessionid', 'date', 'time' ]\n",
    "response_counting = ['responses']\n",
    "is_col_main  =  ['feel_air','feel_temp','feel_health','feel_bright','feel_noise']\n",
    "is_col_main1  = ['feel_air1','feel_temp1','feel_health1','feel_bright1'] #,'feel_noise1'\n",
    "is_col_main2  = ['feel_air2','feel_temp2','feel_health2','feel_bright2'] #,'feel_noise2'\n",
    "is_col_main3  = ['feel_air3','feel_temp3','feel_health3','feel_bright3'] #,'feel_noise3'\n",
    "is_col_main4  = ['feel_air4','feel_temp4','feel_health4','feel_bright4'] #,'feel_noise4'\n",
    "\n",
    "is_main_air    = ['feel_air1', 'feel_air2', 'feel_air3', 'feel_air4']\n",
    "is_main_temp   = ['feel_temp1', 'feel_temp2', 'feel_temp3', 'feel_temp4']\n",
    "is_main_bright = ['feel_bright1', 'feel_bright2', 'feel_bright3', 'feel_bright4']\n",
    "is_main_health = ['feel_health1', 'feel_health2', 'feel_health3', 'feel_health4']\n",
    "is_main_noise  = ['feel_noise1', 'feel_noise2', 'feel_noise3', 'feel_noise4']\n",
    "\n",
    "is_col_air   = ['air_smell','air_heavy','air_dry','air_dust','air_electshock']\n",
    "is_col_temp  = ['temp_coldhot','temp_draw','temp_coldfloor','temp_heatsun','temp_heater' ]\n",
    "is_col_bright = ['bright_sun','bright_lamp_hi','bright_lamp_low']\n",
    "is_col_health  = ['health_head','health_cough','health_tired','health_dryskin']\n",
    "is_col_multi  = ['feel_air','feel_temp','feel_health','feel_bright','feel_noise','temp_coldhot']\n",
    "is_col_subquestion = is_col_air+ is_col_temp + is_col_bright + is_col_health\n",
    "pd_col       = ['pd_air', 'pd_temp', 'pd_health', 'pd_bright', 'pd_noise']\n",
    "columns_formation   = col_header + is_col_main + is_col_air + is_col_temp + is_col_bright + is_col_health\n",
    "\n",
    "school_col_name   = 'school_id'\n",
    "room_col_name     = 'room_id'\n",
    "date_name         = 'date'\n",
    "time_name         = 'time'\n",
    "manuf_col_name    = 'manuf_id'\n",
    "instr_col_name    = 'instr_id'\n",
    "serial_col_name   = 'serial_id'\n",
    "\n",
    "#Give a name for new column in purpose to count how many response / hour or response / day\n",
    "response_counting = 'responses'\n",
    "\n",
    "#columns_formation = ['title', 'sessionid', 'date', 'time' , 'Room Air', 'Room Temperature', 'Room Lighting', 'Health Feeling', 'Room Noice', 'BAD SMELL' , 'BAD HEAVY/AIR', 'DRY AIR', 'DUST AND DIRT', 'ELECTRIC SHOCK', '(-COLD) OR HOT', 'DRAWING COLD AIR', 'COLD ON THE FLOOR', 'MUCH HEAT FROM SUNSHINE', 'MUCH HEAT FROM FURNACES', 'LIGHT FROM THE SUN', 'BRIGHT LIGHT CEILING LAMPS', 'WEAK LIGHT CEILING LAMPS', 'HEADACHE', 'COUGH/SHORENESS', 'TIRED/UNCONCENTRATED', 'DRY EYES/HANDS']\n",
    "\n",
    "\n",
    "word_indoor   = 'Indoor '\n",
    "word_outdoor  = 'Outdoor ' \n",
    "temp_v_label  = word_indoor+ 'Temp. by Vent system (*C)'\n",
    "co2_v_label   = word_indoor+ 'CO2 level by Vent System (ppm)'\n",
    "vent_v_label  = word_indoor+ 'Supply Air (CMH)'\n",
    "temp_a_label  = word_indoor+ 'Temp. by add. sensors (*C)'\n",
    "rh_a_label    = word_indoor+ 'Relative Humidity (%)'\n",
    "co2_a_label   = word_indoor+ 'CO2 level by add. sensors (ppm)'\n",
    "voc_a_label   = word_indoor+ 'VOC contaminant level (ppb)'\n",
    "bright_a_label= word_indoor+ 'Luminous Intensity (%)'\n",
    "sound_a_label = word_indoor+ 'Sound Pressure Intensity (Pa)'\n",
    "pm25_a_label  = word_indoor+ 'PM2.5 density (ug/m3)'\n",
    "pm1_a_label   = word_indoor+ 'PM1.0 density (ug/m3)'\n",
    "rdn_a_label   = word_indoor+ 'Radon Level (Bq/m3)'\n",
    "press_a_label = word_indoor+ 'Air pressure (hPa)'\n",
    "swc_f_label   = word_indoor+ 'Floor Heater -On/Off'\n",
    "swc_r_label   = word_indoor+ 'Baseboard Heater-On/Off'\n",
    "temp_f_label  = word_indoor+ 'Floor Temperature (*C)'\n",
    "temp_o_label  = word_outdoor+ 'Temperature (*C)'\n",
    "rh_o_label    = word_outdoor+ 'Relative Humidiy (%)'\n",
    "winds_o_label = word_outdoor+ 'Wind Speed (m/s)'\n",
    "sun_o_label   = word_outdoor+ 'Mean Global Radiation (W/m2)'\n",
    "pm25_o_label  = word_outdoor+ 'PM2.5 density (ug/m3)'\n",
    "pm10_o_label  = word_outdoor+ 'PM10 density (ug/m3)'\n",
    "\n",
    "directory_path = platform_directory +  '/' + folder_project + '/' + folder_ready   +  '/'   +  folder_year  + '/'\n",
    "co2_name          = 'co2_a'\n",
    "temp_data_name    = 'temp_a'\n",
    "rel_hum_name      = 'rh_a'\n",
    "voc_name          = 'voc_a'\n",
    "bright_name        = 'bright_a'\n",
    "sound_name        = 'sound_a'\n",
    "pm25_name         = 'pm2.5_a'\n",
    "pm1_name          = 'pm1_a'\n",
    "radon_name        = 'rdn_a'\n",
    "press_name        = 'press_a'\n",
    "\n",
    "\n",
    "airthingsPlus = [ co2_name,temp_data_name, rel_hum_name, press_name, 'enth_a', 'hura_a', voc_name,bright_name, radon_name, 'pmv', 'ppd']\n",
    "airthingsPro  = [ co2_name,temp_data_name, rel_hum_name, press_name, 'enth_a', 'hura_a', voc_name,bright_name, sound_name, pm1_name, pm25_name,  'pmv', 'ppd']\n",
    "weather_volda = [ 'temp_o', 'rh_o', 'winds_o', 'rain_o','enth_o', 'hura_o', 'press_o']\n",
    "weather_oslo  = [ 'temp_o', 'rh_o', 'winds_o', 'rain_o','enth_o', 'hura_o', 'press_o', 'sun_o', 'pm2.5_o', 'pm10_o']\n",
    "weather_extra = [ 'nox_o', 'no2_o', 'no_o']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crMceS7W6IAG"
   },
   "source": [
    "Time Zone Changing (GMT to OSLO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CLrjONZL6N9w"
   },
   "outputs": [],
   "source": [
    " #convert time format to datetime  : time_zone_converter (df, 'Zone Origin', 'Zone Destination')\n",
    "def time_zone_converter(df, zone_origin, zone_destination):\n",
    "  #df['datetime'] = pd.to_datetime((df['date'] + ' ' + df['time']), format='%Y-%m-%d %H:%M:%S')\n",
    "  #df['DateTime'] = datechange.dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "  df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "  df['date'] = df['datetime'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "  # Define the timezones (GMT and Europe/Oslo)\n",
    "  origin_timezone = pytz.timezone(zone_origin)\n",
    "  destination_timezone = pytz.timezone(zone_destination)\n",
    "\n",
    "    # Adjust the datetime to the Oslo timezone and account for daylight saving time\n",
    "  df['adjusted datetime'] = df['datetime'].dt.tz_localize(origin_timezone, ambiguous='NaT', nonexistent='NaT').dt.tz_convert(destination_timezone)\n",
    "\n",
    "  # Extract the adjusted time\n",
    "  df['date'] = df['adjusted datetime'].dt.strftime('%Y-%m-%d')\n",
    "  df['time'] = df['adjusted datetime'].dt.strftime('%H:%M')\n",
    "  df['hour'] = df['adjusted datetime'].dt.hour\n",
    "  df['datetime'] = df['adjusted datetime'] #.dt.strftime('%Y-%m-%d %H:%M')\n",
    "  df.drop(['adjusted datetime'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_creator (df,start_date,end_date,season_ranges):\n",
    "    # Convert the date column to datetime\n",
    "    df['date2'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Initialize the 'season' column with default value 0\n",
    "    df['season'] = 0\n",
    "\n",
    "    # Assign the season codes based on the date ranges\n",
    "    for start_date, end_date, season_code in season_ranges:\n",
    "        start = pd.to_datetime(start_date)\n",
    "        end = pd.to_datetime(end_date)\n",
    "        df.loc[(df['date2'] >= start) & (df['date2'] <= end), 'season'] = season_code\n",
    "    df.drop(['date2'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WX9DV2WOmc51"
   },
   "source": [
    "# Bratteberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "id": "CWKnGChdptnG"
   },
   "outputs": [],
   "source": [
    "school_name = 'bratteberg'\n",
    "class1 = 141\n",
    "class2 = 142\n",
    "class3 = 210\n",
    "class4 = 'Øst brakker'\n",
    "occupants_replace =  { 'Øst brakker': 25 , 141:28 , 142:14,  210: 18 }  # for bratteberg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "pR5p_DTnX_X1"
   },
   "outputs": [],
   "source": [
    "values_temp2   =  ['swc_f', 'swc_r', 'temp_v', 'temp_f', 'temp_s', 'press_a', 'temp_a', 'rh_a','enth_a','temp_o', 'rh_o','pmv', 'ppd']\n",
    "values_air2    =  ['temp_a', 'rh_a', 'co2_a', 'voc_a', 'bright_a', 'rdn_a', 'enth_a','pmv', 'ppd', 'temp_o', 'rh_o', 'winds_o']\n",
    "values_health2 =  ['temp_f','temp_a', 'rh_a', 'co2_a', 'voc_a', 'bright_a','hura_a','pm1_a', 'pm2.5_a','temp_o', 'rh_o','winds_o']\n",
    "values_bright2 =  ['temp_a', 'bright_a', 'temp_o', 'rh_o','winds_o', 'rain_o', 'enth_o']\n",
    "values_noise2  =  ['swc_f', 'swc_r', 'temp_v', 'temp_f','co2_a', 'voc_a', 'sound_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_year    = '2023'\n",
    "\n",
    "\n",
    "start_date = '2023-03-06'\n",
    "end_date  =  '2023-04-25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_year    = '2024'\n",
    "\n",
    "start_date = '2024-02-26'\n",
    "end_date  =  '2024-03-08'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_year    = 'all years'\n",
    "\n",
    "start_date = '2023-03-06'\n",
    "end_date  =  '2024-03-08'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQc0T5Blmi74"
   },
   "source": [
    "## Klasserom 141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "id": "owL8mZnAmXVA"
   },
   "outputs": [],
   "source": [
    "class_dir    = '141'\n",
    "school_name  = 'bratteberg'\n",
    "# start_hour = 6\n",
    "# end_hour   = 11\n",
    "\n",
    "another_interpolate_col = airthingsPlus + weather_volda\n",
    "sd_fill_col =  ['swc_f', 'swc_r', 'temp_s'] # [] #\n",
    "sd_meaninterpolate_col = ['temp_v','temp_f']   #   []    #\n",
    "\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbG9dH5Hfhhk"
   },
   "source": [
    "## Klasserom 142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "q9772r1mfhhl"
   },
   "outputs": [],
   "source": [
    "class_dir = '142'\n",
    "school_name = 'bratteberg'\n",
    "# start_hour = 6\n",
    "# end_hour   = 11\n",
    "\n",
    "another_interpolate_col = airthingsPlus + weather_volda\n",
    "sd_fill_col =  ['swc_f', 'swc_r', 'temp_s'] # [] #\n",
    "sd_meaninterpolate_col = ['temp_v','temp_f']   #   []    #\n",
    "\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hY_w7yIfhoH"
   },
   "source": [
    "## Klasserom 210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "AUKBD7xEfhoH"
   },
   "outputs": [],
   "source": [
    "class_dir   = '210'\n",
    "school_name = 'bratteberg'\n",
    "# start_hour = 6\n",
    "# end_hour   = 11\n",
    "\n",
    "another_interpolate_col = airthingsPlus + weather_volda\n",
    "\n",
    "sd_fill_col            = ['swc_r', 'temp_s'] # \n",
    "sd_meaninterpolate_col = ['temp_v']          #   \n",
    "\n",
    "interpolate_col = sd_fill_col  + sd_meaninterpolate_col + another_interpolate_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6zUOlu5zMkL"
   },
   "source": [
    "## Klasserom Øst Brakker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "id": "aTJUqmA8zMkT"
   },
   "outputs": [],
   "source": [
    "class_dir   = 'Øst brakker'\n",
    "school_name = 'bratteberg'\n",
    "# start_hour = 6\n",
    "# end_hour   = 11\n",
    "\n",
    "another_interpolate_col = airthingsPro + weather_volda\n",
    "\n",
    "sd_fill_col             = [] #[ 'swc_r']\n",
    "sd_meaninterpolate_col  = [] #['temp_v']\n",
    "\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwekofXHQvdi"
   },
   "source": [
    "# Øyra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "id": "soeMAAcJpzR9"
   },
   "outputs": [],
   "source": [
    "class1 = 302\n",
    "class2 = 303\n",
    "class3 = 339\n",
    "occupants_replace =  { 302:25   , 303:26    , 339:27 } # for øyra\n",
    "\n",
    "sd_fill_col            =   [ 'temp_s']\n",
    "sd_meaninterpolate_col =   ['temp_v','vent_v']   #    \n",
    "\n",
    "interpolate_col =  sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_year = '2023'\n",
    "start_date  = '2023-03-13'\n",
    "end_date    = '2023-04-18'\n",
    "\n",
    "#  class result from 302, and 303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_year = '2024'\n",
    "start_date  = '2024-04-03'\n",
    "end_date    = '2024-04-17'\n",
    "\n",
    "#  class result from 303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_year = 'all years'\n",
    "start_date  = '2023-03-13'\n",
    "end_date    = '2024-04-18'\n",
    "\n",
    "#  class result from 303"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB0-pjj2Qvdo"
   },
   "source": [
    "## Klasserom 302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "id": "gRIpXjpTQvdp"
   },
   "outputs": [],
   "source": [
    "class_dir = '302'\n",
    "school_name = 'øyra'\n",
    "# start_hour = 6\n",
    "# end_hour   = 12\n",
    "\n",
    "another_interpolate_col = airthingsPro + weather_volda\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOUsloymQvdp"
   },
   "source": [
    "## Klasserom 303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "id": "-6EBBp4fQvdp"
   },
   "outputs": [],
   "source": [
    "class_dir = '303'\n",
    "school_name = 'øyra'\n",
    "# start_hour = 6\n",
    "# end_hour   = 12\n",
    "\n",
    "another_interpolate_col = airthingsPro + weather_volda\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54FnWTOOQvdp"
   },
   "source": [
    "## Klasserom 339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dir = '339'\n",
    "school_name = 'øyra'\n",
    "# start_hour = 6\n",
    "# end_hour   = 12\n",
    "\n",
    "another_interpolate_col = airthingsPro + weather_volda\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brannfjell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_ranges = [\n",
    "    ('2023-03-03', '2023-04-25', 1),\n",
    "    ('2023-11-10', '2023-11-30', 2),\n",
    "    ('2024-02-02', '2024-03-10', 3),\n",
    "    ('2024-04-07', '2024-04-30', 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1 = 11\n",
    "class2 = 23\n",
    "class3 = 27\n",
    "class4 = 31\n",
    "occupants_replace =  { 11:30   , 23:26    , 27:31 } # for brannfjell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 1\n",
    "# REMEMBER :  class1 : 23, 27\n",
    "\n",
    "folder_year    = '2023'\n",
    "\n",
    "sd_fill_col =   [ 'temp_s']  # []     #\n",
    "sd_meaninterpolate_col =  ['temp_v', 'co2_v']  #     []   #\n",
    "another_interpolate_col = airthingsPlus + weather_oslo\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col\n",
    "\n",
    "start_date = '2023-03-22'\n",
    "end_date  =  '2023-04-23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 2\n",
    "# REMEMBER :  class result : 11, 23, 27\n",
    "\n",
    "folder_year    = '2023'\n",
    "\n",
    "sd_fill_col    = []     #  [ 'temp_s']  # []     #\n",
    "sd_meaninterpolate_col =   ['temp_v', 'co2_v']  #     []   #\n",
    "another_interpolate_col = airthingsPlus + weather_oslo\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col\n",
    "\n",
    "start_date = '2023-11-13'\n",
    "end_date  =  '2023-11-24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 3\n",
    "# REMEMBER : class result : 23, 27\n",
    "\n",
    "folder_year    = '2024'\n",
    "\n",
    "sd_fill_col    = []     #  [ 'temp_s']  # []     #\n",
    "sd_meaninterpolate_col =    ['temp_v', 'co2_v']  #     []   #\n",
    "another_interpolate_col = airthingsPlus + weather_oslo\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col\n",
    "\n",
    "start_date = '2024-02-26'\n",
    "end_date  =  '2024-03-08'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 4\n",
    "# REMEMBER :  class result : 11, 23, 27\n",
    "folder_year    = '2024'\n",
    "sd_fill_col    = []     #  [ 'temp_s']  # []     #\n",
    "sd_meaninterpolate_col =  ['temp_v', 'co2_v']  #     []   #\n",
    "another_interpolate_col = airthingsPlus + weather_oslo\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col\n",
    "\n",
    "start_date = '2024-04-15'\n",
    "end_date  =  '2024-04-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch 4\n",
    "# REMEMBER :  class result : 11, 23, 27\n",
    "folder_year    = 'all years'\n",
    "sd_fill_col    = []     #  [ 'temp_s']  # []     #\n",
    "sd_meaninterpolate_col =  ['temp_v', 'co2_v']  #     []   #\n",
    "another_interpolate_col = airthingsPlus + weather_oslo\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col\n",
    "\n",
    "start_date = '2023-03-03'\n",
    "end_date  =  '2024-04-30'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasserom 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dir = '11'\n",
    "school_name = 'brannfjell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasserom 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "HPF757sSQvdp"
   },
   "outputs": [],
   "source": [
    "class_dir = '23'\n",
    "school_name = 'brannfjell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasserom 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dir = '27'\n",
    "school_name = 'brannfjell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasserom 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dir = '31'\n",
    "school_name = 'brannfjell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KubenVGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1 = 1707\n",
    "class2 = 1713\n",
    "class3 = 2707\n",
    "class4 = 2713\n",
    "class5 = 3707\n",
    "class6 = 3708\n",
    "class7 = 3713\n",
    "occupants_replace =  { class1:30   , class2:18    , class3:30 , class4:17   ,class5: 20   ,class6: 18   ,class7:  25       }\n",
    "airthingsDevice = airthingsPro\n",
    "school_name     = 'kubenVGS'\n",
    "end_file_name  = 'classes'\n",
    "weather_data    = weather_oslo + weather_extra\n",
    "another_interpolate_col = airthingsDevice + weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasserom 1707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dir = str(class1)\n",
    "school_name     = 'kubenVGS'\n",
    "# start_date = '2023-03-16'\n",
    "# end_date  =  '2023-11-25'\n",
    "sd_fill_col = [] #[ 'temp_s']\n",
    "sd_meaninterpolate_col =  []  #['temp_v', 'co2_v']\n",
    "another_interpolate_col = airthingsDevice + weather_data\n",
    "interpolate_col =  sd_fill_col+ another_interpolate_col #+ sd_meaninterpolate_col "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasserom 1713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dir = str(class2)\n",
    "school_name     = 'kubenVGS'\n",
    "# start_date = '2023-03-16'\n",
    "# end_date  =  '2023-11-25'\n",
    "sd_fill_col = [] #[ 'temp_s']\n",
    "sd_meaninterpolate_col =  []  #['temp_v', 'co2_v']\n",
    "another_interpolate_col = airthingsDevice + weather_data\n",
    "interpolate_col =  sd_fill_col+ another_interpolate_col #+ sd_meaninterpolate_col "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasserom 2707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dir = str(class3)\n",
    "school_name     = 'kubenVGS'\n",
    "# start_date = '2023-03-16'\n",
    "# end_date  =  '2023-11-25'\n",
    "sd_fill_col = [] #[ 'temp_s']\n",
    "sd_meaninterpolate_col =  []  #['temp_v', 'co2_v']\n",
    "another_interpolate_col = airthingsDevice + weather_data\n",
    "interpolate_col =  sd_fill_col+ another_interpolate_col #+ sd_meaninterpolate_col "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasserom 2713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dir = str(class4)\n",
    "school_name     = 'kubenVGS'\n",
    "# start_date = '2023-03-16'\n",
    "# end_date  =  '2023-11-25'\n",
    "sd_fill_col = [] #[ 'temp_s']\n",
    "sd_meaninterpolate_col =  []  #['temp_v', 'co2_v']\n",
    "another_interpolate_col = airthingsDevice + weather_data\n",
    "interpolate_col =  sd_fill_col+ another_interpolate_col #+ sd_meaninterpolate_col "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasserom 3707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dir = str(class5)\n",
    "school_name     = 'kubenVGS'\n",
    "# start_date = '2023-03-16'\n",
    "# end_date  =  '2023-11-25'\n",
    "sd_fill_col = [] #[ 'temp_s']\n",
    "sd_meaninterpolate_col =  []  #['temp_v', 'co2_v']\n",
    "another_interpolate_col = airthingsDevice + weather_data\n",
    "interpolate_col =  sd_fill_col+ another_interpolate_col #+ sd_meaninterpolate_col "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasserom 3708"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dir = str(class6)\n",
    "school_name     = 'kubenVGS'\n",
    "# start_date = '2023-03-16'\n",
    "# end_date  =  '2023-11-25'\n",
    "sd_fill_col = [] #[ 'temp_s']\n",
    "sd_meaninterpolate_col =  []  #['temp_v', 'co2_v']\n",
    "another_interpolate_col = airthingsDevice + weather_data\n",
    "interpolate_col =  sd_fill_col+ another_interpolate_col #+ sd_meaninterpolate_col "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasserom 3713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dir = str(class7)\n",
    "school_name     = 'kubenVGS'\n",
    "# start_date = '2023-03-16'\n",
    "# end_date  =  '2023-11-25'\n",
    "sd_fill_col = [] #[ 'temp_s']\n",
    "sd_meaninterpolate_col =  []  #['temp_v', 'co2_v']\n",
    "another_interpolate_col = airthingsDevice + weather_data\n",
    "interpolate_col =  sd_fill_col+ another_interpolate_col #+ sd_meaninterpolate_col "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z405_o3xfeQN"
   },
   "source": [
    "# Processor #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSDpvM_xerFF"
   },
   "source": [
    "## SD Anlegg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "id": "THdAfcGtmP03"
   },
   "outputs": [],
   "source": [
    "manuf_id    = 'sdanlegg'\n",
    "input_file_directory    = platform_directory + '/' + folder_project + '/' + folder_ready + '/' + folder_year + '/' + school_name + '/' + manuf_id + '/'+class_dir+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "id": "dK5McFlwmsEm"
   },
   "outputs": [],
   "source": [
    "folder_path1 =  input_file_directory # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith('.csv')] # get list of csv files\n",
    "\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_csv(file_path, delimiter = ',') #.iloc[3:] # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list\n",
    "    \n",
    "# concatenate dataframes in the list into a single dataframe\n",
    "sdanlegg = pd.concat(dfs1, ignore_index=True) #.fillna(method='backfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdanlegg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYEx2hNvm27_"
   },
   "outputs": [],
   "source": [
    "datetime_indexer(sdanlegg )\n",
    "sdanlegg = sdanlegg[sdanlegg['date'].between(start_date,end_date) \n",
    "                    # & sdanlegg['time'].between(start_time,end_time)\n",
    "                    ]\n",
    "sdanlegg = sdanlegg.iloc[:,7: ]\n",
    "sdanlegg = sdanlegg.resample(time_series_resample).quantile(0.5)\n",
    "#sdanlegg = sdanlegg.resample(time_series_resample).mean()\n",
    "sdanlegg = sdanlegg.resample(desample_option).fillna(method='bfill').round(2)\n",
    "\n",
    "#select special date\n",
    "sdanlegg = sdanlegg.reset_index()\n",
    "sdanlegg['date'] = pd.to_datetime(sdanlegg['datetime']).dt.strftime('%Y-%m-%d')\n",
    "sdanlegg  = sdanlegg[sdanlegg['date'].between(start_date,end_date)].drop('date', axis=1).set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdanlegg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFPD_fO6e8m0"
   },
   "source": [
    "## Airthings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "id": "1Lu-h4Aye8m1"
   },
   "outputs": [],
   "source": [
    "manuf_id    = 'airthings'\n",
    "input_file_directory    = platform_directory + '/' + folder_project + '/' + folder_ready + '/' + folder_year + '/' + school_name + '/' + manuf_id + '/'+class_dir+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "id": "mzqCQ7pqe8m2"
   },
   "outputs": [],
   "source": [
    "folder_path1 = input_file_directory # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith('.csv')] # get list of csv files\n",
    "\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_csv(file_path, delimiter = ',').iloc[3:] # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXLYQPNie8m2"
   },
   "outputs": [],
   "source": [
    "# concatenate dataframes in the list into a single dataframe\n",
    "airthings = pd.concat(dfs1, ignore_index=True) #.fillna(method='backfill')\n",
    "datetime_indexer(airthings )\n",
    "airthings = airthings[airthings['date'].between(start_date,end_date) \n",
    "                    #   & airthings['time'].between(start_time,end_time)\n",
    "                      ]\n",
    "airthings = airthings.iloc[:,7: ]\n",
    "airthings = airthings.resample(time_series_resample).quantile(0.5)\n",
    "#airthings = airthings.resample(time_series_resample).mean()\n",
    "airthings = airthings.resample(desample_option).fillna(method='bfill').round(2)\n",
    "\n",
    "#select special date\n",
    "airthings = airthings.reset_index()\n",
    "airthings['date'] = pd.to_datetime(airthings['datetime']).dt.strftime('%Y-%m-%d')\n",
    "airthings  = airthings[airthings['date'].between(start_date,end_date)].drop('date', axis=1).set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24hJnBugkYVH"
   },
   "outputs": [],
   "source": [
    "# Initialize psychrometric library\n",
    "psychrolib.SetUnitSystem(psychrolib.SI)  # Set units to SI (Celsius, Pa, kg/kg, etc.)\n",
    "\n",
    "# Calculate enthalpy using dry bulb temperature and relative humidity\n",
    "enthalpy = []\n",
    "humid_rat = []\n",
    "wet_bulb = []\n",
    "for index, row in airthings.iterrows():\n",
    "    tdb = row['temp_a']  # Dry bulb temperature in Celsius\n",
    "    rh = row['rh_a']/100  # Relative humidity (fraction, not percentage)\n",
    "    pa = row['press_a']*100  #atmospheric pressure in Pascal\n",
    "\n",
    "    h = psychrolib.GetHumRatioFromRelHum(tdb, rh, pa)  # Calculate humidity ratio\n",
    "    wb = psychrolib.GetTWetBulbFromRelHum(tdb, rh, pa )\n",
    "    #append all calculation\n",
    "    enthalpy.append(psychrolib.GetMoistAirEnthalpy(tdb, h) / 1000)  # Calculate enthalpy (in kJ/kg) using tdb and humid ratio, Convert J/kg to kJ/kg\n",
    "    humid_rat.append(h)\n",
    "    wet_bulb.append(wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "id": "itthjXdslpN7"
   },
   "outputs": [],
   "source": [
    "# Add enthalpy column to the DataFrame\n",
    "airthings['enth_a']   = np.around(enthalpy,2)\n",
    "airthings['hura_a']   = humid_rat\n",
    "airthings['hura_a']   = np.around((airthings['hura_a']  *1000),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "id": "1qIqIyMXKGrn"
   },
   "outputs": [],
   "source": [
    "pmv_ppd_generator(airthings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airthings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N36vbgUPO1aK"
   },
   "source": [
    "## IS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "id": "VcLAyNWmRuaZ"
   },
   "outputs": [],
   "source": [
    "manuf_id    = 'niluapp'\n",
    "input_file_directory      = platform_directory + '/' + folder_project + '/' + folder_ready + '/' + folder_year + '/' + school_name + '/'+ manuf_id + '/' +class_dir+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path1 =  input_file_directory # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith('.csv')] # get list of csv files\n",
    "\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_csv(file_path, delimiter = ',') #.iloc[3:] # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list\n",
    "    \n",
    "# concatenate dataframes in the list into a single dataframe\n",
    "niluapp = pd.concat(dfs1, ignore_index=True) #.fillna(method='backfill')\n",
    "\n",
    "datetime_indexer(niluapp )\n",
    "#niluapp = niluapp[niluapp['date'].between(start_date,end_date) & niluapp['time'].between(start_time,end_time)]\n",
    "niluapp = niluapp.iloc[:,8: ]\n",
    "niluapp[response_counting] = 1\n",
    "niluapp.insert(0, response_counting, niluapp.pop(response_counting))\n",
    "\n",
    "\n",
    "#select special date\n",
    "niluapp = niluapp.reset_index()\n",
    "niluapp['date'] = pd.to_datetime(niluapp['datetime']).dt.strftime('%Y-%m-%d')\n",
    "niluapp  = niluapp[niluapp['date'].between(start_date,end_date)].drop('date', axis=1).set_index('datetime')\n",
    "\n",
    "quantitative_niluapp(niluapp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "id": "R8csaQqBg9qp"
   },
   "outputs": [],
   "source": [
    "answer_replace        = { 0:np.nan, 1:2 , 2:1 ,     3:-1 , 4:-2 }\n",
    "#answer_replace        = { 0:np.nan , 1:-2 , 2:-1 , 3:1 , 4:2 }\n",
    "#answer_replace        = { 0:np.nan , 1:1 , 2:1 , 3:-1 , 4:-1 }\n",
    "answer_replace_coldhot = {1:3 , 2:2 , 3:1 , 4:0 , 5:-1 , 6:-2 , 7:-3 }\n",
    "#answer_replace        = {0:0 ,1:1 , 2:0.5, 3:-0.5, 4:-1 }\n",
    "niluapp['temp_coldhot'] = niluapp['temp_coldhot'].replace(answer_replace_coldhot)\n",
    "niluapp.iloc[:,1:6] = niluapp.iloc[:,1:6].replace(answer_replace)   #replace the codification of response's answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "id": "iZhXBtwesmAk"
   },
   "outputs": [],
   "source": [
    "niluapp1 = niluapp[is_col_multi].resample(survey_resample).quantile(0.5).round(0)      #.median()\n",
    "niluapp2 = niluapp.drop(is_col_multi, axis=1).resample(survey_resample).sum()\n",
    "niluapp  = niluapp1 .join(niluapp2, how='outer')                       #resampling and summarize mainquestion's value\n",
    "niluapp  = niluapp[~(niluapp.iloc[:,6:21] == 0).all(axis=1)]         #delete a rows if they have 0 values in main questionscolumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9ScIpbjXRPh"
   },
   "source": [
    "## outdoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "id": "mjShFxLgXRPq"
   },
   "outputs": [],
   "source": [
    "manuf_id    = 'seklima'\n",
    "input_file_directory  = platform_directory + '/' + folder_project + '/' + folder_ready + '/' + folder_year + '/' + school_name + '/' + manuf_id + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "id": "bot3bfZzXRPq"
   },
   "outputs": [],
   "source": [
    "folder_path1 = input_file_directory # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith('.csv')] # get list of csv files\n",
    "\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_csv(file_path).iloc[3:] # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list\n",
    "    \n",
    "# concatenate dataframes in the list into a single dataframe\n",
    "outdoor = pd.concat(dfs1, ignore_index=True) #.fillna(method='backfill')\n",
    "\n",
    "\n",
    "datetime_indexer(outdoor )\n",
    "#outdoor = outdoor[outdoor['date'].between(start_date,end_date) & outdoor['time'].between(start_time,end_time)]\n",
    "outdoor = outdoor.iloc[:,6: ]\n",
    "outdoor = outdoor.resample(time_series_resample).mean().interpolate(method='index')\n",
    "#outdoor = outdoor.resample(desample_option).bfill().round(2).dropna(how='all',axis=0)\n",
    "\n",
    "#select special date\n",
    "outdoor = outdoor.reset_index()\n",
    "outdoor['date'] = pd.to_datetime(outdoor['datetime']).dt.strftime('%Y-%m-%d')\n",
    "outdoor  = outdoor[outdoor['date'].between(start_date,end_date)].drop('date', axis=1).set_index('datetime')\n",
    "\n",
    "\n",
    "\n",
    "# datetime_indexer(outdoor )\n",
    "# outdoor = outdoor[outdoor['date'].between(start_date,end_date) \n",
    "#                 #   & outdoor['time'].between(start_time,end_time)\n",
    "#                   ]\n",
    "# outdoor2 = outdoor.iloc[:,6: ].drop(columns=['rain_o'], axis=1)\n",
    "# outdoor2 = outdoor2.resample(time_series_resample).mean().interpolate(method='index').round(3)\n",
    "# outdoor2['rain_o'] = outdoor['rain_o'].resample(time_series_resample).sum().interpolate(method='index')\n",
    "# outdoor = outdoor.resample(desample_option).bfill().round(2).dropna(how='all',axis=0)\n",
    "\n",
    "# #select special date\n",
    "# outdoor2 = outdoor2.reset_index()\n",
    "# outdoor2['date'] = pd.to_datetime(outdoor2['datetime']).dt.strftime('%Y-%m-%d')\n",
    "# outdoor  = outdoor2[outdoor2['date'].between(start_date,end_date)].drop('date', axis=1).set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "id": "VA6GlaY7Pr8a"
   },
   "outputs": [],
   "source": [
    "# Initialize psychrometric library\n",
    "psychrolib.SetUnitSystem(psychrolib.SI)  # Set units to SI (Celsius, Pa, kg/kg, etc.)\n",
    "\n",
    "# Calculate enthalpy using dry bulb temperature and relative humidity\n",
    "enthalpy = []\n",
    "humid_rat = []\n",
    "wet_bulb = []\n",
    "for index, row in outdoor.iterrows():\n",
    "    tdb = row['temp_o']  # Dry bulb temperature in Celsius\n",
    "    rh = row['rh_o']/100  # Relative humidity (fraction, not percentage)\n",
    "    pa = row['press_o']*100 #atmospheric pressure in Pascal\n",
    "\n",
    "    h = psychrolib.GetHumRatioFromRelHum(tdb, rh, pa)  # Calculate humidity ratio\n",
    "    wb = psychrolib.GetTWetBulbFromRelHum(tdb, rh, pa )\n",
    "    #append all calculation\n",
    "    enthalpy.append(psychrolib.GetMoistAirEnthalpy(tdb, h) / 1000)  # Calculate enthalpy (in kJ/kg) using tdb and humid ratio, Convert J/kg to kJ/kg\n",
    "    humid_rat.append(h)\n",
    "    wet_bulb.append(wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "id": "ojY85hAcPr8g"
   },
   "outputs": [],
   "source": [
    "# Add enthalpy column to the DataFrame\n",
    "outdoor['enth_o']   = np.around(enthalpy,2)\n",
    "outdoor['hura_o'] = humid_rat\n",
    "outdoor['hura_o']   = np.around((outdoor['hura_o']  *1000),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNQmOz_ioSj8"
   },
   "source": [
    "## Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jd_HdDVToQ5d"
   },
   "outputs": [],
   "source": [
    "combined1 =sdanlegg .join(airthings, how='outer').join(outdoor, how='outer').join(niluapp, how='outer')\n",
    "# combined1 =airthings.join(outdoor, how='outer').join(niluapp, how='outer')\n",
    "\n",
    "#numeric correction (Interpolating & Filling)\n",
    "combined1.loc[:,interpolate_col] = combined1.loc[:,interpolate_col].interpolate(method= 'index').round(2)\n",
    "combined1.loc[:,sd_fill_col] = combined1.loc[:,sd_fill_col].fillna(method='ffill')\n",
    "combined1['hura_d'] = combined1['hura_a'] - combined1['hura_o']\n",
    "#selector\n",
    "datedayhour_creator(combined1)\n",
    "combined1 = combined1[(combined1['date'].between(start_date,end_date)) \n",
    "                    #  &(combined1['day'].between(start_day,end_day)) &\n",
    "                    #  &(combined1['hour'].between(start_hour,end_hour))\n",
    "                     ] .drop(['date', 'day'],axis = 1).set_index(['datetime'])#.dropna(how= 'any')\n",
    "combined1 [room_col_name]   = class_dir\n",
    "combined1 [school_col_name] = school_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlxHR-En3TMt"
   },
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "id": "A732QtbsNdbV"
   },
   "outputs": [],
   "source": [
    "#put spesific Directory and File Name.\n",
    "directory_path = platform_directory + '/' + folder_project + '/' + folder_ready + '/'+ folder_year + '/' + school_name + '/' + folder_export + '/'\n",
    "combined1.reset_index(inplace=True)\n",
    "\n",
    "combined1['date'] = combined1['datetime'].dt.strftime('%Y-%m-%d')\n",
    "combined1['time'] = combined1['datetime'].dt.strftime('%H:%M')\n",
    "filename   = school_name+'_'+ class_dir + '_combined all.csv'\n",
    "filenamexl = school_name+'_'+ class_dir + '_combined all.xlsx'\n",
    "\n",
    "# combined1 = combined1.drop(['datetime'], axis = 1)\n",
    "# save dataframe to CSV file in specified directory\n",
    "directory_export = directory_path + data_for1 + '/' + time_series_resample + '_'\n",
    "# combined1.to_excel( directory_export + filenamexl, index=False)\n",
    "combined1.to_csv( directory_export + filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2Dee-cRh4lI"
   },
   "source": [
    "# Processor #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koFK93iGh4lP"
   },
   "source": [
    "## SD Anlegg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "id": "v4BhdtFBh4lP"
   },
   "outputs": [],
   "source": [
    "manuf_id    = 'sdanlegg'\n",
    "input_file_directory      = platform_directory + '/' + folder_project + '/' + folder_ready + '/' + folder_year + '/' + school_name + '/' + manuf_id + '/' +class_dir+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "id": "BXeQdxqih4lP"
   },
   "outputs": [],
   "source": [
    "folder_path1 =  input_file_directory # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith('.csv')] # get list of csv files\n",
    "\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_csv(file_path, delimiter = ',') #.iloc[3:] # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "id": "WdK0Iy3Uh4lP"
   },
   "outputs": [],
   "source": [
    "# concatenate dataframes in the list into a single dataframe\n",
    "sdanlegg = pd.concat(dfs1, ignore_index=True) #.fillna(method='backfill')\n",
    "datetime_indexer(sdanlegg )\n",
    "sdanlegg1 = sdanlegg.loc[:,sd_fill_col].resample(survey_resample).quantile(0.5) #.round(2)\n",
    "sdanlegg1 = sdanlegg1.resample(minutely_resample).bfill().round(2)\n",
    "sdanlegg2 = sdanlegg.loc[:,sd_meaninterpolate_col].resample(survey_resample).quantile(0.75) #.round(2)\n",
    "#sdanlegg2 = sdanlegg.loc[:,sd_meaninterpolate_col].resample(time_series_resample).mean() #.round(2)\n",
    "sdanlegg2 = sdanlegg2.resample(minutely_resample).bfill().round(2) #.asfreq()\n",
    "sdanlegg = sdanlegg1.join(sdanlegg2, how = 'outer')\n",
    "\n",
    "#select special date\n",
    "sdanlegg = sdanlegg.reset_index()\n",
    "sdanlegg['date'] = pd.to_datetime(sdanlegg['datetime']).dt.strftime('%Y-%m-%d')\n",
    "sdanlegg  = sdanlegg[sdanlegg['date'].between(start_date,end_date)].drop('date', axis=1).set_index('datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zc5_INxwh4lP"
   },
   "source": [
    "## Airthings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "id": "c3eRdmt0h4lQ"
   },
   "outputs": [],
   "source": [
    "manuf_id    = 'airthings'\n",
    "input_file_directory      = platform_directory + '/' + folder_project + '/' + folder_ready + '/' + folder_year + '/' + school_name + '/' + manuf_id + '/' +class_dir+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "id": "FlMg8AQvh4lQ"
   },
   "outputs": [],
   "source": [
    "folder_path1 = input_file_directory # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith('.csv')] # get list of csv files\n",
    "\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_csv(file_path, delimiter = ',').iloc[3:] # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dataframes in the list into a single dataframe\n",
    "airthings = pd.concat(dfs1, ignore_index=True) #.fillna(method='backfill')\n",
    "datetime_indexer(airthings )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "id": "CaIOkowNh4lQ"
   },
   "outputs": [],
   "source": [
    "# concatenate dataframes in the list into a single dataframe\n",
    "airthings = pd.concat(dfs1, ignore_index=True) #.fillna(method='backfill')\n",
    "datetime_indexer(airthings )\n",
    "airthings = airthings.iloc[:,7: ]\n",
    "airthings = airthings.resample(survey_resample).quantile(0.5)\n",
    "#airthings = airthings.resample(time_series_resample).mean()\n",
    "airthings = airthings.resample(minutely_resample).bfill().round(2)\n",
    "\n",
    "#select special date\n",
    "airthings = airthings.reset_index()\n",
    "airthings['date'] = pd.to_datetime(airthings['datetime']).dt.strftime('%Y-%m-%d')\n",
    "airthings  = airthings[airthings['date'].between(start_date,end_date)].drop('date', axis=1).set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLJASRvYpRy7"
   },
   "outputs": [],
   "source": [
    "# Initialize psychrometric library\n",
    "psychrolib.SetUnitSystem(psychrolib.SI)  # Set units to SI (Celsius, Pa, kg/kg, etc.)\n",
    "\n",
    "# Calculate enthalpy using dry bulb temperature and relative humidity\n",
    "enthalpy = []\n",
    "humid_rat = []\n",
    "wet_bulb = []\n",
    "for index, row in airthings.iterrows():\n",
    "    tdb = row['temp_a']  # Dry bulb temperature in Celsius\n",
    "    rh = row['rh_a']/100  # Relative humidity (fraction, not percentage)\n",
    "    pa = row['press_a']*100  #atmospheric pressure in Pascal\n",
    "\n",
    "    h = psychrolib.GetHumRatioFromRelHum(tdb, rh, pa)  # Calculate humidity ratio\n",
    "    wb = psychrolib.GetTWetBulbFromRelHum(tdb, rh, pa )\n",
    "    #append all calculation\n",
    "    enthalpy.append(psychrolib.GetMoistAirEnthalpy(tdb, h) / 1000)  # Calculate enthalpy (in kJ/kg) using tdb and humid ratio, Convert J/kg to kJ/kg\n",
    "    humid_rat.append(h)\n",
    "    wet_bulb.append(wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "id": "eD2U_LlkpRzP"
   },
   "outputs": [],
   "source": [
    "# Add enthalpy column to the DataFrame\n",
    "airthings['enth_a']   = np.around(enthalpy,2)\n",
    "airthings['hura_a']   = humid_rat\n",
    "airthings['hura_a']   = np.around((airthings['hura_a']  *1000),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "id": "umBUDxWNLNFZ"
   },
   "outputs": [],
   "source": [
    "pmv_ppd_generator(airthings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I79_2UxPh4lQ"
   },
   "source": [
    "## IS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "id": "nISw3mbyh4lQ"
   },
   "outputs": [],
   "source": [
    "manuf_id    = 'niluapp'\n",
    "input_file_directory      = platform_directory + '/' + folder_project + '/' + folder_ready + '/' + folder_year + '/' + school_name + '/'+ manuf_id + '/' +class_dir+'/'\n",
    "folder_path1 =  input_file_directory # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith('.csv')] # get list of csv files\n",
    "\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_csv(file_path, delimiter = ',') #.iloc[3:] # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list\n",
    "    \n",
    "# concatenate dataframes in the list into a single dataframe\n",
    "niluapp = pd.concat(dfs1, ignore_index=True) #.fillna(method='backfill')\n",
    "\n",
    "\n",
    "datetime_indexer(niluapp )\n",
    "#niluapp = niluapp[niluapp['date'].between(start_date,end_date) & niluapp['time'].between(start_time,end_time)]\n",
    "niluapp = niluapp.iloc[:,8: ]\n",
    "\n",
    "#select special date\n",
    "niluapp = niluapp.reset_index()\n",
    "niluapp['date'] = pd.to_datetime(niluapp['datetime']).dt.strftime('%Y-%m-%d')\n",
    "niluapp  = niluapp[niluapp['date'].between(start_date,end_date)].drop('date', axis=1).set_index('datetime')\n",
    "\n",
    "quantitative_niluapp(niluapp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "id": "9ZMu-6ULqjIQ"
   },
   "outputs": [],
   "source": [
    "answer_replace        = { 0:np.nan, 1:2 , 2:1 ,     3:-1 , 4:-2 }\n",
    "#answer_replace        = { 0:np.nan , 1:-2 , 2:-1 , 3:1 , 4:2 }\n",
    "#answer_replace        = { 0:np.nan , 1:1 , 2:1 , 3:-1 , 4:-1 }\n",
    "answer_replace_coldhot = {0:np.nan , 1:-3 , 2:-2 , 3:-1 , 4:0 , 5:1 , 6:2 , 7:3 }\n",
    "#answer_replace        = {0:0 ,1:1 , 2:0.5, 3:-0.5, 4:-1 }\n",
    "niluapp.iloc[:,:5] = niluapp.iloc[:,0:5].replace(answer_replace)   #replace the codification of response's answers\n",
    "niluapp['temp_coldhot'] = niluapp['temp_coldhot'].replace(answer_replace_coldhot)\n",
    "niluapp[response_counting] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cr-ENcYh4lQ"
   },
   "source": [
    "## outdoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "id": "Ly6giazkh4lQ"
   },
   "outputs": [],
   "source": [
    "manuf_id    = 'seklima'\n",
    "input_file_directory  = platform_directory + '/' + folder_project + '/' + folder_ready + '/' + folder_year + '/' + school_name + '/' + manuf_id + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "id": "R5CIB9nXh4lR"
   },
   "outputs": [],
   "source": [
    "folder_path1 = input_file_directory # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith('.csv')] # get list of csv files\n",
    "\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_csv(file_path, delimiter = ',').iloc[3:] # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "id": "w_5HDI6Mh4lR"
   },
   "outputs": [],
   "source": [
    "# concatenate dataframes in the list into a single dataframe\n",
    "outdoor = pd.concat(dfs1, ignore_index=True) #.fillna(method='backfill')\n",
    "datetime_indexer(outdoor )\n",
    "outdoor = outdoor.iloc[:,6: ]\n",
    "outdoor = outdoor.resample(time_series_resample).quantile(0.75)\n",
    "#outdoor = outdoor.resample(survey_resample).interpolate(method='index').round(2)\n",
    "outdoor = outdoor.resample(minutely_resample).bfill().round(2)\n",
    "outdoor['rain_o'] = outdoor['rain_o'].resample(time_series_resample).sum().interpolate(method='index')\n",
    "\n",
    "#select special date\n",
    "outdoor = outdoor.reset_index()\n",
    "outdoor['date'] = pd.to_datetime(outdoor['datetime']).dt.strftime('%Y-%m-%d')\n",
    "outdoor  = outdoor[outdoor['date'].between(start_date,end_date)].drop('date', axis=1).set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uj5KnrFYAr-"
   },
   "outputs": [],
   "source": [
    "# Initialize psychrometric library\n",
    "psychrolib.SetUnitSystem(psychrolib.SI)  # Set units to SI (Celsius, Pa, kg/kg, etc.)\n",
    "\n",
    "# Calculate enthalpy using dry bulb temperature and relative humidity\n",
    "enthalpy = []\n",
    "humid_rat = []\n",
    "wet_bulb = []\n",
    "for index, row in outdoor.iterrows():\n",
    "    tdb = row['temp_o']  # Dry bulb temperature in Celsius\n",
    "    rh = row['rh_o']/100  # Relative humidity (fraction, not percentage)\n",
    "    pa = row['press_o']*100  #atmospheric pressure in Pascal\n",
    "\n",
    "    h = psychrolib.GetHumRatioFromRelHum(tdb, rh, pa)  # Calculate humidity ratio\n",
    "    wb = psychrolib.GetTWetBulbFromRelHum(tdb, rh, pa )\n",
    "    #append all calculation\n",
    "    enthalpy.append(psychrolib.GetMoistAirEnthalpy(tdb, h) / 1000)  # Calculate enthalpy (in kJ/kg) using tdb and humid ratio, Convert J/kg to kJ/kg\n",
    "    humid_rat.append(h)\n",
    "    wet_bulb.append(wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "id": "wAfUGyfeYAsD"
   },
   "outputs": [],
   "source": [
    "# Add enthalpy column to the DataFrame\n",
    "outdoor['enth_o']   = np.around(enthalpy,2)\n",
    "outdoor['hura_o']   = humid_rat\n",
    "outdoor['hura_o']   = np.around((outdoor['hura_o']  *1000),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhgUtIl6h4lR"
   },
   "source": [
    "## Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0RmwFrEh4lR"
   },
   "outputs": [],
   "source": [
    "# combined2 = airthings.join(outdoor, how='outer').join(niluapp, how='outer')\n",
    "combined2 = sdanlegg.join(airthings, how='outer').join(outdoor, how='outer').join(niluapp, how='outer')\n",
    "\n",
    "combined2.loc[:,interpolate_col] = combined2.loc[:,interpolate_col].fillna(method='ffill').round(2)\n",
    "combined2['hura_d'] = combined2['hura_a'] + combined2['hura_o']\n",
    "#selector\n",
    "datedayhour_creator(combined2)\n",
    "combined2 = combined2[(combined2['date'].between(start_date,end_date) & combined2['day'].between(start_day,end_day) & combined2['hour'].between(start_hour,end_hour))].set_index(['datetime']) .drop(['date', 'day'],axis = 1).dropna(subset=is_col_main)\n",
    "combined2[room_col_name]= class_dir\n",
    "combined2[school_col_name] = school_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZ2qerJt3Ypl"
   },
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "id": "7xuVZyQ2-9wQ"
   },
   "outputs": [],
   "source": [
    "#put spesific Directory and File Name.\n",
    "directory_path = platform_directory + '/' + folder_project + '/' + folder_ready+ '/' + folder_year + '/' + school_name + '/' + folder_export + '/'\n",
    "\n",
    "combined2.reset_index(inplace=True)\n",
    "combined2['date'] = combined2['datetime'].dt.strftime('%Y-%m-%d')\n",
    "combined2['time'] = combined2['datetime'].dt.strftime('%H:%M')\n",
    "filename = school_name+'_'+class_dir + '_combined all.csv'\n",
    "filenamexl = school_name+'_'+class_dir + '_combined all.xlsx'\n",
    "\n",
    "directory_export = directory_path + data_for2+ '/' + survey_resample + '_'\n",
    "combined2.to_excel( directory_export + filenamexl, index=False)\n",
    "combined2.to_csv( directory_export + filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(directory_export+filenamexl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1699286377194,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "T39GLeP2kLF4",
    "outputId": "6f4441e8-7334-4e13-cbed-36f6369514a1"
   },
   "outputs": [],
   "source": [
    "print(do you know Stop?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processor #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SD Anlegg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_interpolate_col = [ co2_name,temp_data_name, rel_hum_name, press_name, voc_name,bright_name, radon_name ]\n",
    "sd_meaninterpolate_col = ['temp_v', 'co2_v']\n",
    "\n",
    "interpolate_col =  sd_meaninterpolate_col + another_interpolate_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manuf_id    = 'sdanlegg'\n",
    "input_file_directory    = platform_directory + '/' + folder_project + '/' + folder_ready + '/' + folder_year + '/' + school_name + '/' + manuf_id + '/'+class_dir+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path1 =  input_file_directory # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith('.csv')] # get list of csv files\n",
    "\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_csv(file_path, delimiter = ',') #.iloc[3:] # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list\n",
    "    \n",
    "# concatenate dataframes in the list into a single dataframe\n",
    "sdanlegg = pd.concat(dfs1, ignore_index=True) #.fillna(method='backfill')\n",
    "\n",
    "datetime_indexer(sdanlegg )\n",
    "#sdanlegg = sdanlegg[sdanlegg['date'].between(start_date,end_date) & sdanlegg['time'].between(start_time,end_time)]\n",
    "sdanlegg = sdanlegg.iloc[:,7: ]\n",
    "sdanlegg = sdanlegg.resample(time_series_resample).quantile(0.5)\n",
    "#sdanlegg = sdanlegg.resample(time_series_resample).mean()\n",
    "sdanlegg = sdanlegg.resample(desample_option).fillna(method='bfill').round(2)\n",
    "\n",
    "#select special date\n",
    "sdanlegg = sdanlegg.reset_index()\n",
    "sdanlegg['date'] = pd.to_datetime(sdanlegg['datetime']).dt.strftime('%Y-%m-%d')\n",
    "sdanlegg = sdanlegg.set_index(['datetime'])[sd_meaninterpolate_col]\n",
    "# sdanlegg  = sdanlegg[sdanlegg['date'].between(start_date,end_date)].drop('date', axis=1).set_index('datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airthings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manuf_id    = 'airthings'\n",
    "input_file_directory    = platform_directory + '/' + folder_project + '/' + folder_ready + '/' + folder_year + '/' + school_name + '/' + manuf_id + '/'+class_dir+'/'\n",
    "airthingsDevice = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path1 = input_file_directory # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith('.csv')] # get list of csv files\n",
    "\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_csv(file_path, delimiter = ',').iloc[3:] # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dataframes in the list into a single dataframe\n",
    "airthings = pd.concat(dfs1, ignore_index=True) #.fillna(method='backfill')\n",
    "datetime_indexer(airthings )\n",
    "#airthings = airthings[airthings['date'].between(start_date,end_date) & airthings['time'].between(start_time,end_time)]\n",
    "airthings = airthings.iloc[:,7: ]\n",
    "airthings = airthings.resample(time_series_resample).quantile(0.5)\n",
    "#airthings = airthings.resample(time_series_resample).mean()\n",
    "airthings = airthings.resample(desample_option).fillna(method='bfill').round(2)\n",
    "\n",
    "#select special date\n",
    "airthings = airthings.reset_index()\n",
    "airthings['date'] = pd.to_datetime(airthings['datetime']).dt.strftime('%Y-%m-%d')\n",
    "airthings = airthings.set_index(['datetime'])\n",
    "# airthings  = airthings[airthings['date'].between(start_date,end_date)].drop('date', axis=1).set_index('datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manuf_id    = 'niluapp'\n",
    "input_file_directory      = platform_directory + '/' + folder_project + '/' + folder_ready + '/' + folder_year + '/' + school_name + '/'+ manuf_id + '/' +class_dir+'/'\n",
    "input_file_directory\n",
    "folder_path1 =  input_file_directory # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith('.csv')] # get list of csv files\n",
    "\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_csv(file_path, delimiter = ',') #.iloc[3:] # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list\n",
    "    \n",
    "# concatenate dataframes in the list into a single dataframe\n",
    "niluapp = pd.concat(dfs1, ignore_index=True) #.fillna(method='backfill')\n",
    "\n",
    "datetime_indexer(niluapp )\n",
    "#niluapp = niluapp[niluapp['date'].between(start_date,end_date) & niluapp['time'].between(start_time,end_time)]\n",
    "niluapp = niluapp.iloc[:,8: ]\n",
    "niluapp[response_counting] = 1\n",
    "niluapp.insert(0, response_counting, niluapp.pop(response_counting))\n",
    "\n",
    "\n",
    "#select special date\n",
    "niluapp = niluapp.reset_index()\n",
    "niluapp['date'] = pd.to_datetime(niluapp['datetime']).dt.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niluapp['date'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined3 =sdanlegg .join(airthings, how='inner', on='datetime')\n",
    "# combined1 =airthings.join(outdoor, how='outer').join(niluapp, how='outer')\n",
    "combined3 = pd.concat([airthings,sdanlegg], axis= 'columns', ignore_index=False)#.drop('temp_s', axis=1)  #dropna(subset=['co2_v', 'co2_a' ], axis=0)\n",
    "#numeric correction (Interpolating & Filling)\n",
    "# combined3.loc[:,interpolate_col] = combined3.loc[:,interpolate_col].interpolate(method= 'index').round(2)\n",
    "# combined3.loc[:,sd_fill_col] = combined3.loc[:,sd_fill_col].fillna(method='ffill')\n",
    "\n",
    "#selector\n",
    "datedayhour_creator(combined3)\n",
    "combined3 = combined3[           (combined3['date'].isin  (niluapp['date'].unique())) &\n",
    "                     (combined3['day'].between(start_day,end_day)) & (combined3['hour'].between(6,16)\n",
    "                     )] .drop([ 'day'],axis = 1).set_index(['datetime'])\n",
    "combined3 [room_col_name]   = class_dir\n",
    "combined3 [school_col_name] = school_name\n",
    "combined3 = combined3[~(combined3['temp_a'].isna() & combined3['temp_v'].isna())]\n",
    "combined3#.dropna(subset=['co2_v', 'co2_a' ])\n",
    "#numeric correction (Interpolating & Filling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put spesific Directory and File Name.\n",
    "directory_path = platform_directory + '/' + folder_project + '/' + folder_ready + '/'+ folder_year + '/' + school_name + '/' + folder_export + '/'\n",
    "combined3.reset_index(inplace=True)\n",
    "\n",
    "combined3['date'] = combined3['datetime'].dt.strftime('%Y-%m-%d')\n",
    "combined3['time'] = combined3['datetime'].dt.strftime('%H:%M')\n",
    "name_date = datetime.strptime((combined3[date_name].iloc[1]),'%Y-%m-%d').strftime('%Y%m%d')\n",
    "filename   = school_name+'_'+ class_dir+ '_' + name_date + '.csv'\n",
    "filenamexl = school_name+'_'+ class_dir+ '_' + name_date + '.xlsx'\n",
    "\n",
    "time_zone_converter(combined3, 'UTC', 'Europe/Oslo')\n",
    "datetime_indexer_noindex(combined3)\n",
    "datedayhour_creator_noindex(combined3)\n",
    "# save dataframe to CSV file in specified directory\n",
    "directory_export = directory_path + data_for3 + '/' + time_series_resample + '_'\n",
    "combined3.to_excel( directory_export + filenamexl, index=False)\n",
    "combined3.to_csv( directory_export + filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(directory_export + filenamexl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(combined3 , x='co2_v', y= 'co2_a')\n",
    "# sns.scatterplot(combined3 , x='temp_v', y= 'temp_a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# x_axis = 'co2_v'\n",
    "# y_axis = 'co2_a'\n",
    "x_axis = 'temp_v'\n",
    "y_axis = 'temp_a'\n",
    "regress_data = combined3.copy().dropna(axis=0)\n",
    "# regress_data = regress_data\n",
    "xb=regress_data[[x_axis]]\n",
    "yb=regress_data[[y_axis]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lin_reg = LinearRegression()\n",
    "Lin_reg.fit(xb, yb)\n",
    "\n",
    "y_pred = Lin_reg.predict(xb)#*back_scaling\n",
    "\n",
    "print(Lin_reg.intercept_)\n",
    "print(Lin_reg.coef_.flatten().round(4))\n",
    "print(r2_score(yb,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_ODCbu5pT1U"
   },
   "source": [
    "# COMBINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1699258259364,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "Gx3hu6gXZmnj",
    "outputId": "e4cd0a71-7ca8-4e1c-bf8e-9bd698e26038"
   },
   "outputs": [],
   "source": [
    "print(\"school is \" +school_name +  ' with class: ' +  class_dir+ ' with resampling: ' + time_series_resample)\n",
    "directory_path = platform_directory + '/' + folder_project + '/' + folder_ready+ '/' + folder_year + '/' + school_name + '/' + folder_export + '/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1699258259365,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "M_bTAQaBYdLp",
    "outputId": "09d15925-3731-4f57-f26c-31844f87e03e"
   },
   "outputs": [],
   "source": [
    "print((directory_path + data_for1 + '/' +school_name +'/') )   # replace with the folder path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path3 = directory_path + data_for3 + '/'    # replace with the folder path\n",
    "csv_files3 = [f for f in os.listdir(folder_path3) if f.endswith('.xlsx') and f.startswith(time_series_resample + '_' + school_name + '_' )] # get list of csv files\n",
    "dfs3 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files3:\n",
    "    file_path = os.path.join(folder_path3, file) # get full path of csv file\n",
    "    df = pd.read_excel(file_path) # read csv file into a dataframe\n",
    "    dfs3.append(df) # append dataframe to list\n",
    "\n",
    "# concatenate dataframes in the list into a single dataframe\n",
    "combined3 = pd.concat(dfs3, ignore_index=True)\n",
    "\n",
    "#we convert time zone of dataframe, from GMT to Local\n",
    "time_zone_converter(combined3, 'UTC', 'Europe/Oslo')\n",
    "datetime_indexer_noindex(combined3)\n",
    "datedayhour_creator_noindex(combined3)\n",
    "\n",
    "\n",
    "#put spesific Directory and File Name.\n",
    "directory_path = platform_directory + '/' + folder_project + '/' + folder_ready + '/' + folder_year + '/' + school_name + '/' + folder_export + '/'\n",
    "\n",
    "combined3['date'] = combined3['datetime'].dt.strftime('%Y-%m-%d')\n",
    "combined3['time'] = combined3['datetime'].dt.strftime('%H:%M')\n",
    "name_date = datetime.strptime((combined3[date_name].iloc[1]),'%Y-%m-%d').strftime('%Y%m%d')\n",
    "filename = school_name+ '_' + name_date + '_combined all classes.csv'\n",
    "filenamexl = school_name+ '_' + name_date + '_combined all classes.xlsx'\n",
    "\n",
    "# save dataframe to CSV file in specified directory\n",
    "directory_export = directory_path + data_for3 + '/'+ time_series_resample\n",
    "combined3.to_excel( directory_export + '_1_' + filenamexl, index=False)\n",
    "combined3.to_csv( directory_export + '_1_' + filename, index=False)\n",
    "combined3.describe().to_excel( directory_export + '_descstat_3_' + school_name + '_' + name_date + '.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvOSXSA4AoyN"
   },
   "source": [
    "## panel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {
    "id": "Co6uCaJxWQx9"
   },
   "outputs": [],
   "source": [
    "folder_path1 = directory_path + data_for1 + '/'    # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith('all.csv') and f.startswith(time_series_resample)] # get list of csv files\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_csv(file_path) # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list\n",
    "\n",
    "# concatenate dataframes in the list into a single dataframe\n",
    "combined1 = pd.concat(dfs1, ignore_index=True)\n",
    "\n",
    "#we convert time zone of dataframe, from GMT to Local\n",
    "time_zone_converter(combined1, 'UTC', 'Europe/Oslo')\n",
    "datetime_indexer_noindex(combined1)\n",
    "datedayhour_creator_noindex(combined1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "id": "qJiH7ldNk_I2"
   },
   "outputs": [],
   "source": [
    "combined1['students'] = combined1['room_id'].replace(occupants_replace)\n",
    "combined1['response_rate'] = combined1[response_counting] / combined1['students']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "id": "eeNLGUtZZHmG"
   },
   "outputs": [],
   "source": [
    "weight_1 = 1\n",
    "weight_2 = 1#0.5  #1\n",
    "weight_3 = 1#0.5  #1\n",
    "weight_4 = 1\n",
    "\n",
    "combined1['ad_air']    = (combined1['feel_air4']*weight_4 + combined1['feel_air3']*weight_3)/combined1[response_counting]\n",
    "combined1['ad_temp']   = (combined1['feel_temp4']*weight_4 + combined1['feel_temp3']*weight_3)/combined1[response_counting]\n",
    "combined1['ad_health'] = (combined1['feel_health4']*weight_4 + combined1['feel_health3']*weight_3)/combined1[response_counting]\n",
    "combined1['ad_bright'] = (combined1['feel_bright4']*weight_4 + combined1['feel_bright3']*weight_3)/combined1[response_counting]\n",
    "combined1['ad_noise']  = (combined1['feel_noise4']*weight_4 + combined1['feel_noise3']*weight_3)/combined1[response_counting]\n",
    "combined1['as_air']    = (combined1['feel_air1']*weight_1 + combined1['feel_air2']*weight_2)/combined1[response_counting]\n",
    "combined1['as_temp']   = (combined1['feel_temp1']*weight_1 + combined1['feel_temp2']*weight_2)/combined1[response_counting]\n",
    "combined1['as_health'] = (combined1['feel_health1']*weight_1 + combined1['feel_health2']*weight_2)/combined1[response_counting]\n",
    "combined1['as_bright'] = (combined1['feel_bright1']*weight_1 + combined1['feel_bright2']*weight_2)/combined1[response_counting]\n",
    "combined1['as_noise']  = (combined1['feel_noise1']*weight_1 + combined1['feel_noise2']*weight_2)/combined1[response_counting]\n",
    "combined1['pps'] = 1-combined1['ppd']\n",
    "\n",
    "combined1['rd_air']    = (combined1['feel_air4']*weight_4 + combined1['feel_air3']*weight_3)/combined1['students']\n",
    "combined1['rd_temp']   = (combined1['feel_temp4']*weight_4 + combined1['feel_temp3']*weight_3)/combined1['students']\n",
    "combined1['rd_health'] = (combined1['feel_health4']*weight_4 + combined1['feel_health3']*weight_3)/combined1['students']\n",
    "combined1['rd_bright'] = (combined1['feel_bright4']*weight_4 + combined1['feel_bright3']*weight_3)/combined1['students']\n",
    "combined1['rd_noise']  = (combined1['feel_noise4']*weight_4 + combined1['feel_noise3']*weight_3)/combined1['students']\n",
    "combined1['rs_air']    = (combined1['feel_air1']*weight_1 + combined1['feel_air2']*weight_2)/combined1['students']\n",
    "combined1['rs_temp']   = (combined1['feel_temp1']*weight_1 + combined1['feel_temp2']*weight_2)/combined1['students']\n",
    "combined1['rs_health'] = (combined1['feel_health1']*weight_1 + combined1['feel_health2']*weight_2)/combined1['students']\n",
    "combined1['rs_bright'] = (combined1['feel_bright1']*weight_1 + combined1['feel_bright2']*weight_2)/combined1['students']\n",
    "combined1['rs_noise']  = (combined1['feel_noise1']*weight_1 + combined1['feel_noise2']*weight_2)/combined1['students']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {
    "id": "4uQvXSUpd9sM"
   },
   "outputs": [],
   "source": [
    "#put spesific Directory and File Name.\n",
    "directory_path = platform_directory + '/' + folder_project + '/' + folder_ready + '/' + folder_year + '/' + school_name + '/' + folder_export + '/'\n",
    "\n",
    "combined1['date'] = combined1['datetime'].dt.strftime('%Y-%m-%d')\n",
    "combined1['time'] = combined1['datetime'].dt.strftime('%H:%M')\n",
    "name_date = datetime.strptime((combined1[date_name].iloc[1]),'%Y-%m-%d').strftime('%Y%m%d')\n",
    "filename = school_name+ '_' + name_date + '_combined all classes.csv'\n",
    "filenamexl = school_name+ '_' + name_date + '_combined all classes.xlsx'\n",
    "\n",
    "# #IF ALL YEARS\n",
    "# filename = school_name + '_alldate.csv'\n",
    "# filenamexl = school_name + '_alldate.xlsx'\n",
    "\n",
    "# save dataframe to CSV file in specified directory\n",
    "directory_export = directory_path + data_for1 + '/'+ time_series_resample\n",
    "combined1.to_excel( directory_export + '_1_' + filenamexl, index=False)\n",
    "combined1.to_csv( directory_export + '_1_' + filename, index=False)\n",
    "combined1.describe().to_excel( directory_export + '_descstat_1_' + school_name + '_' + name_date + '.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1699258261364,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "aaMW3ZzPPV3D",
    "outputId": "855c1a15-0e7e-4a98-bc9a-1a43f9e4c713"
   },
   "outputs": [],
   "source": [
    "print(folder_path1)\n",
    "print(directory_export + '_1_' + filenamexl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1699258261364,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "f3n4I7W0zO8T",
    "outputId": "03636a13-98e2-4815-a64c-e98af72155e7"
   },
   "outputs": [],
   "source": [
    "print(combined1['room_id'].unique())\n",
    "print(combined1['school_id'].unique())\n",
    "print(combined1['time'].unique())\n",
    "print(combined1[response_counting].sum())\n",
    "\n",
    "\n",
    "print(directory_export + '_1_' + filenamexl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydfWPGsFAui5"
   },
   "source": [
    "## cross sectional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Air7tUClFDD5"
   },
   "outputs": [],
   "source": [
    "directory_path = platform_directory + '/' + folder_project + '/' + folder_ready+ '/' + folder_year +'/'+school_name + '/' + folder_export + '/'\n",
    "\n",
    "folder_path1 = directory_path + data_for2 + '/'  #+ time_series_resample + '_'  # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith('all.xlsx') and f.startswith(survey_resample + '_' + school_name)] # get list of csv files\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_excel(file_path) # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list\n",
    "\n",
    "# concatenate dataframes in the list into a single dataframe\n",
    "combined2 = pd.concat(dfs1, ignore_index=True)\n",
    "combined2['temp_coldhot'] = combined2['temp_coldhot'].fillna(0)\n",
    "#we convert time zone of dataframe, from GMT to Local\n",
    "time_zone_converter(combined2, 'UTC', 'Europe/Oslo')\n",
    "datetime_indexer_noindex(combined2)\n",
    "print(folder_path1)\n",
    "#filterized a 'lazy responses', if a response contains 0 (skipped answer) value, it would be eliminated\n",
    "#min_feel = 1\n",
    "#combined2 = combined2[(combined2['feel_air']>=min_feel) & (combined2['feel_temp']>=min_feel) & (combined2['feel_health']>=min_feel) & (combined2['feel_bright']>=min_feel) & (combined2['feel_noise']>=min_feel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {
    "id": "5C_KUlZsLBxu"
   },
   "outputs": [],
   "source": [
    "#put spesific Directory and File Name.\n",
    "directory_path = platform_directory + '/' + folder_project + '/' + folder_ready+ '/' + folder_year +'/'+school_name + '/'\n",
    "\n",
    "#specific for each season\n",
    "combined2['date'] = combined2['datetime'].dt.strftime('%Y-%m-%d')\n",
    "combined2['time'] = combined2['datetime'].dt.strftime('%H:%M')\n",
    "name_date = datetime.strptime((combined2[date_name].iloc[1]),'%Y-%m-%d').strftime('%Y%m%d')\n",
    "filename = school_name+'_' + name_date + '_combined all classes.csv'\n",
    "filenamexl = school_name+'_' + name_date + '_combined all classes.xlsx'\n",
    "\n",
    "# #special for all years\n",
    "# filename = school_name+'_' + '_alldate.csv'\n",
    "# filenamexl = school_name+'_' + '_alldate.xlsx'\n",
    "\n",
    "# save dataframe to CSV file in specified directory\n",
    "directory_export = directory_path + folder_export +'/' + data_for2 + '/'  + survey_resample\n",
    "combined2.to_excel( directory_export + '_2_' + filenamexl, index=False)\n",
    "combined2.to_csv( directory_export + '_2_' + filename, index=False)\n",
    "combined2.describe().to_excel( directory_export + '_descstat_2_' + school_name + '_' + name_date + '.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1699258261720,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "8oMvdRMtBrbJ",
    "outputId": "dc466850-d2c3-4fee-cb5a-0187246a2087"
   },
   "outputs": [],
   "source": [
    "print(combined2['room_id'].unique())\n",
    "print(combined2['school_id'].unique())\n",
    "print(directory_export + '_2_' + filenamexl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1699258262052,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "Qn833Q4HI88H",
    "outputId": "1d76a99d-9724-40af-811a-5b3bff205ce8"
   },
   "outputs": [],
   "source": [
    "combined2.iloc[:,23:45].describe().iloc[1:].round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIVOT all schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_name = 'all schools'\n",
    "folder_export = 'export'\n",
    "data_for4 = 'combined longitudinal'\n",
    "last_file = 'alldate.xlsx'\n",
    "\n",
    "directory_path = platform_directory + '/' + folder_project + '/' + folder_ready+ '/' + folder_year +'/'+school_name + '/' + folder_export + '/'\n",
    "\n",
    "folder_path1 = directory_path + data_for4 + '/'  #+ time_series_resample + '_'  # replace with the folder path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith(last_file) and f.startswith(survey_resample )] # get list of csv files\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_excel(file_path) # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list\n",
    "combined4 = pd.concat(dfs1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined4.to_excel( folder_path1 + survey_resample + '_all schools_' + last_file , index=False)\n",
    "combined4.to_csv( folder_path1 + survey_resample + '_all schools_' + 'alldate.csv' , index=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WX9DV2WOmc51",
    "sQc0T5Blmi74",
    "pbG9dH5Hfhhk",
    "1hY_w7yIfhoH",
    "MwekofXHQvdi",
    "koFK93iGh4lP",
    "zc5_INxwh4lP",
    "I79_2UxPh4lQ"
   ],
   "provenance": [
    {
     "file_id": "1DaVmzULJmgxLTMU-wEdHHu5LVD0Pg5JN",
     "timestamp": 1699215672297
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "DIGGMINSKOLE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
