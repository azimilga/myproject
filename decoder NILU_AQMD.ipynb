{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-gvYTR0y3NQ"
   },
   "source": [
    "**WRITER'S IDENTITY**\n",
    "- AZIMIL GANI ALAM\n",
    "- Ph.D Student\n",
    "- Dept. Energy & Process Tech - NTNU\n",
    "\n",
    "\n",
    "**NOTE :**\n",
    "- This Python File runs in GoogleColab\n",
    "- This work uses\n",
    "  - **Weather Data Record by WWW.SEKLIMA.NO**\n",
    "  - **Outdoor Data Record by NILU Air Quality Monitoring Data**\n",
    "- we want to filterize data results from CSV models become process-ready for Machine Learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOrE8iE7ZNN1"
   },
   "source": [
    "**RULES**\n",
    "1. Run Library Import\n",
    "2. Select Notebook platform by run one of those commands.\n",
    "3. Expand Schools Tab\n",
    " - Run Identity and Data Caller\n",
    " - Expand Classrooms\n",
    " - Run one of Klasserom- XXX\n",
    " - Run Processor. if done, select again one of Klasserom and run again the processor. it is cyclic process\n",
    "\n",
    "to check the data cleaning result, run 'Data Preparation'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obgZUNYWOz3R"
   },
   "source": [
    "# Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1295,
     "status": "ok",
     "timestamp": 1696500528630,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "0L3H4n-ROuzb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folder naming directory\n",
    "platform_directory = 'C:/Users/azimilga'   \n",
    "platform_directory = 'G:/My Drive/Colab Notebooks'      \n",
    "folder_raw     = 'data-raw'\n",
    "folder_ready   = 'data-ready'\n",
    "folder_year    = '2024'\n",
    "folder_project = 'Project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1696500528630,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "fmJz6DACeXr9"
   },
   "outputs": [],
   "source": [
    "school_col_name   = 'school_id'\n",
    "room_col_name     = 'room_id'\n",
    "date_name         = 'date'\n",
    "time_name         = 'time'\n",
    "manuf_col_name    = 'manuf_id'\n",
    "instr_col_name    = 'instr_id'\n",
    "serial_col_name   = 'serial_id'\n",
    "temp_set_name     = 'temp_o'\n",
    "rel_hum_name      = 'rh_o'\n",
    "sun_rad_name      = 'sun_o'\n",
    "winds_name        =  'winds_o'\n",
    "percipitation_name= 'rain_o'\n",
    "pm25_name         = 'pm2.5_o'\n",
    "pm10_name         = 'pm10_o'\n",
    "pressure_name     = 'press_o'\n",
    "gas_nox_name      = 'nox_o'\n",
    "gas_no_name      = 'no_o'\n",
    "gas_no2_name      = 'no2_o'\n",
    "\n",
    "school_col_name   = 'school_id'\n",
    "room_col_name     = 'room_id'\n",
    "date_name         = 'date'\n",
    "time_name         = 'time'\n",
    "manuf_col_name    = 'manuf_id'\n",
    "instr_col_name    = 'instr_id'\n",
    "serial_col_name   = 'serial_id'\n",
    "\n",
    "#identify a datetime name : wont be displayed. just coding purpose\n",
    "datetime_name = 'datetime'\n",
    "\n",
    "#filtering rows data regarding to desired specific time\n",
    "start_time =  '00:00:00'\n",
    "end_time   =  '23:59:59'\n",
    "start_date =  '2024-01-01'\n",
    "end_date   =  '2024-05-30'\n",
    "date_format_show =  '%Y-%m-%d'\n",
    "time_format_show =  '%H:%M'\n",
    "spec_date = pd.to_datetime('today').strftime(date_format_show)\n",
    "name_date = pd.to_datetime('today').strftime(\"%y%m%d\")\n",
    "\n",
    "# Define the timezones (UTC and Europe/Oslo)\n",
    "utc_timezone = pytz.timezone('UTC')\n",
    "oslo_timezone = pytz.timezone('Europe/Oslo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btfS2eEBG8ac"
   },
   "source": [
    "# Volda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9F3Zqe20opXy"
   },
   "source": [
    "## Bratteberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zGs7pEkRHapb"
   },
   "outputs": [],
   "source": [
    "school_name = 'bratteberg'\n",
    "manuf_id    = 'seklima'\n",
    "instr_id    = 'volda lufthamn'\n",
    "serial_id   = 'SN59680'\n",
    "input_file_directory    = platform_directory + '/' + folder_project + '/' + folder_raw   + '/' + folder_year + '/' + school_name + '/' + manuf_id + '/'\n",
    "export_file_directory   = platform_directory + '/' + folder_project + '/' + folder_ready + '/' + folder_year + '/' + school_name + '/' + manuf_id + '/'\n",
    "\n",
    "weather_column_array = [  instr_col_name,serial_col_name, datetime_name, pressure_name, temp_set_name,rel_hum_name, winds_name,  percipitation_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgM-WViwovMm"
   },
   "source": [
    "## Øyra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7r1oLsVgoxDI"
   },
   "outputs": [],
   "source": [
    "school_name = 'øyra'\n",
    "manuf_id    = 'seklima'\n",
    "instr_id    = 'volda lufthamn'\n",
    "serial_id   = 'SN59680'\n",
    "input_file_directory  = platform_directory + '/' + folder_project + '/' + folder_raw + '/' + folder_year + '/' + school_name + '/' + manuf_id + '/'\n",
    "export_file_directory   = platform_directory + '/'+ folder_project + '/' + folder_ready + '/' + folder_year + '/' + school_name + '/' + manuf_id + '/'\n",
    "\n",
    "weather_column_array = [  instr_col_name,serial_col_name, datetime_name, pressure_name, temp_set_name,rel_hum_name, winds_name,  percipitation_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path1 = input_file_directory # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith('.xlsx')] # get list of csv files\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_excel(file_path  ) # read csv file into a dataframe delimiter=';', \n",
    "    dfs1.append(df) # append dataframe to list\n",
    "\n",
    "# concatenate dataframes in the list into a single dataframe\n",
    "DF1 = pd.concat(dfs1, ignore_index=True).iloc[:-1]\n",
    "DF1.columns = weather_column_array\n",
    "DF1[datetime_name] = pd.to_datetime((DF1[datetime_name]), format= '%d.%m.%Y %H:%M')\n",
    "timezone = DF1[datetime_name].dt.tz_localize(oslo_timezone,  ambiguous='NaT', nonexistent='shift_forward').dt.tz_convert(utc_timezone)\n",
    "DF1[datetime_name] =  timezone.dt.tz_localize(None)\n",
    "\n",
    "#DF1.set_index(datetime_name, inplace=True)\n",
    "DF1.iloc[:,3:] = DF1.iloc[:,3:].replace('-', regex=True).astype(float).interpolate(method='linear')  #to handle minus string values. tell pandas to read it as minus value, not a string typing\n",
    "\n",
    "DF1[instr_col_name] = instr_id  \n",
    "DF1[school_col_name] = school_name\n",
    "DF1[serial_col_name] = serial_id   \n",
    "DF1[manuf_col_name] = manuf_id\n",
    "DF1[date_name] = DF1[datetime_name ].dt.strftime(date_format_show)\n",
    "DF1[time_name] = DF1[datetime_name ].dt.strftime(time_format_show)\n",
    "\n",
    "#we move columns named 'Date' and 'Time' to the front side\n",
    "cols_to_move = [school_col_name, date_name, time_name, manuf_col_name, instr_col_name, serial_col_name ]\n",
    "DF1         = DF1[ cols_to_move + [ col for col in DF1.columns if col not in cols_to_move ] ].drop([datetime_name], axis=1).dropna()\n",
    "\n",
    "#put spesific Directory and File Name.\n",
    "name_time = datetime.strptime((DF1[time_name].iloc[0]),'%H:%M').strftime('%H%M')\n",
    "name_date = datetime.strptime((DF1[date_name].iloc[0]),'%Y-%m-%d').strftime('%Y%m%d')\n",
    "\n",
    "directory_path = export_file_directory+'/'\n",
    "filename = school_name+ '_'+ instr_id + '_'+serial_id +'_'+name_date +'_'+ name_time + '.csv'\n",
    "\n",
    "# save dataframe (accumulative format) to CSV file in specified directory\n",
    "DF1.to_csv(directory_path + filename, index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrKCtaesFqy1"
   },
   "source": [
    "# Oslo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqN1TrrYN3da"
   },
   "source": [
    "## Brannfjell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_name = 'brannfjell'\n",
    "manuf_id    = 'seklima'\n",
    "instr_id = 'Oslo - Blindern'\n",
    "instr_id2   = ' & nilu manglerud'\n",
    "serial_id = 'SN18700'\n",
    "input_file_directory  = platform_directory + '/' + folder_project + '/' + folder_raw + '/' + folder_year + '/' + school_name + '/' + manuf_id + '/'\n",
    "\n",
    "weather_column_array = [ instr_col_name,serial_col_name, datetime_name, temp_set_name, rel_hum_name, winds_name, percipitation_name, pressure_name, sun_rad_name]\n",
    "pollution_column_array = [datetime_name,pm10_name,pm25_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHLciD7ZNRyz"
   },
   "source": [
    "## KubenVGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_name = 'kubenVGS'\n",
    "manuf_id    = 'seklima'\n",
    "instr_id = 'Oslo - Blindern'\n",
    "instr_id2   = ' & nilu alnabru'\n",
    "serial_id = 'SN18700'\n",
    "input_file_directory  = platform_directory + '/' + folder_project + '/' + folder_raw + '/' + folder_year + '/' + school_name + '/' + manuf_id + '/'\n",
    "\n",
    "weather_column_array = [ instr_col_name,serial_col_name, datetime_name, temp_set_name, rel_hum_name, winds_name, percipitation_name, pressure_name, sun_rad_name]\n",
    "pollution_column_array = [datetime_name,pm10_name,pm25_name, gas_nox_name, gas_no2_name, gas_no_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KubenYA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_name = 'kubenFS'\n",
    "manuf_id    = 'seklima'\n",
    "instr_id = 'Oslo - Blindern'\n",
    "instr_id2   = ' & nilu alnabru'\n",
    "serial_id = 'SN18700'\n",
    "input_file_directory  = platform_directory + '/' + folder_project + '/' + folder_raw + '/' + folder_year + '/' + school_name + '/' + manuf_id + '/'\n",
    "\n",
    "weather_column_array = [ instr_col_name,serial_col_name, datetime_name, temp_set_name, rel_hum_name, winds_name, percipitation_name, pressure_name, sun_rad_name]\n",
    "pollution_column_array = [datetime_name,pm10_name,pm25_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path1 = input_file_directory # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith('.xlsx')] # get list of csv files\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_excel(file_path  ) # read csv file into a dataframe delimiter=';', \n",
    "    dfs1.append(df) # append dataframe to list\n",
    "\n",
    "# concatenate dataframes in the list into a single dataframe\n",
    "DF1 = pd.concat(dfs1, ignore_index=True).iloc[:-1]\n",
    "\n",
    "DF1.columns = weather_column_array\n",
    "\n",
    "DF1[datetime_name] = pd.to_datetime((DF1[datetime_name]), format= '%d.%m.%Y %H:%M')\n",
    "timezone = DF1[datetime_name].dt.tz_localize(oslo_timezone,  ambiguous='NaT', nonexistent='shift_forward').dt.tz_convert(utc_timezone)\n",
    "DF1[datetime_name] =  timezone.dt.tz_localize(None)\n",
    "\n",
    "#special for Oslo\n",
    "DF1.set_index(datetime_name, inplace=True)\n",
    "# DFsolar= DF1[DF1[serial_col_name]!=serial_id][sun_rad_name].iloc[2:].reset_index().set_index(datetime_name).applymap(lambda x: max(0, x))\n",
    "# DF1 =   DF1[DF1[serial_col_name]==serial_id].drop(sun_rad_name, axis=1).join(DFsolar, how='outer').dropna(axis=0).iloc[1:]\n",
    "DF1.iloc[:,2:] = DF1.iloc[:,2:].replace('-', regex=True).astype(float).interpolate(method='linear')  #to handle minus string values. tell pandas to read it as minus value, not a string typing\n",
    "DF1#.mean() #.info()\n",
    "\n",
    "\n",
    "#importing data from NILU HISTORIKK DATA\n",
    "manuf_id2    = 'niluaqmd'\n",
    "input_file_directory2  = platform_directory + '/' + folder_project + '/' + folder_raw + '/' + folder_year + '/' + school_name + '/' + manuf_id2 + '/'\n",
    "folder_path2 = input_file_directory2 # replace with the folder path\n",
    "csv_files2 = [f for f in os.listdir(folder_path2) if f.endswith('.csv')] # get list of csv files\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files2:\n",
    "    file_path = os.path.join(folder_path2, file) # get full path of csv file\n",
    "    df = pd.read_csv(file_path,delimiter=\";\", header=3)#.iloc[5,:] # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list\n",
    "\n",
    "# concatenate dataframes in the list into a single dataframe\n",
    "DF2 = pd.concat(dfs1, ignore_index=True)\n",
    "DF2 = DF2.drop(columns=DF2.filter(regex=f'Dekning', axis=1).columns, axis=1)\n",
    "DF2.columns= pollution_column_array\n",
    "\n",
    "DF2[datetime_name] = pd.to_datetime((DF2[datetime_name]), format= '%d.%m.%Y %H:%M')\n",
    "timezone = DF2[datetime_name].dt.tz_localize(oslo_timezone,  ambiguous='NaT', nonexistent='shift_forward').dt.tz_convert(utc_timezone)\n",
    "DF2[datetime_name] =  timezone.dt.tz_localize(None)\n",
    "\n",
    "DF2 = DF2.set_index(datetime_name)\n",
    "DF2 = DF2.astype(str).replace(',', '.', regex=True).astype(float)#.applymap(lambda x: max(0, x))#.astype(float)\n",
    "DF2 = DF2.interpolate(method='linear').applymap(lambda x: max(0, x))\n",
    "\n",
    "#JOINING SEKLIMA AND NILU HISTORIKK DATA\n",
    "DF3 = DF1.join(DF2, how='outer').dropna()\n",
    "DF3 = DF3.drop([instr_col_name,serial_col_name],axis=1).resample('1H').mean().reset_index()\n",
    "\n",
    "DF3[instr_col_name] = instr_id  + instr_id2 \n",
    "DF3[school_col_name] = school_name\n",
    "DF3[serial_col_name] = serial_id   \n",
    "DF3[manuf_col_name] = manuf_id + '+' + manuf_id2\n",
    "DF3[date_name] = DF3[datetime_name ].dt.strftime(date_format_show)\n",
    "DF3[time_name] = DF3[datetime_name ].dt.strftime(time_format_show)\n",
    "DF3\n",
    "\n",
    "#we move columns named 'Date' and 'Time' to the front side\n",
    "cols_to_move = [school_col_name, date_name, time_name, manuf_col_name, instr_col_name, serial_col_name  ,   temp_set_name, rel_hum_name, winds_name  ]\n",
    "DF3         = DF3[ cols_to_move + [ col for col in DF3.columns if col not in cols_to_move ] ].drop([datetime_name], axis=1).dropna()\n",
    "\n",
    "DF3.iloc[:,6:]=DF3.iloc[:,6:].astype(float)#.describe()\n",
    "\n",
    "#put spesific Directory and File Name.\n",
    "name_time = datetime.strptime((DF3[time_name].iloc[0]),'%H:%M').strftime('%H%M')\n",
    "name_date = datetime.strptime((DF3[date_name].iloc[0]),'%Y-%m-%d').strftime('%Y%m%d')\n",
    "\n",
    "export_file_directory   = platform_directory + '/'+ folder_project + '/' + folder_ready + '/' + folder_year + '/'  + school_name + '/' + manuf_id + '/'\n",
    "directory_path = export_file_directory+'/'\n",
    "filename = school_name+ '_'+ manuf_id + '_'+serial_id +'_'+name_date +'_'+ name_time + '.csv'\n",
    "\n",
    "# save dataframe (accumulative format) to CSV file in specified directory\n",
    "DF3.to_csv(directory_path + filename, index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF3"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOYJ07KTLjWEl/tAQGB/o2C",
   "mount_file_id": "1P5IOtxAkOr2boEMjAxddpY9-z28aEu5P",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "DIGGMINSKOLE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
