{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-I0eo41eyleK"
   },
   "source": [
    "**WRITER'S IDENTITY**\n",
    "- AZIMIL GANI ALAM\n",
    "- Ph.D Student\n",
    "- Dept. Energy & Process Tech - NTNU\n",
    "\n",
    "\n",
    "**NOTE :**\n",
    "- This Python File runs in GoogleColab\n",
    "- This work uses **All Data**\n",
    "- we want to combine all data results from CSV models become process-ready for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1726741476510,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "7UnAXxrzzTVd"
   },
   "outputs": [],
   "source": [
    "folder_project = 'Project'\n",
    "platform_directory = 'C:/Users/azimilga/My Drive/Colab Notebooks'   # NTNU Laptop\n",
    "platform_directory = 'G:/My Drive/Colab Notebooks'                  # NILU Laptop\n",
    "# platform_directory = '/content/drive/MyDrive/Colab Notebooks'     # Google Collab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_IofPUjjeHs"
   },
   "source": [
    "# Select Notebook Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55653,
     "status": "ok",
     "timestamp": 1726741368040,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "sNN6UG20F95f",
    "outputId": "374eb245-af6b-4907-842a-f98984dbae27"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10782,
     "status": "ok",
     "timestamp": 1726741395924,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "6Xr_6169znxX",
    "outputId": "6fae76d3-3286-4d6f-98f2-fd5e423aeb34"
   },
   "outputs": [],
   "source": [
    "! pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mut4f2nkycvj"
   },
   "source": [
    "# **Library Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1726741439076,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "fjhmov-c_PhT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind,f_oneway, chi2_contingency\n",
    "import statistics\n",
    "import re\n",
    "import shap\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "from collections import Counter\n",
    "#from imblearn.under_sampling import RandomUnderSampler\n",
    "#from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "import xgboost\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k8zB7YMWQH2"
   },
   "source": [
    "# Default  Names & Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_Nht7mEvuKP"
   },
   "source": [
    "and then this is highly important : resampling option : every how many hour?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1726742295926,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "E5Af39MDxyLG"
   },
   "outputs": [],
   "source": [
    "# tell resample data\n",
    "time_series_resample = '10min'   #for processor 1\n",
    "survey_resample      = '10min'    #for processor 2\n",
    "desample_option      = '10min'\n",
    "minutely_resample    = '1min'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1726742296507,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "A2zIqDgVmqZM"
   },
   "outputs": [],
   "source": [
    "#filtering rows data regarding to desired specific time\n",
    "start_time = '00:00:00'\n",
    "end_time   = '23:59:59'\n",
    "start_date = '2023-03-06'\n",
    "end_date   =  '2023-05-30'\n",
    "folder_year    = '2023'\n",
    "\n",
    "# Folder Naming\n",
    "folder_raw     = 'data-raw'\n",
    "folder_ready   = 'data-ready'\n",
    "folder_pivot   = 'pivot'\n",
    "\n",
    "folder_export  = 'export'\n",
    "folder_is      = 'niluapp'\n",
    "data_for1      = 'combined panel'\n",
    "data_for2      = 'combined longitudinal'\n",
    "word_indoor    = 'Indoor '\n",
    "word_outdoor   = 'Outdoor '\n",
    "\n",
    "\n",
    "\n",
    "spec_date = pd.to_datetime('today').strftime(\"%Y-%m-%d\")\n",
    "#name_date = pd.to_datetime('today').strftime(\"%y%m%d\")\n",
    "date_format_show =  '%Y-%m-%d'\n",
    "time_format_show =  '%H:%M'\n",
    "start_day  = 0\n",
    "end_day    = 4\n",
    "start_hour = 7\n",
    "end_hour   = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_0YNmLLxyLG"
   },
   "source": [
    "### sub setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1726742296507,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "aBd2ro3cy7vq"
   },
   "outputs": [],
   "source": [
    "col_header = ['title', 'sessionid', 'date', 'time' ]\n",
    "response_counting = ['responses']\n",
    "is_col_main  =  ['feel_air','feel_temp','feel_health','feel_bright','feel_noise']\n",
    "is_col_main1  = ['feel_air1','feel_temp1','feel_health1','feel_bright1','feel_noise1'] #\n",
    "is_col_main2  = ['feel_air2','feel_temp2','feel_health2','feel_bright2','feel_noise2'] #\n",
    "is_col_main3  = ['feel_air3','feel_temp3','feel_health3','feel_bright3','feel_noise3'] #\n",
    "is_col_main4  = ['feel_air4','feel_temp4','feel_health4','feel_bright4','feel_noise4'] #\n",
    "\n",
    "is_main_air    = ['feel_air1', 'feel_air2', 'feel_air3', 'feel_air4']\n",
    "is_main_temp   = ['feel_temp1', 'feel_temp2', 'feel_temp3', 'feel_temp4']\n",
    "is_main_bright = ['feel_bright1', 'feel_bright2', 'feel_bright3', 'feel_bright4']\n",
    "is_main_health = ['feel_health1', 'feel_health2', 'feel_health3', 'feel_health4']\n",
    "is_main_noise  = ['feel_noise1', 'feel_noise2', 'feel_noise3', 'feel_noise4']\n",
    "\n",
    "is_col_air   = ['air_smell','air_heavy','air_dry','air_dust','air_electshock']\n",
    "is_col_temp  = ['temp_coldhot','temp_draw','temp_coldfloor','temp_heatsun','temp_heater' ]\n",
    "is_col_bright = ['bright_sun','bright_lamp_hi','bright_lamp_low']\n",
    "is_col_health  = ['health_head','health_cough','health_tired','health_dryskin']\n",
    "is_col_multi  = ['feel_air','feel_temp','feel_health','feel_bright','feel_noise','temp_coldhot']\n",
    "is_col_subquestion = is_col_air+ is_col_temp + is_col_bright + is_col_health\n",
    "is_col_subquestion_coldhot = is_col_air+ is_col_temp + is_col_bright + is_col_health + ['too_cold', 'too_hot']\n",
    "pd_col       = ['rd_air', 'rd_temp', 'rd_health', 'rd_bright', 'rd_noise']\n",
    "columns_formation   = col_header + is_col_main + is_col_air + is_col_temp + is_col_bright + is_col_health\n",
    "\n",
    "is_main_ad = ['ad_air','ad_temp','ad_health','ad_bright','ad_noise']\n",
    "is_main_as = ['as_air','as_temp','as_health','as_bright','as_noise']\n",
    "is_main_rd = ['rd_air','rd_temp','rd_health','rd_bright','rd_noise']\n",
    "is_main_rs = ['rs_air','rs_temp','rs_health','rs_bright','rs_noise']\n",
    "\n",
    "school_col_name   = 'school_id'\n",
    "room_col_name     = 'room_id'\n",
    "date_name         = 'date'\n",
    "time_name         = 'time'\n",
    "manuf_col_name    = 'manuf_id'\n",
    "instr_col_name    = 'instr_id'\n",
    "serial_col_name   = 'serial_id'\n",
    "\n",
    "#Give a name for new column in purpose to count how many response / hour or response / day\n",
    "response_counting = 'responses'\n",
    "\n",
    "#columns_formation = ['title', 'sessionid', 'date', 'time' , 'Room Air', 'Room Temperature', 'Room Lighting', 'Health Feeling', 'Room Noice', 'BAD SMELL' , 'BAD HEAVY/AIR', 'DRY AIR', 'DUST AND DIRT', 'ELECTRIC SHOCK', '(-COLD) OR HOT', 'DRAWING COLD AIR', 'COLD ON THE FLOOR', 'MUCH HEAT FROM SUNSHINE', 'MUCH HEAT FROM FURNACES', 'LIGHT FROM THE SUN', 'BRIGHT LIGHT CEILING LAMPS', 'WEAK LIGHT CEILING LAMPS', 'HEADACHE', 'COUGH/SHORENESS', 'TIRED/UNCONCENTRATED', 'DRY EYES/HANDS']\n",
    "\n",
    "word_indoor   = 'Indoor '\n",
    "word_outdoor  = 'Outdoor '\n",
    "temp_v_label  = word_indoor + 'Temp. by Vent system (*C)'\n",
    "co2_v_label   = word_indoor+ 'CO2 level by Vent System (ppm)'\n",
    "vent_v_label  = word_indoor+ 'Supply Air (CMH)'\n",
    "swc_f_label   = word_indoor+ 'Floor Heater -On/Off'\n",
    "swc_r_label   = word_indoor+ 'Baseboard Heater-On/Off'\n",
    "temp_f_label  = word_indoor+ 'Floor Temperature (*C)'\n",
    "\n",
    "temp_a_label  = word_indoor+ 'Temp. by add. sensors (*C)'\n",
    "rh_a_label    = word_indoor+ 'Relative Humidity (%)'\n",
    "co2_a_label   = word_indoor+ 'CO2 level by add. sensors (ppm)'\n",
    "voc_a_label   = word_indoor+ 'VOC contaminant level (ppb)'\n",
    "bright_a_label= word_indoor+ 'Luminous Intensity (%)'\n",
    "sound_a_label = word_indoor+ 'Sound Pressure Intensity (dBA)'\n",
    "pm25_a_label  = word_indoor+ 'PM2.5 density (ug/m3)'\n",
    "pm1_a_label   = word_indoor+ 'PM1.0 density (ug/m3)'\n",
    "rdn_a_label   = word_indoor+ 'Radon Level (Bq/m3)'\n",
    "hura_a_label  = word_indoor+ 'Humidity Ratio (g/kg air)'\n",
    "press_a_label = word_indoor+ 'Air pressure (hPa)'\n",
    "enth_a_label  = word_indoor+ 'entalphy (kJ/kg)'\n",
    "\n",
    "temp_o_label  = word_outdoor+ 'Temperature (*C)'\n",
    "rh_o_label    = word_outdoor+ 'Relative Humidity (%)'\n",
    "winds_o_label = word_outdoor+ 'Wind Speed (m/s)'\n",
    "sun_o_label   = word_outdoor+ 'Mean Global Radiation (W/m2)'\n",
    "pm25_o_label  = word_outdoor+ 'PM2.5 density (ug/m3)'\n",
    "pm10_o_label  = word_outdoor+ 'PM10 density (ug/m3)'\n",
    "enth_o_label  = word_outdoor+ 'entalphy (kJ/kg)'\n",
    "\n",
    "hura_d_label = 'delta Indoor-Outdoor Humidity Ratio (g/kg air)'\n",
    "enth_d_label = 'delta Indoor-Outdoor entalphy (kJ/kg)'\n",
    "hour_label = 'Datetime : Hour'\n",
    "\n",
    "answer_replace2 = {0:0,2:1 , 1:1, -1:-1, -2:-1}\n",
    "answer_replace01   = {0:0,-1:1, -2:1, 1:0, 2:0}\n",
    "answer_replace012  = {0:0,-1:1, -2:2, 1:0, 2:0}\n",
    "answer_replace0123 = {0:0,-1:2, -2:3, 1:1, 2:0}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "co2_name          = 'co2_a'\n",
    "temp_data_name    = 'temp_a'\n",
    "rel_hum_name      = 'rh_a'\n",
    "voc_name          = 'voc_a'\n",
    "bright_name       = 'bright_a'\n",
    "sound_name        = 'sound_a'\n",
    "pm25_name         = 'pm2.5_a'\n",
    "pm1_name          = 'pm1_a'\n",
    "radon_name        = 'rdn_a'\n",
    "press_name        = 'press_a'\n",
    "enthalpy_name     = 'enth_a'\n",
    "humid_ratio_name  = 'hura_a'\n",
    "pmv_ppd_name      = [ 'pmv', 'ppd']\n",
    "\n",
    "airthingsAll  = [ 'co2_a','temp_a', 'rh_a', 'press_a','enth_a', 'hura_a', 'voc_a','bright_a',  'rdn_a','sound_a', 'pm1_a', 'pm2.5_a']\n",
    "# airthingsPlus = [ 'co2_a','temp_a', 'rh_a', 'press_a','enth_a', 'hura_a', 'voc_a','bright_a', 'rdn_a']\n",
    "# airthingsPro  = [ 'co2_a','temp_a', 'rh_a', 'press_a','enth_a', 'hura_a', 'voc_a','bright_a', 'sound_a', 'pm1_a', 'pm2.5_a']\n",
    "# airthingsSmall = [ 'co2_a','temp_a', 'rh_a', 'press_a','enth_a', 'hura_a', 'voc_a','bright_a']\n",
    "weather_volda = [ 'temp_o', 'rh_o', 'winds_o', 'rain_o','enth_o', 'hura_o', 'press_o']\n",
    "weather_oslo  = [ 'temp_o', 'rh_o', 'winds_o', 'rain_o','enth_o', 'hura_o', 'press_o', 'sun_o', 'pm2.5_o', 'pm10_o']\n",
    "\n",
    "airthingsSmall = [co2_name, temp_data_name, rel_hum_name, press_name, enthalpy_name, humid_ratio_name, voc_name, bright_name]\n",
    "airthingsPlus = airthingsSmall + [ radon_name] +pmv_ppd_name\n",
    "airthingsPro  = airthingsSmall + [  sound_name, pm1_name, pm25_name] + pmv_ppd_name\n",
    "weather_volda = [ 'temp_o', 'rh_o', 'winds_o', 'rain_o','enth_o', 'hura_o', 'press_o']\n",
    "weather_oslo  = weather_volda + [ 'sun_o', 'pm2.5_o', 'pm10_o']\n",
    "weather_extra = [ 'nox_o', 'no2_o', 'no_o']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_room_features(df, room_data):\n",
    "    room_df = pd.DataFrame(room_data)\n",
    "    return df.merge(\n",
    "        room_df,\n",
    "        how='left',\n",
    "        on='room_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crMceS7W6IAG"
   },
   "source": [
    "Time Zone Changing (GMT to OSLO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1726742296507,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "CLrjONZL6N9w"
   },
   "outputs": [],
   "source": [
    " #convert time format to datetime  : time_zone_converter (df, 'Zone Origin', 'Zone Destination')\n",
    "def time_zone_converter(df, zone_origin, zone_destination):\n",
    "  #df['datetime'] = pd.to_datetime((df['date'] + ' ' + df['time']), format='%Y-%m-%d %H:%M:%S')\n",
    "  #df['DateTime'] = datechange.dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "  df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "  df['date'] = df['datetime'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "  # Define the timezones (GMT and Europe/Oslo)\n",
    "  origin_timezone = pytz.timezone(zone_origin)\n",
    "  destination_timezone = pytz.timezone(zone_destination)\n",
    "\n",
    "  # Adjust the datetime to the Oslo timezone and account for daylight saving time\n",
    "  df['adjusted datetime'] = df['datetime'].dt.tz_localize(origin_timezone).dt.tz_convert(destination_timezone)\n",
    "\n",
    "  # Extract the adjusted time\n",
    "  df['time'] = df['adjusted datetime'].dt.strftime('%H:%M')\n",
    "  df['datetime'] = df['adjusted datetime'] #.dt.strftime('%Y-%m-%d %H:%M')\n",
    "  df.drop(['adjusted datetime'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1726742296507,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "WJUUXxUeQqb9"
   },
   "outputs": [],
   "source": [
    "#plotting graphs\n",
    "def plotter (df, y_axis, y_axis_label, pd_check, pd_check_label, response_counting, x_function, y_function):\n",
    "    plt.scatter(df.reset_index()[y_axis], df.reset_index()[pd_check]*100, s= df.reset_index()[response_counting])\n",
    "    plt.ylabel (pd_check_label)\n",
    "    plt.xlabel (y_axis_label)\n",
    "    plt.yscale(y_function)\n",
    "    plt.xscale(x_function)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "#plotting graphs\n",
    "def plotter_noresponse (df, y_axis, y_axis_label, pd_check, pd_check_label, x_function, y_function):\n",
    "    plt.scatter(df.reset_index()[y_axis], df.reset_index()[pd_check])\n",
    "    plt.ylabel (pd_check_label)\n",
    "    plt.xlabel (y_axis_label)\n",
    "    plt.yscale(y_function)\n",
    "    plt.xscale(x_function)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def plotter_eachclasses (df, y_axis, y_axis_label, pd_check, pd_check_label, response_counting, x_function, y_function):\n",
    "    for first_index, data in df.groupby(level=0):\n",
    "     # Extract x and y data for the scatter plot\n",
    "        x_data = data[y_axis]\n",
    "        y_data = data[pd_check]\n",
    "        s_data = data[response_counting]# Replace 'YourColumn' with your column name\n",
    "    # Plot a scatter graph for each subset of data\n",
    "        plt.scatter(x_data, y_data, s=s_data , label=first_index ) #, label=first_index\n",
    "    plt.ylabel (pd_check_label)\n",
    "    plt.xlabel (y_axis_label)\n",
    "    plt.yscale(y_function)\n",
    "    plt.xscale(x_function)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def dissatisfaction_rate (df, col, coef, dis_name, satis_name ):\n",
    "    df[dis_name] = ((df[col[3]] + coef * df[col[2]]) / (df[col[3]] + df[col[2]] + df[col[1]] + df[col[0]])).round(3)\n",
    "    df[satis_name] = ((df[col[0]] + coef * df[col[1]]) / (df[col[3]] + df [col[2]] + df[col[1]] + df[col[0]])).round(3)\n",
    "\n",
    "\n",
    "def dissatisfaction_rate2 (df, col, coef, dis_name, satis_name ):\n",
    "    df[dis_name] = ((df[col[3]] + coef * df[col[2]]) / (df['students'])).round(3)\n",
    "    df[satis_name] = ((df[col[0]] + coef * df[col[1]]) / (df['students'])).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1726742296507,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "s9uAirlTQqb-"
   },
   "outputs": [],
   "source": [
    "# Create a heatmap for the pearsons coefficient\n",
    "def heatmap_corrvalue (df, corr_method , size , title, font_size):\n",
    "    corr = df.corr(method=corr_method).round(2)   #, 'feel_bright'\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=size)\n",
    "\n",
    "    plt.xticks(rotation=90, fontsize=font_size)\n",
    "    plt.title('correlation map : '+ '(' + corr_method + ') '+title , fontsize=12)\n",
    "    plt.yticks(fontsize=font_size,rotation=90 )\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220,10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask,cmap=\"coolwarm\", vmin=-1, vmax=1, center=0, cbar=True, annot =True,\n",
    "                square=True, linewidths=.5, annot_kws={\"size\": (font_size-2)})\n",
    "\n",
    "\n",
    "# Create a heatmap for the p-values\n",
    "def heatmap_pvalue_noreplace (df, categorical_column, continuous_column):\n",
    "\n",
    "    # Get categorical and continuous column names\n",
    "    categorical_columns = df[categorical_column].columns  # mentioning list of categorical\n",
    "    continuous_columns = df[continuous_column].columns  # mentioning list of continuous\n",
    "\n",
    "    # Initialize an empty matrix to store p-values\n",
    "    num_cat_cols = len(categorical_columns)\n",
    "    num_cont_cols = len(continuous_columns)\n",
    "    p_values = np.zeros((num_cont_cols, num_cat_cols))\n",
    "\n",
    "    # Perform ANOVA tests for each categorical column against each continuous column\n",
    "    for i, cat_col in enumerate(categorical_columns):  # Iterate through categorical columns\n",
    "        for j, cont_col in enumerate(continuous_columns):  # Iterate through continuous columns\n",
    "            groups = [df[df[cat_col] == val][cont_col] for val in df[cat_col].unique()]\n",
    "            f_stat, p_val_anova = f_oneway(*groups)\n",
    "            p_values[j, i] = p_val_anova.round(4)\n",
    "\n",
    "    sns.heatmap(p_values, cmap=\"rocket\" , vmin=0, vmax=0.100, center=0.03,\n",
    "                cbar=True, annot =True, square=False, linewidths=0.5\n",
    "                #,annot_kws={\"size\": 7}\n",
    "                )\n",
    "    # Customize plot properties\n",
    "    #plt.colorbar(heatmap, label='p-value')\n",
    "    plt.title('ANOVA p-values Heatmap')\n",
    "    plt.ylabel('measured parameters')\n",
    "    plt.xlabel('user feedback')\n",
    "    #plt.yticks(np.arange(num_cont_cols), rotation=0)\n",
    "    #plt.xticks(np.arange(num_cat_cols))\n",
    "    plt.yticks(np.arange(0.5,num_cont_cols), continuous_columns\n",
    "               #,size = 8\n",
    "               ,rotation=0\n",
    "               )\n",
    "    plt.xticks(np.arange(0.5,num_cat_cols), categorical_columns\n",
    "               #,size = 8\n",
    "               ,horizontalalignment='center',rotation=15\n",
    "               )\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    print(p_values)\n",
    "\n",
    "\n",
    "# Create a heatmap for the p-values\n",
    "def heatmap_pvalue_2cat (input_df, categorical_column, continuous_column, figsize):\n",
    "    df = input_df.copy()\n",
    "    answer_replace2 = {0:0,2:1 , 1:1, -1:-1, -2:-1}\n",
    "    df.loc[:,categorical_column] = df.loc[:,categorical_column].replace(answer_replace2)\n",
    "\n",
    "    # Get categorical and continuous column names\n",
    "    categorical_columns = df[categorical_column].columns  # mentioning list of categorical\n",
    "    continuous_columns = df[continuous_column].columns  # mentioning list of continuous\n",
    "\n",
    "    # Initialize an empty matrix to store p-values\n",
    "    num_cat_cols = len(categorical_columns)\n",
    "    num_cont_cols = len(continuous_columns)\n",
    "    p_values = np.zeros((num_cont_cols, num_cat_cols)).round(3)\n",
    "\n",
    "    # Perform ANOVA tests for each categorical column against each continuous column\n",
    "    for i, cat_col in enumerate(categorical_columns):  # Iterate through categorical columns\n",
    "        for j, cont_col in enumerate(continuous_columns):  # Iterate through continuous columns\n",
    "            groups = [df[df[cat_col] == val][cont_col] for val in df[cat_col].unique()]\n",
    "            f_stat, p_val_anova = f_oneway(*groups)\n",
    "            p_values[j, i] = p_val_anova\n",
    "\n",
    "    p_values = p_values.round(3)\n",
    "    plt.figure(figsize=figsize)  #figsize=(5 , 8)\n",
    "    #plt.figure(figsize=(4,6))\n",
    "    sns.heatmap(p_values, cmap=\"rocket\" , vmin=0, vmax=0.100, center=0.03,\n",
    "                cbar=True, annot =True, square=False, linewidths=0.5\n",
    "                #,annot_kws={\"size\": 7}\n",
    "                )\n",
    "    # Customize plot properties\n",
    "    #plt.colorbar(heatmap, label='p-value')\n",
    "    plt.title('ANOVA p-values Heatmap')\n",
    "    plt.ylabel('measured parameters')\n",
    "    plt.xlabel('user feedback')\n",
    "    #plt.yticks(np.arange(num_cont_cols), rotation=0)\n",
    "    #plt.xticks(np.arange(num_cat_cols))\n",
    "    plt.yticks(np.arange(0.5,num_cont_cols), continuous_columns\n",
    "               #,size = 8\n",
    "               ,rotation=0\n",
    "               )\n",
    "    plt.xticks(np.arange(0.5,num_cat_cols), categorical_columns\n",
    "               #,size = 8\n",
    "               ,horizontalalignment='center',rotation=45\n",
    "               )\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    print(p_values)\n",
    "\n",
    "\n",
    "# Create a heatmap for the p-values\n",
    "def heatmap_pvalue_4cat (df, categorical_column, continuous_column,figsize):\n",
    "\n",
    "    # Get categorical and continuous column names\n",
    "    categorical_columns = df[categorical_column].columns  # mentioning list of categorical\n",
    "    continuous_columns = df[continuous_column].columns  # mentioning list of continuous\n",
    "\n",
    "    # Initialize an empty matrix to store p-values\n",
    "    num_cat_cols = len(categorical_columns)\n",
    "    num_cont_cols = len(continuous_columns)\n",
    "    p_values = np.zeros((num_cont_cols, num_cat_cols)).round(3)\n",
    "\n",
    "    # Perform ANOVA tests for each categorical column against each continuous column\n",
    "    for i, cat_col in enumerate(categorical_columns):  # Iterate through categorical columns\n",
    "        for j, cont_col in enumerate(continuous_columns):  # Iterate through continuous columns\n",
    "            groups = [df[df[cat_col] == val][cont_col] for val in df[cat_col].unique()]\n",
    "            f_stat, p_val_anova = f_oneway(*groups)\n",
    "            p_values[j, i] = p_val_anova\n",
    "    p_values = p_values.round(3)\n",
    "    plt.figure(figsize=figsize)\n",
    "    #plt.figure(figsize=(4,6))\n",
    "    sns.heatmap(p_values, cmap=\"rocket\" , vmin=0, vmax=0.100, center=0.03,\n",
    "                cbar=True, annot =True, square=False, linewidths=0.5\n",
    "                #,annot_kws={\"size\": 7}\n",
    "                )\n",
    "    # Customize plot properties\n",
    "    #plt.colorbar(heatmap, label='p-value')\n",
    "    plt.title('ANOVA p-values Heatmap')\n",
    "    plt.ylabel('measured parameters')\n",
    "    plt.xlabel('user feedback')\n",
    "    #plt.yticks(np.arange(num_cont_cols), rotation=0)\n",
    "    #plt.xticks(np.arange(num_cat_cols))\n",
    "    plt.yticks(np.arange(0.5,num_cont_cols), continuous_columns,\n",
    "               #size = 8,\n",
    "               rotation=0\n",
    "               )\n",
    "    plt.xticks(np.arange(0.5,num_cat_cols), categorical_columns,\n",
    "               #size = 8,\n",
    "               horizontalalignment='center',rotation=45\n",
    "               )\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    print(p_values)\n",
    "\n",
    "def heatmap_pvalue_subquest (df, categorical_column, continuous_column, figsize):\n",
    "\n",
    "    answer_replace2 = {0:'No', 1:'Yes'}\n",
    "    df.loc[:,categorical_column] = df.loc[:,categorical_column].fillna(0).replace(answer_replace2)\n",
    "\n",
    "    # Get categorical and continuous column names\n",
    "    categorical_columns = df[categorical_column].columns  # mentioning list of categorical\n",
    "    continuous_columns  = df[continuous_column].columns  # mentioning list of continuous\n",
    "\n",
    "    # Initialize an empty matrix to store p-values\n",
    "    num_cat_cols = len(categorical_columns)\n",
    "    num_cont_cols = len(continuous_columns)\n",
    "    p_values = np.zeros((num_cont_cols, num_cat_cols))\n",
    "\n",
    "    # Perform ANOVA tests for each categorical column against each continuous column\n",
    "    for i, cat_col in enumerate(categorical_columns):  # Iterate through categorical columns\n",
    "        for j, cont_col in enumerate(continuous_columns):  # Iterate through continuous columns\n",
    "            groups = [df[df[cat_col] == val][cont_col] for val in df[cat_col].unique()]\n",
    "            f_stat, p_val_anova = f_oneway(*groups)\n",
    "            p_values[j, i] = p_val_anova.round(decimals=3)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    sns.heatmap(p_values, cmap=\"rocket\" , vmin=0, vmax=0.100, center=0.03,\n",
    "                cbar=True, annot =True, square=False, linewidths=0.5\n",
    "                #,annot_kws={\"size\": 7}\n",
    "                )\n",
    "    # Customize plot properties\n",
    "    #plt.colorbar(heatmap, label='p-value')\n",
    "    plt.title('ANOVA p-values Heatmap')\n",
    "    plt.ylabel('measured parameters')\n",
    "    plt.xlabel('user feedback')\n",
    "    #plt.yticks(np.arange(num_cont_cols), rotation=0)\n",
    "    #plt.xticks(np.arange(num_cat_cols))\n",
    "    plt.yticks(np.arange(0.5,num_cont_cols), continuous_columns\n",
    "               #,size = 8\n",
    "               ,rotation=0\n",
    "               )\n",
    "    plt.xticks(np.arange(0.5,num_cat_cols), categorical_columns\n",
    "               #,size = 8\n",
    "               ,horizontalalignment='center',rotation=15\n",
    "               )\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    print(p_values)\n",
    "\n",
    "# Create a heatmap for the p-values\n",
    "def heatmap_chitest_2cat (df,answer_replace2, figsize):\n",
    "    df = df.replace(answer_replace2)\n",
    "    # Create a contingency table for each pair of columns and store the p-values in a DataFrame\n",
    "    p_values = pd.DataFrame(index=df.columns, columns=df.columns)\n",
    "\n",
    "    for col1 in df.columns:\n",
    "        for col2 in df.columns:\n",
    "            if col1 != col2:\n",
    "                contingency_table = pd.crosstab(df[col1], df[col2])\n",
    "                chi2, p, _, _ = chi2_contingency(contingency_table )\n",
    "                p_values.loc[col1, col2] = p\n",
    "\n",
    "    # Convert p-values to numeric for plotting\n",
    "    p_values = p_values.astype(float).round(3)\n",
    "\n",
    "    plt.figure(figsize=figsize)  #figsize=(5 , 8)\n",
    "    sns.heatmap(p_values, cmap=\"rocket\" , vmin=0, vmax=0.100, center=0.03,\n",
    "                cbar=True, annot =True, square=False, linewidths=0.5\n",
    "                ,annot_kws={\"size\": 7}\n",
    "                )\n",
    "    # Customize plot properties\n",
    "    #plt.colorbar(heatmap, label='p-value')\n",
    "    plt.title('chi2 test p-values Heatmap')\n",
    "    plt.ylabel('user feedback')\n",
    "    plt.xlabel('user feedback')\n",
    "    #plt.yticks(np.arange(num_cont_cols), rotation=0)\n",
    "    #plt.xticks(np.arange(num_cat_cols))\n",
    "    plt.yticks(size = 8,rotation=0\n",
    "               )\n",
    "    plt.xticks(size = 8,horizontalalignment='center',rotation=15\n",
    "               )\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    print(p_values)\n",
    "\n",
    "def rename_spec_columns(df, text_add ):\n",
    "    suffix = text_add\n",
    "    # Define a lambda function to rename columns\n",
    "    rename_func = lambda col: col + suffix if col.endswith('_a') or col.endswith('_o') else col\n",
    "\n",
    "    # Rename the columns using the lambda function\n",
    "    df = df.rename(columns=rename_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1726742296891,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "erQ4k90gQqb-"
   },
   "outputs": [],
   "source": [
    "def stat_test_boxplot (df, column_feel, column_value, column_label):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(y=column_feel, x=column_value , orient='h' ,data=df)\n",
    "    plt.title(column_feel + ' vs ' + column_value)\n",
    "    plt.xlabel(column_label)\n",
    "    plt.legend()\n",
    "    plt.ylabel(column_feel)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def stat_test_boxplot_hue (df, column_feel, column_value, column_label):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(y=column_feel, x=column_value , orient='h' , hue = 'time', data=df)\n",
    "    plt.title(column_feel + ' vs ' + column_value + ' [time separated]')\n",
    "    plt.xlabel(column_label)\n",
    "    plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "    plt.ylabel(column_feel)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def stat_test_sub2 (df, column_feel, column_value):\n",
    "# Statistical test: T-test (assuming two categories)\n",
    "    dfs = df #[df[column_value]>=0]\n",
    "    cat_ab = dfs[dfs[column_feel] == 'No'][column_value]\n",
    "    cat_cd = dfs[dfs[column_feel] == 'Yes' ][column_value]\n",
    "    t_stat, p_val = ttest_ind(cat_ab, cat_cd)\n",
    "    print(f\"T-test p-value [2cat]: {p_val}\")\n",
    "# Statistical test: ANOVA (for multiple categories)\n",
    "    f_stat, p_val_anova = f_oneway(cat_ab, cat_cd)\n",
    "    print(f\"ANOVA p-value [2cat]: {p_val_anova}\")\n",
    "\n",
    "def stat_test_main2 (df, column_feel, column_value):\n",
    "# Statistical test: T-test (assuming two categories)\n",
    "    dfs = df #[df[column_value]>=0]\n",
    "    cat_ab = dfs[dfs[column_feel] == 'dissatisf.'][column_value]\n",
    "    cat_cd = dfs[dfs[column_feel] == 'satisfied' ][column_value]\n",
    "    t_stat, p_val = ttest_ind(cat_ab, cat_cd)\n",
    "    print(f\"T-test p-value [2cat]: {p_val}\")\n",
    "# Statistical test: ANOVA (for multiple categories)\n",
    "    f_stat, p_val_anova = f_oneway(cat_ab, cat_cd)\n",
    "    print(f\"ANOVA p-value [2cat]: {p_val_anova}\")\n",
    "\n",
    "def stat_test_main4 (df, column_feel, column_value):\n",
    "# Statistical test: T-test (assuming two categories)\n",
    "    dfs = df #[df[column_value] >= 0 ].copy()\n",
    "    cat_a = dfs[dfs[column_feel] == (np.array(dfs[column_feel].unique()) [0])][column_value]\n",
    "    cat_b = dfs[dfs[column_feel] == (np.array(dfs[column_feel].unique()) [1])][column_value]\n",
    "    cat_c = dfs[dfs[column_feel] == (np.array(dfs[column_feel].unique()) [2])][column_value]\n",
    "    cat_d = dfs[dfs[column_feel] == (np.array(dfs[column_feel].unique()) [3])][column_value]\n",
    "    t_stat4, p_val4 = ttest_ind(cat_a, cat_d)\n",
    "    print(f\"T-test p-value [4cat]: {p_val4}\")\n",
    "    print('T-test between : ' + np.array(dfs[column_feel].unique()) [0] + ' and ' + np.array(dfs[column_feel].unique()) [3])\n",
    "# Statistical test: ANOVA (for multiple categories)\n",
    "    f_stat4, p_val_anova4 = f_oneway(cat_a, cat_b, cat_c, cat_d)\n",
    "    print(f\"ANOVA p-value [4cat]: {p_val_anova4}\")\n",
    "\n",
    "\n",
    "def stat_test (input_data, is_col_main, column_feel, column_value, value_label):\n",
    "    #recategorical as 2 categories\n",
    "    answer_replace2 = {0:'dont know',2:'satisfied' , 1:'satisfied', -1:'dissatisf.', -2:'dissatisf.'}\n",
    "    data_cat_2 = input_data[input_data[column_value]>=0].copy()#.loc[:,is_col_main+interpolate_col+['time']].reset_index(drop=True)\n",
    "    data_cat_2.loc[:,is_col_main] = input_data.loc[:,is_col_main].replace(answer_replace2)\n",
    "    data_cat_2['time'] = data_cat_2[['hour']].applymap(lambda x: 'morning' if x < 10 else 'afternoon')\n",
    "    stat_test_main2 (data_cat_2, column_feel, column_value)\n",
    "    stat_test_boxplot (data_cat_2, column_feel, column_value, value_label)\n",
    "    stat_test_boxplot_hue (data_cat_2, column_feel, column_value, value_label)\n",
    "    #recategorical as 4 categories\n",
    "\n",
    "    answer_replace4 = {0:'dont know',2:'Best (2)' , 1:'Good (1)', -1:'bad (-1)', -2:'worst (-2)'}\n",
    "    data_cat_4 = input_data[input_data[column_value]>=0].copy() #.loc[:,is_col_main+interpolate_col+['time']].reset_index(drop=True)\n",
    "    data_cat_4.loc[:,is_col_main] = input_data.loc[:,is_col_main].replace(answer_replace4)\n",
    "    data_cat_4['time'] = data_cat_4[['hour']].applymap(lambda x: 'morning' if x < 10 else 'afternoon')\n",
    "    stat_test_main4 (data_cat_4, column_feel, column_value)\n",
    "    stat_test_boxplot (data_cat_4, column_feel, column_value, value_label)\n",
    "    stat_test_boxplot_hue (data_cat_4, column_feel, column_value, value_label)\n",
    "\n",
    "def stat_test_subquestion (input_data, is_col_air, column_feel, column_value, value_label):\n",
    "    #recategorical as 2 categories\n",
    "    answer_replace2 = {0:'No', 1:'Yes'}\n",
    "    data_cat_s = input_data[input_data[column_value]>=0].copy()#.loc[:,is_col_main+interpolate_col+['time']].reset_index(drop=True)\n",
    "    data_cat_s.loc[:,is_col_air] = input_data.loc[:,is_col_air].replace(answer_replace2)\n",
    "    data_cat_s['time'] = data_cat_s[['hour']].applymap(lambda x: 'morning' if x < 10 else 'afternoon')\n",
    "    stat_test_sub2 (data_cat_s, column_feel, column_value)\n",
    "    stat_test_boxplot (data_cat_s, column_feel, column_value, value_label)\n",
    "    stat_test_boxplot_hue (data_cat_s, column_feel, column_value, value_label)\n",
    "\n",
    "def stat_trial_plot (dataset, column_feel, column_value, column_label):\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sns.boxplot(y= column_feel, x= column_value, orient='h' , data=dataset )\n",
    "    plt.title(column_feel + ' vs ' + column_value + ' [time separated]')\n",
    "    plt.xlabel(column_label)\n",
    "    plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "    plt.ylabel(column_feel)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "def stat_trial_plot_hue (dataset, column_feel, column_value, column_label):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(y= column_feel, x= column_value, orient='h' , hue ='time', data=dataset )\n",
    "    plt.title(column_feel + ' vs ' + column_value + ' [time separated]')\n",
    "    plt.xlabel(column_label)\n",
    "    plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "    plt.ylabel(column_feel)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def stat_trial_time (input_data, is_col_main, column_feel, column_value, value_label):\n",
    "    #recategorical as 2 categories\n",
    "    answer_replace2 = {0:'dont know',2:'satisfied' , 1:'satisfied', -1:'dissatisf.', -2:'dissatisf.'}\n",
    "    data_cat_2 = input_data.copy()#.loc[:,is_col_main+interpolate_col+['time']].reset_index(drop=True)\n",
    "    data_cat_2.loc[:,is_col_main] = input_data.loc[:,is_col_main].replace(answer_replace2)\n",
    "    data_cat_2['time'] = data_cat_2[['hour']].applymap(lambda x: 'morning' if x < 10 else 'afternoon')\n",
    "    stat_test_main2 (data_cat_2, column_feel, column_value)\n",
    "    stat_trial_plot (data_cat_2, column_feel, column_value, value_label)\n",
    "    stat_trial_plot_hue (data_cat_2, column_feel, column_value, value_label)\n",
    "\n",
    "def stat_trial_all(df, test_feel,test_column,test_label):\n",
    "    stat_trial_time (df, is_col_main, test_feel, test_column, test_label)\n",
    "    stat_trial_time (df[df['hour']<=9], is_col_main, test_feel, test_column, test_label)\n",
    "    stat_trial_time (df[df['hour']>=10], is_col_main, test_feel, test_column, test_label)\n",
    "\n",
    "def stat_trial_hist (dataset, column_feel, column_value, column_label):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(data=dataset, x=column_value, legend='auto', hue=column_feel, multiple='stack', kde=True)\n",
    "    plt.title(column_feel + ' vs ' + column_value + ' [time separated]')\n",
    "    plt.xlabel(column_label)\n",
    "    plt.ylabel('frequency timestep')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def stat_hist_time (input_data, is_col_main, column_feel, column_value, value_label):\n",
    "    #recategorical as 2 categories\n",
    "    answer_replace2 = {0:'dont know',2:'satisfied' , 1:'satisfied', -1:'dissatisf.', -2:'dissatisf.'}\n",
    "    data_cat_2 = input_data.copy()#.loc[:,is_col_main+interpolate_col+['time']].reset_index(drop=True)\n",
    "    data_cat_2.loc[:,is_col_main] = input_data.loc[:,is_col_main].replace(answer_replace2)\n",
    "    data_cat_2['time'] = data_cat_2[['hour']].applymap(lambda x: 'morning' if x < 10 else 'afternoon')\n",
    "    stat_test_main2 (data_cat_2, column_feel, column_value)\n",
    "    stat_trial_hist (data_cat_2, column_feel, column_value, value_label)\n",
    "\n",
    "def stat_hist_all(df, test_feel,test_column,test_label):\n",
    "    stat_hist_time (df, is_col_main, test_feel, test_column, test_label)\n",
    "    stat_hist_time (df[df['hour']<=9], is_col_main, test_feel, test_column, test_label)\n",
    "    stat_hist_time (df[df['hour']>=10], is_col_main, test_feel, test_column, test_label)\n",
    "\n",
    "\n",
    "answer_replace2 = {0:'dont know',2:'satisfied' , 1:'satisfied', -1:'Dissatisf.', -2:'Dissatisf.'}\n",
    "answer_replace4 = {0:0,2:'Best' , 1:'Good', -1:'Bad', -2:'Worst'}\n",
    "no_replace4 = {0:0,2:2 , 1:1, -1:-1, -2:-2}\n",
    "no_replace2 = {0:0,2:1 , 1:1, -1:-1, -2:-1}\n",
    "\n",
    "def feel_confusion_matrix (input_data, column_value1, column_value2, answer_replace):\n",
    "    data_cat_2 = input_data.sort_values([column_value1]).copy()#.loc[:,is_col_main+interpolate_col+['time']].reset_index(drop=True)\n",
    "    data_cat_2.loc[:,is_col_main] = data_cat_2.loc[:,is_col_main].replace(answer_replace)\n",
    "    identical_percent = (accuracy_score(data_cat_2[column_value1],data_cat_2[column_value2])*100).round(3)\n",
    "    print('identical accuracy  : ' + str(identical_percent) + '%')\n",
    "    cm_matrix = confusion_matrix(data_cat_2[column_value1],data_cat_2[column_value2])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix= cm_matrix, display_labels=data_cat_2[column_value2].unique() )\n",
    "    disp.plot()\n",
    "    plt.xlabel(column_value1)\n",
    "    plt.ylabel(column_value2)\n",
    "    plt.title('Identicality : ' + column_value1 + ' vs. ' +column_value2 + '= ' + str(identical_percent) + '%')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1726742296891,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "q3GoB0_OQqb-"
   },
   "outputs": [],
   "source": [
    "#custom_colors = [ '#7fff00','#66cd00',  '#a52a2a','#ffa500', '#808080']\n",
    "def pie_plot_feel (df, what_feel):\n",
    "    answer_replace4 = {0:'dont know',2:'Best (2)' , 1:'Good (1)', -1:'bad (-1)', -2:'worst (-2)'}\n",
    "    df.loc[:,what_feel] = df.loc[:,what_feel].replace(answer_replace4)\n",
    "    vote_counts1= (df[what_feel].value_counts().sort_index())\n",
    "    print('total votes: ' + str((df[what_feel].value_counts().sum())))\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.pie(vote_counts1, labels=vote_counts1.index, autopct= lambda x: '{:.0f}'.format(x*vote_counts1.sum()/100) #'%1.1f%%'\n",
    "            # , colors= custom_colors\n",
    "            )\n",
    "    plt.title(school_name  +': ' + what_feel)\n",
    "    plt.show()\n",
    "\n",
    "def pie_plot_feel_class (df, what_feel, class_number):\n",
    "    answer_replace4 = {0:'dont know',2:'Best (2)' , 1:'Good (1)', -1:'bad (-1)', -2:'worst (-2)'}\n",
    "    df.loc[:,what_feel] = df.loc[:,what_feel].replace(answer_replace4)\n",
    "    vote_counts1= (df[df['room_id']==class_number][what_feel].value_counts().sort_index())\n",
    "    print('total votes: ' + str((df[df['room_id']==class_number][what_feel].value_counts().sum())))\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.pie(vote_counts1, labels=vote_counts1.index, autopct= lambda x: '{:.0f}'.format(x*vote_counts1.sum()/100)   #'%1.1f%%'\n",
    "        #     ,  colors= custom_colors\n",
    "            )\n",
    "    plt.title(school_name + ' at Rom ' + str(class_number) +': ' + what_feel)\n",
    "    plt.show()\n",
    "\n",
    "def vote_piechart (get_in_file, what_feel,class_number,school_name,custom_colors):\n",
    "    vote_counts4 = (get_in_file[get_in_file['room_id']==class_number][what_feel].value_counts().sort_index())\n",
    "    # Custom colors for the pie chart segments\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.pie(vote_counts4, labels=vote_counts4.index, autopct='%1.1f%%', startangle=140\n",
    "            # , colors= custom_colors\n",
    "            )\n",
    "    plt.title(school_name + '_' + str(class_number) +': ' + what_feel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1726742296891,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "LLq08z-WxyLI"
   },
   "outputs": [],
   "source": [
    "def season_creator (df,start_date,end_date,season_ranges):\n",
    "    # Convert the date column to datetime\n",
    "    df['date2'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Initialize the 'season' column with default value 0\n",
    "    df['season'] = 0\n",
    "\n",
    "    # Assign the season codes based on the date ranges\n",
    "    for start_date, end_date, season_code in season_ranges:\n",
    "        start = pd.to_datetime(start_date)\n",
    "        end = pd.to_datetime(end_date)\n",
    "        df.loc[(df['date2'] >= start) & (df['date2'] <= end), 'season'] = season_code\n",
    "    df.drop(['date2'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def aggfunc_mode(series):\n",
    "    m, _ = mode(series)\n",
    "    return m[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1726742296891,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "l1R_j51nl_ga"
   },
   "outputs": [],
   "source": [
    "values_temp2   =  [#'swc_r', #'swc_f',\n",
    "                   #'temp_v',  'temp_s', # 'temp_f',\n",
    "                   #'press_a',\n",
    "                   'temp_a', 'rh_a','enth_a', 'hura_a','enth_o', 'hura_o','temp_o', 'rh_o', 'students'\n",
    "                   #,'pmv', 'ppd'\n",
    "                   ]\n",
    "values_air2    =  [#'temp_f', 'temp_v',\n",
    "                   'temp_a','rh_a', 'co2_a', 'voc_a', 'bright_a', 'rdn_a', 'enth_a','enth_o', 'hura_a','hura_o','temp_o', 'rh_o', 'winds_o'\n",
    "                   #, 'sun_o'\n",
    "                   #,'pm10_o', 'pm2.5_o'\n",
    "                   , 'students'\n",
    "                   ]\n",
    "values_health2 =  [#'temp_f', 'temp_v',\n",
    "                   'temp_a', 'rh_a', 'co2_a', 'voc_a', 'bright_a','rdn_a','hura_a','hura_o','enth_a','enth_o'\n",
    "                   #,'pm1_a', 'pm25_a'\n",
    "                   #,'pm10_o', 'pm2.5_o'\n",
    "                   , 'rh_o','winds_o'\n",
    "                   #, 'sun_o'\n",
    "                   , 'students'\n",
    "                   ]\n",
    "values_bright2 =  ['temp_a', 'bright_a', 'temp_o', 'rh_o','winds_o', 'rain_o', 'enth_o', 'students']\n",
    "values_noise2  =  [ 'co2_a', 'voc_a', 'sound_a','enth_a','hura_a','hura_o','enth_o'\n",
    "                   #,'pm1_a', 'pm25_a'\n",
    "                   , 'rh_a'\n",
    "                   , 'students']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1726742296891,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "q77rXBHBxyLI"
   },
   "outputs": [],
   "source": [
    "def determine_batch_to_moment(row):\n",
    "    if row['batch'] == 'morning' :\n",
    "        return 1\n",
    "    elif row['batch'] == 'afternoon':\n",
    "        return 3\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def determine_batch_brannfjell(row):\n",
    "    if str(row['room_id']) in ['11', '23']:\n",
    "        if pd.to_datetime('07:15') <= pd.to_datetime(row['time']) < pd.to_datetime('08:30'):\n",
    "            return 'morning'\n",
    "        elif 12 <= row['hour'] <= 13:\n",
    "            return 'afternoon'\n",
    "    elif str(row['room_id']) == '27':\n",
    "        if pd.to_datetime('09:00') <= pd.to_datetime(row['time']) <= pd.to_datetime('10:30'):\n",
    "            return 'morning'\n",
    "        elif 12 <= row['hour'] <= 13:\n",
    "            return 'afternoon'\n",
    "    return 'skip'\n",
    "\n",
    "def determine_batch_øyra(row):\n",
    "    if row['room_id'] == '302':\n",
    "        if pd.to_datetime('07:10') <= pd.to_datetime(row['time']) <= pd.to_datetime('08:15'):\n",
    "            return 'morning'\n",
    "        elif 11 <= row['hour'] <= 12:\n",
    "            return 'afternoon'\n",
    "\n",
    "    elif row['room_id'] == '303':\n",
    "        if pd.to_datetime('07:40') <= pd.to_datetime(row['time']) <= pd.to_datetime('09:45'):\n",
    "            return 'morning'\n",
    "        elif 11 <= row['hour'] <= 12:\n",
    "            return 'afternoon'\n",
    "    return 'skip'\n",
    "\n",
    "def determine_batch_bratteberg(row):\n",
    "    if row['room_id'] in ['141', '142']:\n",
    "        if pd.to_datetime('07:10') <= pd.to_datetime(row['time']) <= pd.to_datetime('08:30'):\n",
    "            return 'morning'\n",
    "        elif 11 <= row['hour'] <= 12:\n",
    "            return 'afternoon'\n",
    "    elif row['room_id'] == '210':\n",
    "        if pd.to_datetime('07:15') <= pd.to_datetime(row['time']) <= pd.to_datetime('08:15'):\n",
    "            return 'morning'\n",
    "        elif 10 <= row['hour'] <= 11:\n",
    "            return 'afternoon'\n",
    "\n",
    "    elif row['room_id'] == 'Øst brakker':\n",
    "        if pd.to_datetime('07:15') <= pd.to_datetime(row['time']) <= pd.to_datetime('09:15'):\n",
    "            return 'morning'\n",
    "        elif 10 <= row['hour'] <= 12:\n",
    "            return 'afternoon'\n",
    "    return 'skip'\n",
    "\n",
    "def determine_batch_allschools(row):\n",
    "    if row['school_id'] == 'bratteberg':\n",
    "        return determine_batch_bratteberg(row)\n",
    "    elif row['school_id'] == 'brannfjell':\n",
    "        return determine_batch_brannfjell(row)\n",
    "    elif row['school_id'] == 'øyra':\n",
    "        return determine_batch_øyra(row)\n",
    "    return 'skip'\n",
    "# Apply the function to each row\n",
    "# df['batch'] = df.apply(determine_batch_allschools, axis=1)\n",
    "def determine_stay(row):        #FOR COMBINED 2\n",
    "    if row['co2_a'] >= 600: \n",
    "        return 'new_stay'\n",
    "    elif row['co2_a'] == np.nan:\n",
    "        if row['co2_v'] >=600:\n",
    "            return 'new_stay'\n",
    "        else:\n",
    "            return 'after_hour'\n",
    "    else:\n",
    "        return 'after_hour'\n",
    "\n",
    "def determine_duration_stay(row):\n",
    "    if row['co2_a'] >= 600:\n",
    "        return 3\n",
    "    elif row['co2_a'] == np.nan:\n",
    "        if row['co2_v'] >=600:\n",
    "            return 3\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def dissatisfaction_creator(df):\n",
    "    df['rd_air']      = 1*(df['feel_air3']+df['feel_air4'])/df[response_counting]\n",
    "    df['rd_temp']     = 1*(df['feel_temp3']+df['feel_temp4'])/df[response_counting]\n",
    "    df['rd_health']   = 1*(df['feel_health3']+df['feel_health4'])/df[response_counting]\n",
    "    df['rd_bright']   = 1*(df['feel_bright3']+df['feel_bright4'])/df[response_counting]\n",
    "    df['rd_noise']    = 1*(df['feel_noise3']+df['feel_noise4'])/df[response_counting]\n",
    "\n",
    "def dissatisfaction_aggregator(df):\n",
    "    df['acc_air']    = (df['feel_air1']*2+df['feel_air2'] +df['feel_air3']*-1+df['feel_air4']*-2)/df[response_counting]\n",
    "    df['acc_temp']   = (df['feel_temp1']*2+df['feel_temp2'] +df['feel_temp3']*-1+df['feel_temp4']*-2)/df[response_counting]\n",
    "    df['acc_health'] = (df['feel_health1']*2+df['feel_health2'] +df['feel_health3']*-1+df['feel_health4']*-2)/df[response_counting]\n",
    "    df['acc_bright'] = (df['feel_bright1']*2+df['feel_bright2'] +df['feel_bright3']*-1+df['feel_bright4']*-2)/df[response_counting]\n",
    "    df['acc_noise']  = (df['feel_noise1']*2+df['feel_noise2'] +df['feel_noise3']*-1+df['feel_noise4']*-2)/df[response_counting]\n",
    "\n",
    "def assign_room_id(row):\n",
    "    if str(row['room_id']).startswith(('2', '3', '4')):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_batch_brannfjell1(row):   #BATCH FOR MEASUREMENT\n",
    "    if str(row['room_id']) == '11':\n",
    "        if  pd.to_datetime(row['time']) <= pd.to_datetime('07:30'):\n",
    "            return 'morning'\n",
    "        elif 10 <= row['hour'] <= 11:\n",
    "            return 'afternoon'\n",
    "    elif str(row['room_id']) == '23':\n",
    "        if pd.to_datetime(row['time']) <= pd.to_datetime('07:30'):\n",
    "            return 'morning'\n",
    "        elif 11 <= row['hour'] <= 13:\n",
    "            return 'afternoon'\n",
    "    elif str(row['room_id']) == '27':\n",
    "        if 7 <= row['hour'] <= 8:\n",
    "            return 'morning'\n",
    "        elif 12 <= row['hour'] <= 13:\n",
    "            return 'afternoon'\n",
    "    return 'skip'\n",
    "\n",
    "def determine_batch_øyra1(row):\n",
    "    if row['room_id'] == '302':\n",
    "        if pd.to_datetime('07:15') <= pd.to_datetime(row['time']) <= pd.to_datetime('08:15'):\n",
    "            return 'morning'\n",
    "        elif 11 <= row['hour'] <= 12:\n",
    "            return 'afternoon'\n",
    "\n",
    "    elif row['room_id'] == '303':\n",
    "        if pd.to_datetime('07:40') <= pd.to_datetime(row['time']) <= pd.to_datetime('09:45'):\n",
    "            return 'morning'\n",
    "        elif 11 <= row['hour'] <= 12:\n",
    "            return 'afternoon'\n",
    "    return 'skip'\n",
    "\n",
    "def determine_batch_bratteberg1(row):\n",
    "    if row['room_id'] in ['141', '142']:\n",
    "        if pd.to_datetime('07:15') <= pd.to_datetime(row['time']) <= pd.to_datetime('08:30'):\n",
    "            return 'morning'\n",
    "        elif 11 <= row['hour'] <= 12:\n",
    "            return 'afternoon'\n",
    "    elif row['room_id'] == '210':\n",
    "        if pd.to_datetime('07:15') <= pd.to_datetime(row['time']) <= pd.to_datetime('08:15'):\n",
    "            return 'morning'\n",
    "        elif 10 <= row['hour'] <= 11:\n",
    "            return 'afternoon'\n",
    "    elif row['room_id'] == 'Øst brakker':\n",
    "        if pd.to_datetime('07:15') <= pd.to_datetime(row['time']) <= pd.to_datetime('09:15'):\n",
    "            return 'morning'\n",
    "        elif 10 <= row['hour'] <= 12:\n",
    "            return 'afternoon'\n",
    "    return 'skip'\n",
    "\n",
    "def determine_batch_allschools1(row):      #BATCH FOR MEASUREMENT\n",
    "    if row['school_id'] == 'bratteberg':\n",
    "        return determine_batch_bratteberg1(row)\n",
    "    elif row['school_id'] == 'brannfjell':\n",
    "        return determine_batch_brannfjell1(row)\n",
    "    elif row['school_id'] == 'øyra':\n",
    "        return determine_batch_øyra1(row)\n",
    "    return 'skip'\n",
    "# Apply the function to each row\n",
    "# df['batch'] = df.apply(determine_batch_allschools, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_batch_brannfjell2(row):       #BATCH FOR QUESTIONNAIRE\n",
    "    if str(row['room_id']) == '11':\n",
    "        if 7 <= row['hour'] <= 8:\n",
    "            return 'morning'\n",
    "        elif 12 <= row['hour'] <= 13:\n",
    "            return 'afternoon'\n",
    "    elif str(row['room_id']) == '23':\n",
    "        if 7 <= row['hour'] <= 9:\n",
    "            return 'morning'\n",
    "        elif 12 <= row['hour'] <= 14:\n",
    "            return 'afternoon'\n",
    "    elif str(row['room_id']) == '27':\n",
    "        if 8<= row['hour'] <= 10:\n",
    "            return 'morning'\n",
    "        elif 12 <= row['hour'] <= 14:\n",
    "            return 'afternoon'\n",
    "    return 'skip'\n",
    "\n",
    "def determine_batch_øyra2(row):\n",
    "    if row['room_id'] == '302':\n",
    "        if pd.to_datetime('07:15') <= pd.to_datetime(row['time']) <= pd.to_datetime('09:30'):\n",
    "            return 'morning'\n",
    "        elif 10 <= row['hour'] <= 15:\n",
    "            return 'afternoon'\n",
    "\n",
    "    elif row['room_id'] == '303':\n",
    "        if pd.to_datetime('07:15') <= pd.to_datetime(row['time']) <= pd.to_datetime('09:45'):\n",
    "            return 'morning'\n",
    "        elif 10 <= row['hour'] <= 15:\n",
    "            return 'afternoon'\n",
    "    return 'skip'\n",
    "\n",
    "def determine_batch_bratteberg2(row):\n",
    "    if row['room_id'] in ['141', '142']:\n",
    "        if pd.to_datetime('07:15') <= pd.to_datetime(row['time']) <= pd.to_datetime('08:30'):\n",
    "            return 'morning'\n",
    "        elif 9 <= row['hour'] <= 14:\n",
    "            return 'afternoon'\n",
    "    elif row['room_id'] == '210':\n",
    "        if pd.to_datetime('07:15') <= pd.to_datetime(row['time']) <= pd.to_datetime('08:15'):\n",
    "            return 'morning'\n",
    "        elif 9 <= row['hour'] <= 14:\n",
    "            return 'afternoon'\n",
    "\n",
    "    elif row['room_id'] == 'Øst brakker':\n",
    "        if pd.to_datetime('07:15') <= pd.to_datetime(row['time']) <= pd.to_datetime('09:15'):\n",
    "            return 'morning'\n",
    "        elif 9 <= row['hour'] <= 14:\n",
    "            return 'afternoon'\n",
    "    return 'skip'\n",
    "\n",
    "def determine_batch_allschools2(row):           #BATCH FOR QUESTIONNAIRE\n",
    "    if row['school_id'] == 'bratteberg':\n",
    "        return determine_batch_bratteberg2(row)\n",
    "    elif row['school_id'] == 'brannfjell':\n",
    "        return determine_batch_brannfjell2(row)\n",
    "    elif row['school_id'] == 'øyra':\n",
    "        return determine_batch_øyra2(row)\n",
    "    return 'skip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WX9DV2WOmc51"
   },
   "source": [
    "# Bratteberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "QUNTKjuzxyLJ"
   },
   "outputs": [],
   "source": [
    "class1 = 141\n",
    "class2 = 142\n",
    "class3 = 210\n",
    "class4 = 'Øst brakker'\n",
    "school_name    = 'bratteberg'\n",
    "end_file_name  = 'classes'\n",
    "\n",
    "airthingsDevice = airthingsAll\n",
    "weather_data = weather_volda\n",
    "\n",
    "another_interpolate_col =  airthingsDevice + weather_data\n",
    "afternoon_hour_limit = 13\n",
    "season_ranges = [\n",
    "    ('2023-03-03', '2023-04-25', 1),\n",
    "    ('2024-02-26', '2024-03-08', 2),\n",
    "]\n",
    "\n",
    "room_protokoll_bratteberg = {'room_id': ['141' , '142', '210', 'Øst brakker'],\n",
    "    'north_window': [ 0  , 0  , 0 ,  0.12],\n",
    "    'south_window': [ 0.08   , 0.12   , 0.08 ,  0.12],\n",
    "    'west_window':  [ 0  , 0  , 0 ,  0.12],\n",
    "    'east_window':  [ 0.17   , 0.24   , 0.17 , 0 ],\n",
    "    'floor_first' : [1,1,2,2],\n",
    "    'cold_wall_ratio' : [0.17, 0.36, 0.17, 0.12],\n",
    "    'student_age' : [12,12,12,12],\n",
    "    \n",
    "    }\n",
    "room_protokoll_pivoting = room_protokoll_bratteberg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBeOt8SDxyLJ"
   },
   "source": [
    "Batch 1 - Pilot Study March 2023\n",
    "\n",
    "data come from : 141  ,  142  ,  210  , Øst brakker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owL8mZnAmXVA"
   },
   "outputs": [],
   "source": [
    "folder_year    = '2023'\n",
    "\n",
    "sd_fill_col =  ['swc_f', 'swc_r', 'temp_s'] # [] #\n",
    "sd_meaninterpolate_col = ['temp_v','temp_f']   #   []    #\n",
    "\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col\n",
    "\n",
    "start_date = '2023-03-06'\n",
    "end_date  =  '2023-04-25'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5CKcv5rxyLJ"
   },
   "source": [
    "Batch 2 - 2024\n",
    "\n",
    "data come from : 141  ,  210  , Øst brakker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20vYjiX8xyLJ"
   },
   "outputs": [],
   "source": [
    "folder_year    = '2024'\n",
    "\n",
    "sd_fill_col            =    ['swc_f', 'swc_r', 'temp_s'] #\n",
    "sd_meaninterpolate_col =    ['temp_v','temp_f']          #\n",
    "\n",
    "interpolate_col = sd_fill_col  + sd_meaninterpolate_col + another_interpolate_col\n",
    "\n",
    "start_date = '2024-02-26'\n",
    "end_date  =  '2024-03-08'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7304arLoxyLJ"
   },
   "source": [
    "All years in same schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "6DAPEr2_xyLJ"
   },
   "outputs": [],
   "source": [
    "folder_year    = 'all years'\n",
    "\n",
    "sd_fill_col =  ['swc_f', 'swc_r', 'temp_s'] # [] #\n",
    "sd_meaninterpolate_col = ['temp_v','temp_f']   #   []    #\n",
    "\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col\n",
    "\n",
    "start_date = '2023-03-06'\n",
    "end_date  =  '2024-04-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMX3XRbmQqb_"
   },
   "outputs": [],
   "source": [
    "print(another_interpolate_col + ['temp_v'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwekofXHQvdi"
   },
   "source": [
    "# Øyra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "qMk0XFPNxyLK"
   },
   "outputs": [],
   "source": [
    "class1 = 302\n",
    "class2 = 303\n",
    "class3 = 339\n",
    "school_name = 'øyra'\n",
    "end_file_name  = 'classes'\n",
    "\n",
    "airthingsDevice = airthingsPro.copy()\n",
    "weather_data    = weather_volda\n",
    "afternoon_hour_limit = 13\n",
    "another_interpolate_col = airthingsDevice + weather_data\n",
    "\n",
    "season_ranges = [\n",
    "    ('2023-03-08', '2023-04-20', 1),\n",
    "    ('2024-04-03', '2024-04-30', 2)\n",
    "]\n",
    "\n",
    "\n",
    "room_protokoll_øyra = {'room_id': [ '302', '303', '339'],\n",
    "                        'north_window': [ 0     , 0    , 0 ],\n",
    "                        'south_window': [ 0.23  , 0.   , 0 ],\n",
    "                        'west_window':  [ 0     , 0    , 0.23 ],\n",
    "                        'east_window':  [ 0.23  , 0.23 , 0    ],\n",
    "                        'cold_wall_ratio' : [0.2, 0.1, 0.1],\n",
    "                        'floor_first' : [3,3,3] ,\n",
    "                        'student_age' : [12,12,12]}\n",
    "room_protokoll_pivoting = room_protokoll_øyra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bzACCDWxyLK"
   },
   "source": [
    "Batch 1 - Pilot Study March 2023\n",
    "\n",
    "data comes from classrooms :\n",
    "\n",
    "class1 : 302\n",
    "\n",
    "class2 : 303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPF757sSQvdp"
   },
   "outputs": [],
   "source": [
    "folder_year = '2023'\n",
    "start_date  = '2023-03-13'\n",
    "end_date    = '2023-04-18'\n",
    "\n",
    "sd_fill_col = [ 'temp_s']\n",
    "sd_meaninterpolate_col = ['temp_v', 'vent_v']\n",
    "\n",
    "interpolate_col =  sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_LKM9zqxyLK"
   },
   "source": [
    "Batch 2 - April 2024\n",
    "data comes from classrooms :\n",
    "\n",
    "class2 : 303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPmzMgvMxyLK"
   },
   "outputs": [],
   "source": [
    "folder_year = '2024'\n",
    "start_date  = '2024-04-03'\n",
    "end_date    = '2024-04-17'\n",
    "\n",
    "sd_fill_col            =  [] #   [ 'temp_s']\n",
    "sd_meaninterpolate_col =  ['temp_v'] #   ['temp_v', 'vent_v']\n",
    "\n",
    "interpolate_col =  sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Itu88Y9nxyLK"
   },
   "source": [
    "All years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "vfjsLZKmxyLK"
   },
   "outputs": [],
   "source": [
    "folder_year = 'all years'\n",
    "start_date  = '2023-03-13'\n",
    "end_date    = '2024-04-18'\n",
    "\n",
    "sd_fill_col = [ 'temp_s']\n",
    "sd_meaninterpolate_col = ['temp_v', 'vent_v']\n",
    "\n",
    "interpolate_col =  sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Z8B6FmJxyLK"
   },
   "source": [
    "# KubenVGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQbVO4oJxyLK"
   },
   "outputs": [],
   "source": [
    "class1 = 1707\n",
    "class2 = 1713\n",
    "class3 = 2707\n",
    "class4 = 2713\n",
    "class5 = 3707\n",
    "class6 = 3708\n",
    "class7 = 3713\n",
    "\n",
    "airthingsDevice = airthingsPro\n",
    "school_name     = 'kubenVGS'\n",
    "end_file_name  = 'classes'\n",
    "weather_data    = weather_oslo\n",
    "another_interpolate_col = airthingsDevice + weather_data\n",
    "afternoon_hour_limit = 13\n",
    "season_ranges = [\n",
    "    ('2023-03-03', '2023-04-25', 1),\n",
    "    ('2023-11-10', '2023-11-30', 2),\n",
    "    ('2024-02-02', '2024-03-10', 3),\n",
    "    ('2024-04-07', '2024-04-30', 4)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmCxNoXuxyLK"
   },
   "outputs": [],
   "source": [
    "folder_year    = '2023'\n",
    "\n",
    "sd_fill_col =    []     #\n",
    "sd_meaninterpolate_col =      []   #\n",
    "another_interpolate_col = airthingsDevice + weather_oslo\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col\n",
    "\n",
    "start_date = '2023-03-10'\n",
    "end_date  =  '2023-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6mPy-W1xyLL"
   },
   "outputs": [],
   "source": [
    "folder_year    = '2024'\n",
    "\n",
    "sd_fill_col =    []     #\n",
    "sd_meaninterpolate_col =      []   #\n",
    "another_interpolate_col = airthingsDevice + weather_oslo\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col\n",
    "\n",
    "start_date = '2023-03-10'\n",
    "end_date  =  '2023-12-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIvmuqzJQqb_"
   },
   "source": [
    "# Brannfjell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1726741449818,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "c0SRQbDzxyLL"
   },
   "outputs": [],
   "source": [
    "class1 = 11\n",
    "class2 = 23\n",
    "class3 = 27\n",
    "airthingsDevice = airthingsPlus\n",
    "school_name     = 'brannfjell'\n",
    "end_file_name  = 'classes'          #if we want to process each single season only\n",
    "end_file_name  = 'alldate'      #if we want to process all seasons massively together\n",
    "\n",
    "weather_data    = weather_oslo\n",
    "another_interpolate_col = airthingsDevice + weather_data\n",
    "afternoon_hour_limit = 13\n",
    "season_ranges = [\n",
    "    ('2023-03-03', '2023-04-25', 1),\n",
    "    ('2023-11-10', '2023-11-30', 2),\n",
    "    ('2024-02-02', '2024-03-10', 3),\n",
    "    ('2024-04-07', '2024-04-30', 4)\n",
    "]\n",
    "\n",
    "room_protokoll_brannfjell = {'room_id': ['11' , '12', '21', '22', '23', '24', '25', '26', '27', '28', '31', '32', '33', '34', '35', '36',     '37', '38', '39', '20', '29' ],\n",
    "                  'floor_first' : [1, 1,  2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,    3, 3, 3,2, 2 ],\n",
    "                  'north_window' : [0, 0,  0, 0, 0, 0, 0, 0,0, 0, 0, 0, 0, 0, 0, 0,    0, 0, 0,0, 0 ],\n",
    "                  'south_window' : [0, 0,  0, 0, 0, 0, 0, 0,0, 0, 0, 0, 0, 0, 0, 0,    0, 0, 0,0, 0 ],\n",
    "                 'east_window' : [0, 0,  0, 0, 0, 0, 0, 0,0.22, 0.24, 0, 0, 0, 0.24, 0.24, 0.24,    0, 0, 0,0, 0 ],\n",
    "                 'west_window' : [0.22, 0.22, 0.22, 0.22, 0.24,0.22, 0.22,0.22, 0, 0,0.22, 0.22,0.22, 0, 0, 0,    0, 0, 0,0, 0],\n",
    "                 'cold_wall_ratio' : [0.15, 0.15, 0.15, 0.15, 0.15,0.15, 0.15,0.15, 0.18, 0.18,0.15, 0.15,0.15, 0.18, 0.18, 0.18,    0, 0, 0,0, 0],\n",
    "                 'student_age' : [15, 15,  15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,    15, 15, 15, 15, 15]\n",
    "                #   'students': [30 , 0, 0, 0, 26, 0, 0,0, 31, 0,0, 0, 0, 0, 0, 0,     0, 0, 0, 0, 0 ],\n",
    "                 }\n",
    "room_protokoll_pivoting = room_protokoll_brannfjell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rAubUM6xyLL"
   },
   "source": [
    "Batch 1 - Pilot Study March 2023\n",
    "\n",
    "data comes from Classrooms :\n",
    "\n",
    "class2 : 23\n",
    "\n",
    "class3 : 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1726741450111,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "UqNBwashQqb_"
   },
   "outputs": [],
   "source": [
    "folder_year    = '2023'\n",
    "\n",
    "sd_fill_col =   [ 'temp_s']  # []     #\n",
    "sd_meaninterpolate_col =  ['temp_v', 'co2_v']  #     []   #\n",
    "another_interpolate_col = airthingsDevice + weather_oslo\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col\n",
    "\n",
    "start_date = '2023-03-22'\n",
    "end_date  =  '2023-04-23'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFxjIQbzxyLL"
   },
   "source": [
    "Batch 2 - November 2023\n",
    "\n",
    "data comes from Classrooms :\n",
    "\n",
    "class1 : 11\n",
    "\n",
    "class2 : 23\n",
    "\n",
    "class3 : 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1726741450111,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "7B3C7hLxxyLL"
   },
   "outputs": [],
   "source": [
    "folder_year    = '2023'\n",
    "\n",
    "sd_fill_col    = []     #  [ 'temp_s']  # []     #\n",
    "sd_meaninterpolate_col =  []   #  ['temp_v', 'co2_v']  #     []   #\n",
    "another_interpolate_col = airthingsDevice + weather_oslo\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col\n",
    "\n",
    "start_date = '2023-11-13'\n",
    "end_date  =  '2023-11-24'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uuc9NK3wxyLL"
   },
   "source": [
    "Batch 3 - February 2024\n",
    "\n",
    "data comes from Classrooms :\n",
    "\n",
    "class2 : 23\n",
    "\n",
    "class3 : 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1726741450111,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "a7AFWF4KxyLL"
   },
   "outputs": [],
   "source": [
    "folder_year    = '2024'\n",
    "\n",
    "sd_fill_col    = []     #  [ 'temp_s']  # []     #\n",
    "sd_meaninterpolate_col =  []   #  ['temp_v', 'co2_v']  #     []   #\n",
    "another_interpolate_col = airthingsDevice + weather_oslo\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col\n",
    "\n",
    "start_date = '2024-02-26'\n",
    "end_date  =  '2024-03-08'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16FNRBsixyLL"
   },
   "source": [
    "Batch 4 - April 2024\n",
    "\n",
    "data comes from Classrooms :\n",
    "\n",
    "class1 : 11\n",
    "\n",
    "class2 : 23\n",
    "\n",
    "class3 : 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1726741450111,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "XrkU5VVcxyLL"
   },
   "outputs": [],
   "source": [
    "folder_year    = '2024'\n",
    "sd_fill_col    = []     #  [ 'temp_s']  # []     #\n",
    "sd_meaninterpolate_col =  []   #  ['temp_v', 'co2_v']  #     []   #\n",
    "another_interpolate_col = airthingsDevice + weather_oslo\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col\n",
    "\n",
    "start_date = '2024-04-15'\n",
    "end_date  =  '2024-04-30'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EvJ93K1xyLL"
   },
   "source": [
    "All years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1726741450111,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "3m86GbUNxyLL"
   },
   "outputs": [],
   "source": [
    "folder_year    = 'all years'\n",
    "\n",
    "sd_fill_col =   []     #\n",
    "sd_meaninterpolate_col =  ['temp_v', 'co2_v']  #     []   #\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col\n",
    "\n",
    "start_date = '2023-03-22'\n",
    "end_date  =  '2024-04-30'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Au4afVkkxyLL"
   },
   "source": [
    "# all Schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1726742297619,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "KlLn0WF1xyLL"
   },
   "outputs": [],
   "source": [
    "school_name    = 'all schools'\n",
    "end_file_name  = 'alldate'\n",
    "\n",
    "airthingsDevice = airthingsAll#airthingsSmall #airthingsAll\n",
    "weather_data = weather_oslo\n",
    "afternoon_hour_limit = 13\n",
    "another_interpolate_col =  airthingsDevice + weather_data\n",
    "\n",
    "folder_year    = 'all years'\n",
    "\n",
    "sd_fill_col = [ 'temp_s','swc_f', 'swc_r', 'temp_s']  # []     #\n",
    "sd_meaninterpolate_col =  ['temp_v', 'co2_v', 'vent_v']  #     []   #\n",
    "interpolate_col = sd_fill_col+ sd_meaninterpolate_col + another_interpolate_col\n",
    "\n",
    "start_date = '2023-03-22'\n",
    "end_date  =  '2024-04-30'\n",
    "\n",
    "season_ranges = [\n",
    "    ('2023-03-03', '2023-04-25', 1),\n",
    "    ('2023-11-10', '2023-11-30', 2),\n",
    "    ('2024-02-02', '2024-03-10', 3),\n",
    "    ('2024-04-07', '2024-04-30', 4)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYxkwBLmIZdm"
   },
   "source": [
    "# Import back again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1709125399661,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "Y1mocpGfQqb_",
    "outputId": "bdac7dbf-9d27-4c6e-9370-dbc2a68035a7"
   },
   "outputs": [],
   "source": [
    "directory_path1 = platform_directory + '/' + folder_project + '/' + folder_ready + '/'+ folder_year + '/' + school_name + '/' + folder_export\n",
    "folder_path1 = directory_path1 + '/' + data_for1 + '/' # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith(end_file_name+'.csv') and f.startswith(time_series_resample +  '_1_' + school_name)] # get list of csv files\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_csv(file_path, delimiter=',') # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list\n",
    "combined1 = pd.concat(dfs1, ignore_index=True)\n",
    "\n",
    "print(folder_path1 + time_series_resample +  '_1_' )\n",
    "\n",
    "# combined1['sound_a_Pa'] = np.where((20*(10**(combined1['sound_a']/20))*10**(-6)*1000) > 200, 200, (20*(10**(combined1['sound_a']/20))*10**(-6)*1000)).round(2)\n",
    "combined1 = combined1[combined1['date'].between(start_date,end_date)]\n",
    "combined1['hura_d'] = combined1['hura_a'] - combined1['hura_o']\n",
    "combined1['room_id'] = combined1['room_id'].astype(str)\n",
    "season_creator ( combined1,start_date,end_date,season_ranges)\n",
    "\n",
    "# combined1['hour'] = pd.to_datetime(combined1['time'], format='%H:%M').dt.hour\n",
    "# combined1['batch'] = combined1.apply(determine_batch_allschools1, axis=1)\n",
    "combined1#.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = platform_directory + '/' + folder_project + '/' + folder_ready+ '/' + folder_year +'/'+school_name + '/'\n",
    "# save dataframe to CSV file in specified directory\n",
    "directory_export = directory_path + folder_export  +'/' + data_for1 + '/'  + survey_resample\n",
    "combined1.to_excel( directory_export + '_1_' + school_name + '_alldate.xlsx', index=False)\n",
    "combined1.to_csv( directory_export + '_1_' + school_name + '_alldate.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6AMto1YxyLM"
   },
   "outputs": [],
   "source": [
    "combined1[combined1[school_col_name]=='brannfjell'].dropna(subset='temp_a').describe(percentiles=[0.1,0.25,0.5,0.75,0.9]).to_excel(folder_path1 + 'brannfjell_descstat.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "executionInfo": {
     "elapsed": 4938,
     "status": "ok",
     "timestamp": 1726742306091,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "3jw-FxGAuJcP",
    "outputId": "fc40e830-e115-464e-8c60-6b94f922fcf8"
   },
   "outputs": [],
   "source": [
    "directory_path2 = platform_directory + '/' + folder_project + '/' + folder_ready + '/'+ folder_year + '/' + school_name + '/' + folder_export\n",
    "folder_path2 = directory_path2 + '/' + data_for2 + '/'  # replace with the folder path\n",
    "csv_files2 = [f for f in os.listdir(folder_path2) if f.endswith(end_file_name+'.csv') and f.startswith(survey_resample +  '_2_' + school_name)] # get list of csv files\n",
    "dfs2 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files2:\n",
    "    file_path = os.path.join(folder_path2, file) # get full path of csv file\n",
    "    df = pd.read_csv(file_path, delimiter=',') # read csv file into a dataframe\n",
    "    dfs2.append(df) # append dataframe to list\n",
    "combined2 = pd.concat(dfs2, ignore_index=True)\n",
    "\n",
    "print(folder_path2 + survey_resample +  '_2_' )\n",
    "\n",
    "# combined2[is_col_main].nunique()\n",
    "# combined2['sound_a'] = np.where((20*(10**(combined2['sound_a']/20))*10**(-6)*1000) > 200, 200, (20*(10**(combined2['sound_a']/20))*10**(-6)*1000)).round(2)\n",
    "combined2['room_id'] = combined2['room_id'] .astype(str)\n",
    "combined2 = combined2[combined2['date'].between(start_date,end_date)]\n",
    "combined2['hura_d'] = combined2['hura_a'] - combined2['hura_o']\n",
    "\n",
    "# combined2['floor_first'] = combined2.apply(assign_room_id, axis=1)\n",
    "combined2['batch'] = combined2.apply(determine_batch_allschools2, axis=1)\n",
    "season_creator ( combined2,start_date,end_date,season_ranges)\n",
    "\n",
    "combined2['too_hot'] = combined2['temp_coldhot'].apply(lambda x: x*0.333333 if x > 1 else 0)\n",
    "combined2['too_cold'] = combined2['temp_coldhot'].apply(lambda x: x*-0.333333 if x < -1 else 0)\n",
    "# combined2 = combined2[combined2['batch']!='skip']\n",
    "combined2#.info()    #.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1726742411862,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "bdArUb7_xyLM"
   },
   "outputs": [],
   "source": [
    "combined2[is_col_main] = combined2[is_col_main].replace({-2:4, -1:3, 1:1, 2:2})\n",
    "combined2['total_feel'] = combined2[['feel_air', 'feel_temp', 'feel_bright', 'feel_noise']].abs().sum(axis=1)#.value_counts()\n",
    "combined2 = combined2[combined2['total_feel']>4]\n",
    "combined2[is_col_main] = combined2[is_col_main].replace({4:-2, 3:-1, 1:1, 2:2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RDcWoL8xyLM"
   },
   "outputs": [],
   "source": [
    "directory_path = platform_directory + '/' + folder_project + '/' + folder_ready+ '/' + folder_year +'/'+school_name + '/'\n",
    "# save dataframe to CSV file in specified directory\n",
    "directory_export = directory_path + folder_export  +'/' + data_for2 + '/'  + survey_resample\n",
    "combined2.to_excel( directory_export + '_2_' + school_name + '_alldate.xlsx', index=False)\n",
    "combined2.to_csv( directory_export + '_2_' + school_name + '_alldate.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stJKQtsuxyLM"
   },
   "outputs": [],
   "source": [
    "\n",
    "combined2['date'] = combined2['datetime'].dt.strftime('%Y-%m-%d')\n",
    "combined2['time'] = combined2['datetime'].dt.strftime('%H:%M')\n",
    "name_date = datetime.strptime((combined2[date_name].iloc[1]),'%Y-%m-%d').strftime('%Y%m%d')\n",
    "filename = school_name+'_' + name_date + '_combined all classes.csv'\n",
    "filenamexl = school_name+'_' + name_date + '_combined all classes.xlsx'\n",
    "\n",
    "# save dataframe to CSV file in specified directory\n",
    "directory_export = directory_path + folder_export +'/' + data_for2 + '/'  + survey_resample\n",
    "combined2.to_excel( directory_export + '_2_' + filenamexl, index=False)\n",
    "combined2.to_csv( directory_export + '_2_' + filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4-lkghXxyLM"
   },
   "outputs": [],
   "source": [
    "print (combined1['school_id'].unique())\n",
    "print (combined1['room_id'].unique())\n",
    "print (combined1['time'].unique())\n",
    "print (combined1['date'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29mdnxhsxyLM"
   },
   "outputs": [],
   "source": [
    "print (combined2['school_id'].unique())\n",
    "print (combined2['room_id'].unique())\n",
    "print (combined2['date'].unique())\n",
    "print (combined2['season'].unique())\n",
    "print (combined2['room_id'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OXy8p1DodkZ"
   },
   "outputs": [],
   "source": [
    "hour_morning = 9\n",
    "hour_afternoon = 10\n",
    "\n",
    "plotcheck1  = combined1[( (combined1['hour'] >= hour_afternoon )\n",
    "                        #  & (combined1['response_rate'] >= 0.4 )&\n",
    "                        #   (combined1['response_rate'] <= 1.3)&\n",
    "                        #   (combined1[response_counting]>=1)\n",
    " #                         & (combined1['room_id']==141)\n",
    "                            ) ] #(combined1[response_counting]<23)&   & (combined1['room_id']==141)\n",
    "plotcheck1m  = combined1[((combined1['hour'] <= hour_morning)\n",
    "                        #   & (combined1['response_rate'] >= 0.4)\n",
    "                        #   & (combined1['response_rate'] <= 1.3)&\n",
    "                        #   (combined1[response_counting]>=1)\n",
    "                          ) ] #(combined1[response_counting]<23)&   & (combined1['room_id']==141)\n",
    "plotcheck2  = combined2 [(combined2['hour'] >= hour_afternoon) ]\n",
    "plotcheck2m = combined2 [(combined2['hour'] < hour_afternoon) ]\n",
    "#plotcheck1 = plotcheck1m.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkAouhWkxIWE"
   },
   "outputs": [],
   "source": [
    "print('time series data              : ', time_series_resample )\n",
    "print('survey questionnaire resample : ', survey_resample)\n",
    "print('desampler                     : ', desample_option)\n",
    "print('file usage                    : ', school_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtFKt_pIQqcA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy.polynomial import Polynomial\n",
    "# Sample data\n",
    "y = np.array(combined1[(combined1['response_rate']>0.5) & (combined1['response_rate']< 1.2)]['ad_temp'])\n",
    "x = np.array(combined1[(combined1['response_rate']>0.5) & (combined1['response_rate']< 1.2)]['temp_a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U45ltFu_xyLM",
    "outputId": "ba3d7ac9-5a09-4e1d-9ad1-e46d1e03a1f0"
   },
   "outputs": [],
   "source": [
    "directory_path3 = platform_directory + '/' + folder_project + '/' + folder_ready + '/'+ folder_year + '/' + school_name + '/' + folder_export\n",
    "folder_path3 = directory_path3 + '/' + folder_pivot + '/' # replace with the folder path\n",
    "csv_files3 = [f for f in os.listdir(folder_path3) if f.endswith('mean_pivot.xlsx') and f.startswith(time_series_resample + '_' +school_name )] # get list of csv files\n",
    "dfs3 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files3:\n",
    "    file_path = os.path.join(folder_path3, file) # get full path of csv file\n",
    "    df = pd.read_excel(file_path) # read csv file into a dataframe\n",
    "    dfs3.append(df) # append dataframe to list\n",
    "combined_pivot_mean = pd.concat(dfs3, ignore_index=True)\n",
    "\n",
    "print(folder_path3 + time_series_resample )\n",
    "\n",
    "# combined_pivot_mean['sound_a'] = np.where((20*(10**(combined_pivot_mean['sound_a']/20))*10**(-6)*1000) > 200, 200, (20*(10**(combined_pivot_mean['sound_a']/20))*10**(-6)*1000)).round(2)\n",
    "combined_pivot_mean['room_id'] = combined_pivot_mean['room_id'].astype(str)\n",
    "\n",
    "combined_pivot_mean['moment'] = combined_pivot_mean.apply(determine_batch_to_moment, axis=1)\n",
    "combined_pivot_mean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRyHLv3exyLM"
   },
   "outputs": [],
   "source": [
    "all_combined_directory = folder_path3  + time_series_resample + '_' + school_name\n",
    "combined_pivot_mean.to_csv(all_combined_directory + '_mean_pivot.csv', index=False)\n",
    "combined_pivot_mean.to_excel(all_combined_directory + '_mean_pivot.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1k6HtClxyLM",
    "outputId": "c875c23e-586d-4feb-d3d4-71a934a63072"
   },
   "outputs": [],
   "source": [
    "directory_path6 = platform_directory + '/' + folder_project + '/' + folder_ready + '/'+ folder_year + '/' + school_name + '/' + folder_export\n",
    "folder_path6 = directory_path6 + '/' + folder_pivot + '/' # replace with the folder path\n",
    "csv_files6 = [f for f in os.listdir(folder_path6) if f.endswith('median_pivot percent.xlsx') and f.startswith(time_series_resample + '_' +school_name)] # get list of csv files\n",
    "dfs6 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files6:\n",
    "    file_path = os.path.join(folder_path6, file) # get full path of csv file\n",
    "    df = pd.read_excel(file_path) # read csv file into a dataframe\n",
    "    dfs6.append(df) # append dataframe to list\n",
    "combined_pivot_med_percent = pd.concat(dfs6, ignore_index=True)\n",
    "\n",
    "print(folder_path6  + time_series_resample )\n",
    "\n",
    "# combined1['sound_a'] = np.where((20*(10**(combined1['sound_a']/20))*10**(-6)*1000) > 200, 200, (20*(10**(combined1['sound_a']/20))*10**(-6)*1000)).round(2)\n",
    "combined_pivot_med_percent['room_id'] = combined_pivot_med_percent['room_id'].astype(str)\n",
    "\n",
    "combined_pivot_med_percent['moment'] = combined_pivot_med_percent.apply(determine_batch_to_moment, axis=1)\n",
    "rename_spec_columns(combined_pivot_med_percent, '_med')\n",
    "combined_pivot_med_percent.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzlseId0xyLM",
    "outputId": "ae0aff8a-9298-4697-f7bb-0359022bf6aa"
   },
   "outputs": [],
   "source": [
    "directory_path5 = platform_directory + '/' + folder_project + '/' + folder_ready + '/'+ folder_year + '/' + school_name + '/' + folder_export\n",
    "folder_path5 = directory_path5 + '/' + folder_pivot + '/' # replace with the folder path\n",
    "csv_files5 = [f for f in os.listdir(folder_path5) if f.endswith('75_pivot percent.xlsx') and f.startswith(time_series_resample + '_' +school_name)] # get list of csv files\n",
    "dfs5 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files5:\n",
    "    file_path = os.path.join(folder_path5, file) # get full path of csv file\n",
    "    df = pd.read_excel(file_path) # read csv file into a dataframe\n",
    "    dfs5.append(df) # append dataframe to list\n",
    "combined_pivot_75_percent = pd.concat(dfs5, ignore_index=True)\n",
    "\n",
    "print(folder_path5  + time_series_resample )\n",
    "\n",
    "# combined1['sound_a'] = np.where((20*(10**(combined1['sound_a']/20))*10**(-6)*1000) > 200, 200, (20*(10**(combined1['sound_a']/20))*10**(-6)*1000)).round(2)\n",
    "combined_pivot_75_percent['room_id'] = combined_pivot_75_percent['room_id'].astype(str)\n",
    "\n",
    "combined_pivot_75_percent['moment'] = combined_pivot_75_percent.apply(determine_batch_to_moment, axis=1)\n",
    "rename_spec_columns(combined_pivot_75_percent, '_75')\n",
    "combined_pivot_75_percent.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txSZ0RAzxyLN",
    "outputId": "4635d276-5d29-4801-f05c-dd518e41586e"
   },
   "outputs": [],
   "source": [
    "directory_path4 = platform_directory + '/' + folder_project + '/' + folder_ready + '/'+ folder_year + '/' + school_name + '/' + folder_export\n",
    "folder_path4 = directory_path4 + '/' + folder_pivot + '/' # replace with the folder path\n",
    "csv_files4 = [f for f in os.listdir(folder_path4) if f.endswith('mean_pivot percent.xlsx') and f.startswith(time_series_resample + '_' +school_name)] # get list of csv files\n",
    "dfs4 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files4:\n",
    "    file_path = os.path.join(folder_path4, file) # get full path of csv file\n",
    "    df = pd.read_excel(file_path) # read csv file into a dataframe\n",
    "    dfs4.append(df) # append dataframe to list\n",
    "combined_pivot_mean_percent = pd.concat(dfs4, ignore_index=True)\n",
    "\n",
    "print(folder_path4 + time_series_resample )\n",
    "\n",
    "# combined1['sound_a'] = np.where((20*(10**(combined1['sound_a']/20))*10**(-6)*1000) > 200, 200, (20*(10**(combined1['sound_a']/20))*10**(-6)*1000)).round(2)\n",
    "combined_pivot_mean_percent['room_id'] = combined_pivot_mean_percent['room_id'].astype(str)\n",
    "\n",
    "combined_pivot_mean_percent['moment'] = combined_pivot_mean_percent.apply(determine_batch_to_moment, axis=1)\n",
    "rename_spec_columns(combined_pivot_mean_percent, '_ave')\n",
    "combined_pivot_mean_percent.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuCdM8ZnxyLN"
   },
   "outputs": [],
   "source": [
    "all_combined_directory = folder_path4  + time_series_resample + '_' + school_name\n",
    "combined_pivot_mean_percent.to_csv(all_combined_directory + '_mean_pivot percent.csv', index=False)\n",
    "combined_pivot_mean_percent.to_excel(all_combined_directory + '_mean_pivot percent.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnGz4bKcxyLN"
   },
   "source": [
    "## Repair data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbJPP3KsxyLN"
   },
   "outputs": [],
   "source": [
    "all_combined_directory = folder_path1  + time_series_resample + '_1_' + school_name + '_alldate'\n",
    "combined1.to_csv(all_combined_directory + '.csv', index=False)\n",
    "combined1.to_excel(all_combined_directory + '.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXXIuat7xyLN",
    "outputId": "0930d53d-4ba5-41a3-c41a-76093831c76f"
   },
   "outputs": [],
   "source": [
    "combined2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEOGCFdHxyLN",
    "outputId": "250a04e5-751b-452a-b326-d784ba27f6a8"
   },
   "outputs": [],
   "source": [
    "combined2['temp_coldhot'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REkMy9x7xyLN"
   },
   "outputs": [],
   "source": [
    "# all_combined_directory = folder_path2  + survey_resample + '_2_' + school_name + '_alldate'\n",
    "all_combined_directory = folder_path2  + survey_resample + '_2_' + school_name + '_alldate'\n",
    "combined2.to_csv(all_combined_directory + '.csv', index=False)\n",
    "combined2.to_excel(all_combined_directory + '.xlsx', index=False)\n",
    "# combined2.describe().to_excel(all_combined_directory + '.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mX2ziZfIxyLN",
    "outputId": "5042f442-74a0-40aa-fd64-8509f01b9563"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_excel_files(folder_path):\n",
    "    # Get a list of all Excel files in the folder\n",
    "    excel_files = [file for file in os.listdir(folder_path) if file.endswith('.xlsx')]\n",
    "\n",
    "    for file in excel_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "\n",
    "        # Fill NaN values in the 'temp_coldhot' column with 0\n",
    "        if 'temp_coldhot' in df.columns:\n",
    "            df['temp_coldhot'] = (df['temp_coldhot']*-1).fillna(0)\n",
    "\n",
    "        # Save the processed DataFrame back to the same file\n",
    "        df.to_excel(file_path, index=False)\n",
    "        print(f\"Processed and saved: {file_path}\")\n",
    "\n",
    "# Set the path to the folder containing the Excel files\n",
    "folder_path = folder_path2\n",
    "\n",
    "# Run the processing function\n",
    "process_excel_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAZU7s9KxyLN",
    "outputId": "65cca174-7734-40c0-e894-fec5f680d880"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_csv_files(folder_path):\n",
    "    # Get a list of all Excel files in the folder\n",
    "    excel_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "    for file in excel_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        # Read the Excel file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Fill NaN values in the 'temp_coldhot' column with 0\n",
    "        if 'temp_coldhot' in df.columns:\n",
    "            df['temp_coldhot'] = (df['temp_coldhot']*-1).fillna(0)\n",
    "\n",
    "        # Save the processed DataFrame back to the same file\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Processed and saved: {file_path}\")\n",
    "\n",
    "# Set the path to the folder containing the Excel files\n",
    "folder_path = folder_path2\n",
    "\n",
    "# Run the processing function\n",
    "process_csv_files(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suMwBIEAQqcA"
   },
   "source": [
    "## threshold check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wa2nhKLhQqcA"
   },
   "outputs": [],
   "source": [
    "df = combined1[['date', 'time', 'feel_temp', 'feel_air']].copy()\n",
    "# Assuming your DataFrame is named 'df'\n",
    "# Convert 'date' and 'time' columns to datetime and set it as index\n",
    "df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
    "df.set_index('datetime', inplace=True)\n",
    "df['feel_tempb'] = df['feel_temp'].copy()\n",
    "\n",
    "# Resample at daily frequency and calculate percentages\n",
    "daily_summary1 = df.resample('D').agg({\n",
    "    'feel_temp': lambda x: (x < 0).sum() #* 100,\n",
    "})\n",
    "\n",
    "daily_summary2 = df.resample('D').agg({\n",
    "    'feel_tempb': lambda x: (x >= 0).sum()# * 100\n",
    "})\n",
    "# Plotting\n",
    "#fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "daily_summary1.join(daily_summary2, how= 'left')#.plot(kind='bar', stacked=True, ax=ax)\n",
    "# Display the resulting DataFrame\n",
    "#print(daily_summary1.join(daily_summary2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ch3n_XQXQqcA"
   },
   "outputs": [],
   "source": [
    "df = combined1[['date', 'time', 'rh_a']].copy()\n",
    "# Assuming your DataFrame is named 'df'\n",
    "# Convert 'date' and 'time' columns to datetime and set it as index\n",
    "df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
    "df.set_index('datetime', inplace=True)\n",
    "df['rh_b'] = df['rh_a'].copy()\n",
    "\n",
    "# Resample at daily frequency and calculate percentages\n",
    "daily_summary1 = df.resample('D').agg({\n",
    "    'rh_a': lambda x: (x < 30).mean() * 100,\n",
    "})\n",
    "\n",
    "daily_summary2 = df.resample('D').agg({\n",
    "    'rh_b': lambda x: (x >= 30).mean() * 100\n",
    "})\n",
    "# Plotting\n",
    "#fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "daily_summary1.join(daily_summary2, how= 'left')#.plot(kind='bar', stacked=True, ax=ax)\n",
    "# Display the resulting DataFrame\n",
    "#print(daily_summary1.join(daily_summary2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2vmAc5oQqcA"
   },
   "outputs": [],
   "source": [
    "df = combined1[['date', 'time', 'co2_a']].copy()\n",
    "# Assuming your DataFrame is named 'df'\n",
    "# Convert 'date' and 'time' columns to datetime and set it as index\n",
    "df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
    "df.set_index('datetime', inplace=True)\n",
    "df['co2_b'] = df['co2_a'].copy()\n",
    "\n",
    "# Resample at daily frequency and calculate percentages\n",
    "daily_summary1 = df.resample('D').agg({\n",
    "    'co2_a': lambda x: (x < 1000).mean() * 100,\n",
    "})\n",
    "\n",
    "daily_summary2 = df.resample('D').agg({\n",
    "    'co2_b': lambda x: (x >= 1000).mean() * 100\n",
    "})\n",
    "# Plotting\n",
    "#fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "daily_summary1.join(daily_summary2, how= 'left')#.plot(kind='bar', stacked=True, ax=ax)\n",
    "# Display the resulting DataFrame\n",
    "#print(daily_summary1.join(daily_summary2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pZHhoDmQqcA"
   },
   "outputs": [],
   "source": [
    "df = combined1[['date', 'time', 'rh_a']].copy()\n",
    "# Assuming your DataFrame is named 'df'\n",
    "# Convert 'date' and 'time' columns to datetime and set it as index\n",
    "df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "# Define a function to categorize temperature levels\n",
    "def categorize_temperature(temp_level):\n",
    "    if temp_level < 30:\n",
    "        return 'below_20'\n",
    "    elif 30 < temp_level < 60:\n",
    "        return 'between_30_and_60'\n",
    "    else:\n",
    "        return 'above_60'\n",
    "\n",
    "# Apply the categorization function to create a new 'temp_category' column\n",
    "df['temp_category'] = df['rh_a'].apply(categorize_temperature)\n",
    "\n",
    "# Resample at daily frequency and calculate percentages\n",
    "daily_summary1 = df.resample('D').agg({\n",
    "    'temp_category': lambda x: (x == 'below_30').mean() * 100\n",
    "})\n",
    "\n",
    "daily_summary2 = df.resample('D').agg({\n",
    "    'temp_category': lambda x: (x == 'between_30_and_60').mean() * 100\n",
    "})\n",
    "\n",
    "daily_summary3 = df.resample('D').agg({\n",
    "    'temp_category': lambda x: (x == 'above_60').mean() * 100\n",
    "})\n",
    "\n",
    "daily_summary3#.join(daily_summary2, how= 'left').join(daily_summary3, how= 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjJeKO2wQqcA"
   },
   "outputs": [],
   "source": [
    "df = combined1[['date', 'time', 'temp_a']].copy()\n",
    "# Assuming your DataFrame is named 'df'\n",
    "# Convert 'date' and 'time' columns to datetime and set it as index\n",
    "df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "# Define a function to categorize temperature levels\n",
    "def categorize_temperature(temp_level):\n",
    "    if temp_level < 20:\n",
    "        return 'below_20'\n",
    "    elif 20 < temp_level < 22:\n",
    "        return 'between_20_and_22'\n",
    "    else:\n",
    "        return 'above_22'\n",
    "\n",
    "# Apply the categorization function to create a new 'temp_category' column\n",
    "df['temp_category'] = df['temp_a'].apply(categorize_temperature)\n",
    "\n",
    "# Resample at daily frequency and calculate percentages\n",
    "daily_summary1 = df.resample('D').agg({\n",
    "    'temp_category': lambda x: (x == 'below_20').mean() * 100\n",
    "})\n",
    "\n",
    "daily_summary2 = df.resample('D').agg({\n",
    "    'temp_category': lambda x: (x == 'between_20_and_22').mean() * 100\n",
    "})\n",
    "\n",
    "daily_summary3 = df.resample('D').agg({\n",
    "    'temp_category': lambda x: (x == 'above_22').mean() * 100\n",
    "})\n",
    "\n",
    "daily_summary3#.join(daily_summary2, how= 'left').join(daily_summary3, how= 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3-1csR3qAqX"
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(combined2, index='hour', aggfunc='sum'  , values=response_counting) #.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q037jqwHxyLO"
   },
   "source": [
    "# Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fG9QD4HWxyLO"
   },
   "outputs": [],
   "source": [
    "class_check = 27\n",
    "low_responserate = 0.4     #0.4\n",
    "high_responserate = 1.5\n",
    "\n",
    "df1 = combined1[combined1['room_id']==str(class_check)].copy()\n",
    "df2 = combined2[combined2['room_id']==str(class_check)].copy()\n",
    "df3 = data_pivot_mean[data_pivot_mean['room_id']==str(class_check)].copy()\n",
    "\n",
    "# Step 1: Filter DataFrames Based on `response_rate`\n",
    "df3_filtered = df3[df3['response_rate'] > low_responserate]\n",
    "filtered_dates = df3_filtered['date'].unique()\n",
    "\n",
    "df1_filtered = df1[df1['date'].isin(filtered_dates)]\n",
    "df2_filtered = df2[df2['date'].isin(filtered_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMtpPpcwxyLO"
   },
   "outputs": [],
   "source": [
    "print (df1_filtered['school_id'].unique())\n",
    "print (df1_filtered['room_id'].unique())\n",
    "print (df1_filtered['time'].unique())\n",
    "print (df1_filtered['date'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbJhVpRWxyLO"
   },
   "outputs": [],
   "source": [
    "[variable_check + ' > ' + str(value_check)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5VKR0lNxyLO"
   },
   "outputs": [],
   "source": [
    "# Calculate hours for specific temperature range in df1\n",
    "variable_check = 'temp_a'\n",
    "value_check_down = 20.5\n",
    "value_check_up = 21.5\n",
    "temp_filtered = df1_filtered[(df1_filtered[variable_check] <= value_check_down) ]\n",
    "hours_temp_range = temp_filtered.groupby('date').size() * (10 / 60)  # 10 minutes to hours\n",
    "hours_temp_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gvDht75xyLO"
   },
   "outputs": [],
   "source": [
    "# Calculate hours for specific temperature range in df1\n",
    "variable_check = 'temp_a'\n",
    "value_check_down = 20.5\n",
    "value_check_up = 21.5\n",
    "temp_filtered = df1_filtered[(df1_filtered['temp_a'] >= value_check_down) & (df1_filtered['temp_a'] <= value_check_up)]\n",
    "hours_temp_range = temp_filtered.groupby('date').size() * (10 / 60)  # 10 minutes to hours\n",
    "hours_temp_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIXv-ACcxyLO"
   },
   "outputs": [],
   "source": [
    "# Calculate hours for specific temperature range in df1\n",
    "variable_check = 'temp_a'\n",
    "value_check_down = 20.5\n",
    "value_check_up = 21.5\n",
    "temp_filtered = df1_filtered[ (df1_filtered['temp_a'] >= value_check_up)]\n",
    "hours_temp_range = temp_filtered.groupby('date').size() * (10 / 60)  # 10 minutes to hours\n",
    "hours_temp_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3Zv13ZfxyLO"
   },
   "outputs": [],
   "source": [
    "variable_check = 'co2_a'\n",
    "value_check = 900\n",
    "filtered_measurement = variable_check + ' > ' + str(value_check)\n",
    "# Step 2: Calculate Duration of CO2 > 1000\n",
    "df1_filtered[filtered_measurement] = df1_filtered[variable_check] > value_check\n",
    "duration_above = df1_filtered.groupby('date')[filtered_measurement].sum() * 10 / 60  # converting to hours\n",
    "duration_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXJqsJnxxyLO"
   },
   "outputs": [],
   "source": [
    "variable_check = 'voc_a'\n",
    "value_check = 220\n",
    "filtered_measurement = variable_check + ' > ' + str(value_check)\n",
    "# Step 2: Calculate Duration of CO2 > 1000\n",
    "df1_filtered[filtered_measurement] = df1_filtered[variable_check] > value_check\n",
    "duration_above = df1_filtered.groupby('date')[filtered_measurement].sum() * 10 / 60  # converting to hours\n",
    "duration_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IY4_7IuxyLO"
   },
   "outputs": [],
   "source": [
    "print(duration_above.sum())\n",
    "print(duration_above.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PM-AvywlxyLO"
   },
   "outputs": [],
   "source": [
    "duration_above# Step 3: Pie Chart and Counting Table for `col_air`\n",
    "col_air_counts = df2_filtered[df2_filtered['season']==2]['feel_air'].value_counts().sort_index()\n",
    "col_air_percentage = col_air_counts / col_air_counts.sum() * 100\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.pie(col_air_counts, labels=col_air_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of col_air')\n",
    "plt.show()\n",
    "\n",
    "print(\"Counting Table for col_air:\")\n",
    "print(col_air_counts)\n",
    "print('total responses',df2_filtered['feel_air'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7_x4GAXxyLO"
   },
   "outputs": [],
   "source": [
    "df2_filtered[is_col_main + is_col_subquestion].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RJHstrsxyLO"
   },
   "outputs": [],
   "source": [
    "df2_filtered['season'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzTOckbbxyLO"
   },
   "outputs": [],
   "source": [
    "df2_filtered[df2_filtered['feel_temp']>=5]['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNaAROrKxyLO"
   },
   "outputs": [],
   "source": [
    "summary_table = df2_filtered[df2_filtered['season']==4][is_col_main] # Get value_counts for each column and combine them into a new dataframe\n",
    "summary_df = pd.concat([summary_table[col].value_counts() for col in summary_table.columns], axis=1).sort_index()\n",
    "summary_df.columns = summary_table.columns\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5HFZuD-xyLP"
   },
   "outputs": [],
   "source": [
    "# Define the replacement dictionary\n",
    "replace_dict = {-2: 'dissatisf.', -1: 'dissatisf.', 1: 'satisfied', 2: 'satisfied'}\n",
    "# replace_dict2 = {1: -2, 7:-2, 6:-1, 2:-1, 5:1, 3:1, 4:2}\n",
    "df2_plotgraph = df2_filtered[(df2_filtered['season']!=3)]#.copy()\n",
    "# df2_plotgraph['feel_air'] = df2_plotgraph[( df2_plotgraph['date']>= '2023-12-16') & ( df2_plotgraph['date']<= '2023-04-01')]['feel_air']. replace (replace_dict2)\n",
    "df2_plotgraph[is_col_main] = df2_filtered[is_col_main]. replace (replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JC1e-kmMxyLP"
   },
   "outputs": [],
   "source": [
    "df2_plotgraph[is_col_main] .nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkkCyS_gxyLP"
   },
   "outputs": [],
   "source": [
    "measure_check = 'co2_a'\n",
    "measure_unit = '( ppm )'\n",
    "perception_check = 'feel_air'\n",
    "\n",
    "\n",
    "# Plot the boxenplot of CO2 values\n",
    "fig, ax1 =  plt.subplots(figsize=(5, 3))\n",
    "# ax1 = plt.subplot(211)\n",
    "sns.histplot( data=df2_plotgraph.set_index('datetime'),x='date',  multiple= 'fill', hue=  perception_check ,kde=True, element= 'poly',  stat='proportion',  palette=\"light:#5A9\")    #, hue_order= hue_ordered_list ,legend='auto', stat='percent',\n",
    "plt.xticks( horizontalalignment='center',rotation=45)\n",
    "ax1.set_title(measure_check + ' vs. ' + perception_check + ' | Room ' + str(class_check))\n",
    "\n",
    "# Plot the 100% stacked bar chart\n",
    "ax2 = ax1. twinx()\n",
    "# df2_col_air_percentage.plot(kind='bar', stacked=True, ax=ax2, colormap='viridis', alpha=0.7)\n",
    "sns.boxplot(x='date', y=measure_check, data=df1_filtered, ax=ax2, fill=False, color= 'red',  linewidth=1.5, width=0.5)\n",
    "plt.xticks( horizontalalignment='center',rotation=45)\n",
    "ax2.set_ylabel(measure_unit)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# valid_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKLcm5pTqAqY"
   },
   "source": [
    "# pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9BQqX0mxyLP"
   },
   "source": [
    "Idebank Algorithm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "id": "rml_sB-exyLP"
   },
   "outputs": [],
   "source": [
    "#IDEBANK ALGORITHM -1\n",
    "\n",
    "is_limit_value = 0.2\n",
    "temp_coldhot_limit = 1\n",
    "dissatisfaction_limit = 0.2\n",
    "\n",
    "def IA_V_toohot (row):\n",
    "    if row['rd_temp'] < dissatisfaction_limit:\n",
    "        return 0\n",
    "    elif row['temp_a'] >= 22:\n",
    "        if row['temp_coldhot'] > temp_coldhot_limit :\n",
    "            return 1\n",
    "        elif row['temp_coldhot'] < temp_coldhot_limit :\n",
    "            if row['temp_heater'] >= is_limit_value:\n",
    "                return 6\n",
    "            else:\n",
    "                return 7\n",
    "    elif row['temp_a'] <= 22:\n",
    "        if row['temp_coldhot'] >= temp_coldhot_limit :\n",
    "            if row['bright_sun'] >= is_limit_value:\n",
    "                return 4\n",
    "            else:\n",
    "                return 7\n",
    "        else:\n",
    "            if row['floor_first'] == 1:\n",
    "                return 7\n",
    "            else:\n",
    "                if row['bright_sun'] >= is_limit_value:\n",
    "                    return 3\n",
    "    return 0  # Default case if none of the conditions are met\n",
    "\n",
    "\n",
    "def IA_V_toocold(row):\n",
    "    if row['rd_temp'] <= dissatisfaction_limit:\n",
    "        return 0\n",
    "    elif row['temp_a'] >= 20:\n",
    "        return 0\n",
    "    elif row['temp_coldhot'] < -temp_coldhot_limit:\n",
    "        if row['batch'] == 'morning':\n",
    "            return 5\n",
    "        else:\n",
    "            return 2\n",
    "    elif row['temp_coldhot'] > temp_coldhot_limit:\n",
    "        return 9\n",
    "    else:\n",
    "        if row['temp_draw'] >= is_limit_value:\n",
    "            return 6\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "def IA_V_ventilation(row):\n",
    "    if row['rd_air'] <= dissatisfaction_limit:\n",
    "        return 0\n",
    "    elif row['co2_a'] < 900:\n",
    "        if row['press_a'] <= 990 or row['press_a'] >= 1012:\n",
    "            return 4\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        if row['air_heavy'] >= is_limit_value:\n",
    "            return 3\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "# Apply the function to create the 'too_hot' column\n",
    "def IA_all_generator (df):\n",
    "    df['ia_too_hot']  = df.apply (IA_V_toohot, axis=1)\n",
    "    df['ia_too_cold'] = df.apply (IA_V_toocold, axis=1)\n",
    "    df['ia_vent']     = df.apply (IA_V_ventilation, axis=1)\n",
    "\n",
    "IA_columns = ['ia_too_hot', 'ia_too_cold', 'ia_vent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjzhtUXkxyLP"
   },
   "source": [
    "Pivoter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "id": "VGghOy8jqAqY"
   },
   "outputs": [],
   "source": [
    "main_index = ['date' ,school_col_name,room_col_name,'batch']\n",
    "all_is_col_main = is_col_main1 + is_col_main2 + is_col_main3 + is_col_main4\n",
    "# combined1['batch'] = combined1['hour'] .apply( lambda x: 'morning' if x<=8 else ('afternoon' if  10<x<13 else 'skip'))\n",
    "# combined2['batch'] = combined2['hour'].apply( lambda x: 'morning' if x<=8 else ('afternoon' if  10<x<13 else 'skip'))\n",
    "# combined1 = combined1[combined1['hour']<=afternoon_hour_limit]\n",
    "# combined2 = combined2[combined2['hour']<=afternoon_hour_limit]\n",
    "\n",
    "combined1['batch'] = combined1.apply(determine_batch_allschools1, axis=1)\n",
    "combined2['batch'] = combined2.apply(determine_batch_allschools2, axis=1)\n",
    "# combined1['batch'] = combined1.apply(determine_batch_brannfjell1, axis=1)\n",
    "# combined2['batch'] = combined2.apply(determine_batch_allschools, axis=1)\n",
    "combined1['moment'] = combined1.apply(determine_batch_to_moment, axis=1)\n",
    "combined2['moment'] = combined2.apply(determine_duration_stay, axis=1)\n",
    "\n",
    "data_measure_agg    = combined1[combined1['hour']<=afternoon_hour_limit] [main_index + airthingsDevice + sd_meaninterpolate_col + weather_data\n",
    "                                                                          + ['hura_d']\n",
    "                                                                          ].copy()\n",
    "data_measure_median = combined1[main_index + sd_fill_col + ['students']].copy()\n",
    "data_response_mean  = combined2[combined2['hour']<=afternoon_hour_limit] [main_index + is_col_main + ['temp_coldhot', 'season']].copy()\n",
    "data_response_sum   = combined2[combined2['hour']<=afternoon_hour_limit] [main_index + all_is_col_main  + [response_counting] + is_col_subquestion_coldhot ].drop(columns=['temp_coldhot'],axis=1).copy()\n",
    "\n",
    "dpvt_measure_mean    = pd.pivot_table(data_measure_agg, index= main_index, aggfunc='mean'  ).round(2)\n",
    "\n",
    "dpvt_measure_med    = pd.pivot_table(data_measure_agg, index= main_index, aggfunc= 'median'  ).round(2)    #FOR AGGFUNC=   select:    lambda x: np.percentile(x, 75)   #aggfunc_mode\n",
    "# dpvt_measure_med.columns = [f'{column}_med' for column in dpvt_measure_med.columns]\n",
    "dpvt_measure_75    = pd.pivot_table(data_measure_agg, index= main_index, aggfunc=lambda x: np.percentile(x, 75)  ).round(2)\n",
    "# dpvt_measure_75.columns = [f'{column}_q75' for column in dpvt_measure_75.columns]\n",
    "dpvt_measure_median  = pd.pivot_table(data_measure_median, index= main_index, aggfunc='median' )\n",
    "dpvt_response_mean   = pd.pivot_table(data_response_mean, index= main_index, aggfunc='mean' ).round(3)\n",
    "dpvt_response_median = pd.pivot_table(data_response_mean, index= main_index, aggfunc='median' ).round(3)\n",
    "dpvt_response_sum    = pd.pivot_table(data_response_sum, index= main_index, aggfunc='sum' )#.apply(dissatisfaction_creator, axis=1)\n",
    "dissatisfaction_creator(dpvt_response_sum)\n",
    "# dissatisfaction_aggregator(dpvt_response_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined1['batch'].value_counts())\n",
    "print(combined2['batch'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "zG6UglRFqAqY"
   },
   "outputs": [],
   "source": [
    "directory_path3 = platform_directory + '/' + folder_project + '/' + folder_ready + '/'+ folder_year + '/' + school_name + '/' + folder_export\n",
    "is_col_subquestion_percent = combined2[is_col_subquestion_coldhot].drop('temp_coldhot', axis=1).columns\n",
    "\n",
    "data_pivot_75 = dpvt_measure_75.join(dpvt_measure_median\n",
    "                                           ).join(dpvt_response_median          #variation\n",
    "                                                  ).join(dpvt_response_sum\n",
    "                                                         ).reset_index().dropna(subset=is_col_main, axis=0)\n",
    "data_pivot_75['response_rate'] = data_pivot_75[response_counting]/data_pivot_75['students']\n",
    "# data_pivot_75 = data_pivot_75.drop(all_is_col_main, axis=1)[data_pivot_75['batch']!='skip']\n",
    "# data_pivot_75['floor_first']  = data_pivot_75.apply(assign_room_id, axis=1)\n",
    "data_pivot_75['temp_coldhot'] = data_pivot_75['temp_coldhot'].fillna(0)\n",
    "data_pivot_75 = add_room_features(data_pivot_75, room_protokoll_pivoting)\n",
    "IA_all_generator (data_pivot_75)\n",
    "\n",
    "# rename_spec_columns(data_pivot_75, '_75')\n",
    "data_pivot_75['moment'] = data_pivot_75.apply(determine_batch_to_moment, axis=1)\n",
    "data_pivot_75.to_excel(directory_path3 + '/'+ 'pivot' + '/'+ time_series_resample + '_'   +school_name+'_'  + '75_pivot.xlsx', index=False)\n",
    "\n",
    "data_pivot_75_percent = data_pivot_75.copy()\n",
    "data_pivot_75_percent.loc[:,is_col_subquestion_percent]  = (data_pivot_75_percent.loc[:,is_col_subquestion_percent].div(data_pivot_75_percent[response_counting], axis=0)*100).round(3)\n",
    "data_pivot_75_percent.to_excel(directory_path3 + '/'+ 'pivot' + '/'+ time_series_resample + '_'   +school_name+'_'  + '75_pivot percent.xlsx', index=False)\n",
    "\n",
    "data_pivot_med = dpvt_measure_med.join(dpvt_measure_median\n",
    "                                           ).join(dpvt_response_median          #variation\n",
    "                                                  ).join(dpvt_response_sum\n",
    "                                                         ).reset_index().dropna(subset=is_col_main, axis=0)\n",
    "data_pivot_med['response_rate'] = data_pivot_med[response_counting]/data_pivot_med['students']\n",
    "# data_pivot_med = data_pivot_med.drop(all_is_col_main, axis=1)[data_pivot_med['batch']!='skip']\n",
    "# data_pivot_med['floor_first']  = data_pivot_med.apply(assign_room_id, axis=1)\n",
    "data_pivot_med['temp_coldhot'] = data_pivot_med['temp_coldhot'].fillna(0)\n",
    "data_pivot_med = add_room_features(data_pivot_med, room_protokoll_pivoting)\n",
    "IA_all_generator (data_pivot_med)\n",
    "\n",
    "# rename_spec_columns(data_pivot_med, '__med')\n",
    "data_pivot_med['moment'] = data_pivot_med.apply(determine_batch_to_moment, axis=1)\n",
    "data_pivot_med.to_excel(directory_path3 + '/'+ 'pivot' + '/'+ time_series_resample + '_'   +school_name+'_'  + 'median_pivot.xlsx', index=False)\n",
    "\n",
    "data_pivot_med_percent = data_pivot_med.copy()\n",
    "data_pivot_med_percent.loc[:,is_col_subquestion_percent]  = (data_pivot_med_percent.loc[:,is_col_subquestion_percent].div(data_pivot_med_percent[response_counting], axis=0)*100).round(3)\n",
    "data_pivot_med_percent.to_excel(directory_path3 + '/'+ 'pivot' + '/'+ time_series_resample + '_'   +school_name+'_'  + 'median_pivot percent.xlsx', index=False)\n",
    "\n",
    "data_pivot_mean = dpvt_measure_mean.join(dpvt_measure_median\n",
    "                                    ).join(dpvt_response_mean                   #variation\n",
    "                                                  ).join(dpvt_response_sum\n",
    "                                                         ).reset_index().dropna(subset=is_col_main, axis=0)\n",
    "data_pivot_mean['response_rate'] = data_pivot_mean[response_counting]/data_pivot_mean['students']\n",
    "# data_pivot_mean = data_pivot_mean.drop(all_is_col_main, axis=1)[data_pivot_mean['batch']!='skip']\n",
    "# data_pivot_mean['floor_first'] = data_pivot_mean.apply(assign_room_id, axis=1)\n",
    "data_pivot_mean['temp_coldhot'] = data_pivot_mean['temp_coldhot'].fillna(0)\n",
    "data_pivot_mean = add_room_features(data_pivot_mean, room_protokoll_pivoting)\n",
    "IA_all_generator (data_pivot_mean)\n",
    "\n",
    "# rename_spec_columns(data_pivot_mean, '__ave')\n",
    "data_pivot_mean['moment'] = data_pivot_mean.apply(determine_batch_to_moment, axis=1)\n",
    "data_pivot_mean.to_excel(directory_path3 + '/'+ 'pivot' + '/'+ time_series_resample + '_'   +school_name+'_'  + 'mean_pivot.xlsx', index=False)\n",
    "\n",
    "data_pivot_mean_percent = data_pivot_mean.copy()\n",
    "data_pivot_mean_percent.loc[:,is_col_subquestion_percent]  = (data_pivot_mean_percent.loc[:,is_col_subquestion_percent].div(data_pivot_mean_percent[response_counting], axis=0)*100).round(3)\n",
    "data_pivot_mean_percent.to_excel(directory_path3 + '/'+ 'pivot' + '/'+ time_series_resample + '_'   +school_name+'_'  + 'mean_pivot percent.xlsx', index=False)\n",
    "#END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(directory_path3 + '/'+ 'pivot' + '/'+ time_series_resample + '_'   +school_name+'_'  + 'mean_pivot percent.xlsx')\n",
    "print(data_pivot_mean_percent['batch'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PIVOTING USING ONLY COMBINED2 DATA\n",
    "main_index = ['date' ,school_col_name,room_col_name,'batch']\n",
    "combined1 = combined1[combined1['hour']<=13]\n",
    "# combined2 = combined2[combined2['hour']<=afternoon_hour_limit]\n",
    "\n",
    "combined1['batch'] = combined1.apply(determine_batch_allschools1, axis=1)\n",
    "# combined2['batch'] = combined2.apply(determine_batch_allschools2, axis=1)\n",
    "\n",
    "data_measure_agg    = combined2 [main_index + airthingsDevice + sd_meaninterpolate_col + weather_data\n",
    "                                                                          + [ 'hura_d']\n",
    "                                                                          ].copy()\n",
    "data_measure_median = combined1 [main_index + sd_fill_col + ['students']].copy()\n",
    "data_response_mean  = combined2 [main_index + is_col_main + ['temp_coldhot', 'season']].copy()\n",
    "data_response_sum   = combined2 [main_index + all_is_col_main  + [response_counting] + is_col_subquestion ].drop(columns=['temp_coldhot'],axis=1).copy()\n",
    "\n",
    "dpvt_measure_mean    = pd.pivot_table(data_measure_agg, index= main_index, aggfunc='mean'  ).round(2)\n",
    "\n",
    "dpvt_measure_med    = pd.pivot_table(data_measure_agg, index= main_index, aggfunc= 'median'  ).round(2)    #FOR AGGFUNC=   select:    lambda x: np.percentile(x, 75)   #aggfunc_mode\n",
    "# dpvt_measure_med.columns = [f'{column}_med' for column in dpvt_measure_med.columns]\n",
    "dpvt_measure_75    = pd.pivot_table(data_measure_agg, index= main_index, aggfunc=lambda x: np.percentile(x, 75)  ).round(2)\n",
    "# dpvt_measure_75.columns = [f'{column}_q75' for column in dpvt_measure_75.columns]\n",
    "dpvt_measure_median  = pd.pivot_table(data_measure_median, index= main_index, aggfunc='median' ).astype(float)\n",
    "dpvt_response_mean   = pd.pivot_table(data_response_mean, index= main_index, aggfunc='mean' ).round(3)\n",
    "dpvt_response_median = pd.pivot_table(data_response_mean, index= main_index, aggfunc='median' ).round(3)\n",
    "dpvt_response_sum    = pd.pivot_table(data_response_sum, index= main_index, aggfunc='sum' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path3 = platform_directory + '/' + folder_project + '/' + folder_ready + '/'+ folder_year + '/' + school_name + '/' + folder_export\n",
    "is_col_subquestion_percent = combined2[is_col_subquestion].drop('temp_coldhot', axis=1).columns\n",
    "\n",
    "data_pivot_75 = dpvt_measure_75.join(\n",
    "    # dpvt_measure_median).join(\n",
    "        dpvt_response_median).join(\n",
    "            dpvt_response_sum).reset_index().dropna(subset=is_col_main, axis=0)\n",
    "\n",
    "dissatisfaction_creator(data_pivot_75)\n",
    "data_pivot_75['temp_coldhot'] = data_pivot_75['temp_coldhot'].fillna(0)\n",
    "data_pivot_75 = add_room_features(data_pivot_75, room_protokoll_brannfjell)\n",
    "data_pivot_75['room_id'] = data_pivot_75['room_id'].astype(str)\n",
    "data_pivot_75['response_rate'] = data_pivot_75[response_counting]/data_pivot_75['students']\n",
    "\n",
    "# rename_spec_columns(data_pivot_75, '_75')\n",
    "data_pivot_75['moment'] = data_pivot_75.apply(determine_batch_to_moment, axis=1)\n",
    "\n",
    "data_pivot_75_percent = data_pivot_75.copy()\n",
    "data_pivot_75_percent.loc[:,is_col_subquestion_percent]  = (data_pivot_75_percent.loc[:,is_col_subquestion_percent].div(data_pivot_75_percent[response_counting], axis=0)*100).round(3)\n",
    "\n",
    "data_pivot_med = dpvt_measure_med.join(\n",
    "    # dpvt_measure_median).join(\n",
    "        dpvt_response_median).join(\n",
    "            dpvt_response_sum).reset_index().dropna(subset=is_col_main, axis=0)\n",
    "        \n",
    "dissatisfaction_creator(data_pivot_med)\n",
    "data_pivot_med['temp_coldhot'] = data_pivot_med['temp_coldhot'].fillna(0)\n",
    "data_pivot_med = add_room_features(data_pivot_med, room_protokoll_brannfjell)\n",
    "data_pivot_med['room_id'] = data_pivot_med['room_id'].astype(str)\n",
    "data_pivot_med['response_rate'] = data_pivot_med[response_counting]/data_pivot_med['students']\n",
    "\n",
    "# rename_spec_columns(data_pivot_med, '__med')\n",
    "data_pivot_med['moment'] = data_pivot_med.apply(determine_batch_to_moment, axis=1)\n",
    "\n",
    "data_pivot_med_percent = data_pivot_med.copy()\n",
    "data_pivot_med_percent.loc[:,is_col_subquestion_percent]  = (data_pivot_med_percent.loc[:,is_col_subquestion_percent].div(data_pivot_med_percent[response_counting], axis=0)*100).round(3)\n",
    "\n",
    "data_pivot_mean = dpvt_measure_mean.join(\n",
    "    # dpvt_measure_median).join(\n",
    "        dpvt_response_median).join(\n",
    "            dpvt_response_sum).reset_index().dropna(subset=is_col_main, axis=0)\n",
    "        \n",
    "dissatisfaction_creator(data_pivot_mean)\n",
    "data_pivot_mean['temp_coldhot'] = data_pivot_mean['temp_coldhot'].fillna(0)\n",
    "data_pivot_mean = add_room_features(data_pivot_mean, room_protokoll_brannfjell)\n",
    "data_pivot_mean['room_id'] = data_pivot_mean['room_id'].astype(str)\n",
    "data_pivot_mean['response_rate'] = data_pivot_mean[response_counting]/data_pivot_mean['students']\n",
    "\n",
    "# rename_spec_columns(data_pivot_mean, '__ave')\n",
    "data_pivot_mean['moment'] = data_pivot_mean.apply(determine_batch_to_moment, axis=1)\n",
    "data_pivot_mean.to_excel(directory_path3 + '/'+ 'pivot' + '/'+ time_series_resample + '_'   +school_name+'_'  + 'mean_pivot.xlsx', index=False)\n",
    "\n",
    "data_pivot_mean_percent = data_pivot_mean.copy()\n",
    "data_pivot_mean_percent.loc[:,is_col_subquestion_percent]  = (data_pivot_mean_percent.loc[:,is_col_subquestion_percent].div(data_pivot_mean_percent[response_counting], axis=0)*100).round(3)\n",
    "\n",
    "data_pivot_med.to_excel(directory_path3 + '/'+ 'pivot' + '/'+ time_series_resample + '_'   +school_name+'_'  + 'median_pivot.xlsx', index=False)\n",
    "data_pivot_med_percent.to_excel(directory_path3 + '/'+ 'pivot' + '/'+ time_series_resample + '_'   +school_name+'_'  + 'median_pivot percent.xlsx', index=False)\n",
    "data_pivot_75.to_excel(directory_path3 + '/'+ 'pivot' + '/'+ time_series_resample + '_'   +school_name+'_'  + '75_pivot.xlsx', index=False)\n",
    "data_pivot_75_percent.to_excel(directory_path3 + '/'+ 'pivot' + '/'+ time_series_resample + '_'   +school_name+'_'  + '75_pivot percent.xlsx', index=False)\n",
    "data_pivot_mean.to_excel(directory_path3 + '/'+ 'pivot' + '/'+ time_series_resample + '_'   +school_name+'_'  + 'mean_pivot.xlsx', index=False)\n",
    "data_pivot_mean_percent.to_excel(directory_path3 + '/'+ 'pivot' + '/'+ time_series_resample + '_'   +school_name+'_'  + 'mean_pivot percent.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Stu0Zjo3xyLP"
   },
   "outputs": [],
   "source": [
    "plot_bar = pd.pivot_table(combined2#[combined2['batch']=='afternoon']\n",
    "                          , response_counting, index= ['batch','season','room_id','date'], aggfunc='sum').reset_index().sort_values(by=['date'])   # .to_excel(directory_path3 + '/'+  '_' + 'graph.xlsx', index=True)    #.sum()\n",
    "plot_bar.to_excel(directory_path3 + '/'+  '_' + 'graph.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8MrbSqvxyLP"
   },
   "outputs": [],
   "source": [
    "melting_process = combined2[['room_id','season']+is_col_main].copy()  #[['room_id','season']+is_col_main]  #[combined2['room_id']==str(class3)]\n",
    "# Melt the dataframe to long format\n",
    "answer_replace2 = {0:'dont know',2:'satisfied' , 1:'satisfied', -1:'dissatisf.', -2:'dissatisf.'}\n",
    "melting_process[is_col_main] = melting_process[is_col_main].replace(answer_replace2)\n",
    "melted_df = melting_process.melt(id_vars=['room_id','season'], value_vars=['feel_air', 'feel_temp', 'feel_health', 'feel_bright', 'feel_noise'],\n",
    "                    var_name='category', value_name='value')\n",
    "\n",
    "# Create the pivot table\n",
    "pivot_table = pd.pivot_table(\n",
    "    melted_df,\n",
    "    index=['room_id','season', 'value'],\n",
    "    columns='category',\n",
    "    aggfunc='size',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Rename the 'value' index level to 'unique'\n",
    "pivot_table.index.names = ['room_id','season', 'value']\n",
    "pivot_table.reset_index()  .to_excel(directory_path3 + '/'+  '_' + 'pivot responses overview 2.xlsx', index=True)\n",
    "pivot_table.reset_index( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tpOTCihxyLP"
   },
   "outputs": [],
   "source": [
    "melting_process = combined2[['room_id','season']+is_col_main].copy()  #[['room_id','season']+is_col_main]  #[combined2['room_id']==str(class3)]\n",
    "# Melt the dataframe to long format\n",
    "melting_process[is_col_main] = melting_process[is_col_main]#.replace(answer_replace2)\n",
    "melted_df = melting_process.melt(id_vars=['room_id','season'], value_vars=['feel_air', 'feel_temp', 'feel_health', 'feel_bright', 'feel_noise'],\n",
    "                    var_name='category', value_name='value')\n",
    "\n",
    "# Create the pivot table\n",
    "pivot_table = pd.pivot_table(\n",
    "    melted_df,\n",
    "    index=['room_id','season', 'value'],\n",
    "    columns='category',\n",
    "    aggfunc='size',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Rename the 'value' index level to 'unique'\n",
    "pivot_table.index.names = ['room_id','season', 'value']\n",
    "pivot_table.reset_index()  .to_excel(directory_path3 + '/'+  '_' + 'pivot responses overview 4.xlsx', index=True)\n",
    "pivot_table.reset_index( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImDDilEnxyLP"
   },
   "outputs": [],
   "source": [
    "print(directory_path3 + '/'+  '_' + 'pivot.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvPi5hf3xyLP"
   },
   "outputs": [],
   "source": [
    "plot_bar = pd.pivot_table(combined2[combined2['batch']!='skip'], response_counting, index= ['batch','season','room_id','date'], aggfunc='sum').reset_index().sort_values(by=['date'])   # .to_excel(directory_path3 + '/'+  '_' + 'graph.xlsx', index=True)    #.sum()\n",
    "plot_bar.to_excel(directory_path3 + '/'+  '_' + 'graph.xlsx', index=True)\n",
    "\n",
    "plt.figure(figsize=(14, 3))\n",
    "sns.barplot(data=plot_bar, x='date', y='responses', hue='room_id', width=0.5)\n",
    "plt.xticks( horizontalalignment='center',rotation=45)\n",
    "plt.yticks([0,10,20,30,40,50])\n",
    "plt.title(school_name + ' ' + str(plot_bar['batch'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqytNOUmxyLP"
   },
   "outputs": [],
   "source": [
    "batch_moment = ' '\n",
    "min_respon_rate = 0.1\n",
    "input_data_heatmap= combined_pivot_mean_percent\n",
    "funct_heatmap= 'pearson'\n",
    "data_plot_heatmap = input_data_heatmap[input_data_heatmap['response_rate']>=min_respon_rate]#[input_data_heatmap['batch']==batch_moment]\n",
    "heatmap_corrvalue(data_plot_heatmap[ weather_data + airthingsAll+ ['moment','temp_coldhot']  + is_main_rd     # 'floor_first', 'season' ,\n",
    "                                    ]#.drop(['pmv','ppd', 'rain_o'], axis=1)\n",
    "                  , funct_heatmap , (18,10)  ,'pivot - '+ school_name + ' ' + batch_moment, 10  )\n",
    "\n",
    "heatmap_corrvalue(data_plot_heatmap[ weather_data + airthingsAll+ [ 'moment']  + is_col_subquestion ]#[input_data_heatmap['batch']==batch_moment]\n",
    "                  , funct_heatmap , (20,11)  ,'pivot - '+ school_name + ' '   + batch_moment\n",
    "                  , 7  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbn-t5oaxyLP"
   },
   "outputs": [],
   "source": [
    "data_plot_heatmap = combined_pivot_mean_percent#[combined_pivot_mean_percent['batch']=='afternoon']\n",
    "heatmap_corrvalue(data_plot_heatmap[ weather_data + airthingsAll+ [ 'floor_first','moment','temp_coldhot']  + is_col_subquestion ]\n",
    "                  , 'pearson', (20,11)  ,'pivot - '+ school_name + ' '   + 'afternoon'\n",
    "                  , 7  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8oq-9XTgxyLP"
   },
   "outputs": [],
   "source": [
    "data_plot_heatmap = combined_pivot_mean_percent[combined_pivot_mean_percent['batch']=='afternoon'][combined_pivot_mean_percent['floor_first']==0]\n",
    "heatmap_corrvalue(data_plot_heatmap[ weather_data + airthingsDevice+ [ 'floor_first','moment','temp_coldhot']  + is_col_subquestion ]\n",
    "                  , 'pearson', (25,12)  ,'pivot - '+ school_name + ' ', 9 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISkAKYoIxyLQ"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data = data_pivot_mean,   #[data_pivot_mean['batch']=='afternoon']\n",
    "                x= 'co2_a', y= 'rd_air', size= 'response_rate', hue= 'room_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KmeDJLjxyLQ"
   },
   "outputs": [],
   "source": [
    "print( 'IA_too Hot :' + str(Counter(data_pivot_med['ia_too_hot'])))\n",
    "print( 'IA_too Cold :' + str(Counter(data_pivot_med['ia_too_cold'])))\n",
    "print( 'IA_Vent :' + str(Counter(data_pivot_med['ia_vent'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoZ1ssGWxyLQ"
   },
   "outputs": [],
   "source": [
    "print( 'IA_too Hot :' + str(Counter(data_pivot_mean['ia_too_hot'])))\n",
    "print( 'IA_too Cold :' + str(Counter(data_pivot_mean['ia_too_cold'])))\n",
    "print( 'IA_Vent :' + str(Counter(data_pivot_mean['ia_vent'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0SJ7NP5qAqY"
   },
   "outputs": [],
   "source": [
    "x = 'temp_a'\n",
    "y = 'rd_air'\n",
    "hue = 'floor_first'\n",
    "data_plot =  combined_pivot_mean[(combined_pivot_mean['response_rate' ]>=0.5) & (combined_pivot_mean['response_rate']<=2)]\n",
    "size = 'response_rate'\n",
    "sns.scatterplot(data=data_plot, x=x, y=y, hue=hue , palette=\"tab10\"  ,  size=size )    #[data_pivot[response_counting]>=20]\n",
    "plt.legend( bbox_to_anchor=(1.04, 0.5), loc=\"upper left\")\n",
    "# regression_params = {}\n",
    "# for B, color in zip(data_plot[hue].unique(), sns.color_palette()):\n",
    "#     subset = df[df[hue] == B]\n",
    "#     X = subset[x]\n",
    "#     Y = subset[y]\n",
    "#     slope, intercept = np.polyfit(X, Y, 2)\n",
    "#     regression_params[hue] = (slope, intercept)\n",
    "\n",
    "#     # Annotate the plot with regression equations\n",
    "#     plt.annotate(f'room_{B}: y = {slope2:.4f}x2 + {slope:.4f}x + {intercept:.2f}',\n",
    "#                  xy=(max(X), max(Y)), color=color, fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyaNnECFqAqY"
   },
   "outputs": [],
   "source": [
    "x = 'voc_a'\n",
    "y = 'feel_air'\n",
    "hue =  'room_id' #  'batch'    # 'room_id'\n",
    "data_plot =  data_pivot_mean#[(data_pivot_mean['response_rate' ]>=0.5) & (data_pivot_mean['response_rate']<=1.5)]\n",
    "size = 'response_rate'\n",
    "sns.lmplot(x=x, y=y, hue=hue, data= data_plot, legend=False,  logx=True)   #[data_pivot[response_counting]>=20]\n",
    "plt.legend( bbox_to_anchor=(1.04, 0.5), loc=\"upper left\")\n",
    "# regression_params = {}\n",
    "# for B, color in zip(data_plot[hue].unique(), sns.color_palette()):\n",
    "#     subset = df[df[hue] == B]\n",
    "#     X = subset[x]\n",
    "#     Y = subset[y]\n",
    "#     slope2, slope, intercept = np.polyfit(X, Y, 2)\n",
    "#     regression_params[hue] = (slope, intercept)\n",
    "\n",
    "#     # Annotate the plot with regression equations\n",
    "#     plt.annotate(f'room_{B}: y = {slope2:.4f}x + {slope:.4f}x + {intercept:.3f}',\n",
    "#                  xy=(max(X), max(Y)), color=color, fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuphcIpnqAqY"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "# sns.scatterplot(data=data_pivot_mean[(data_pivot_mean['response_rate' ]>=0.7) & (data_pivot_mean['response_rate']<=1.5)] , x='voc_a', y= 'feel_air', hue='room_id' , palette=\"tab10\"  ,  size='response_rate' )    #[data_pivot[response_counting]>=20]   ,  size=response_counting  #[(data_pivot_mean_percent[response_counting]>10) & (data_pivot_mean_percent[response_counting]<50)]\n",
    "x = 'voc_a'\n",
    "y = 'feel_air'\n",
    "hue = 'room_id'\n",
    "df  = data_pivot_mean#[(data_pivot_mean['response_rate' ]>=0.7) & (data_pivot_mean['response_rate']<=1.5)]\n",
    "sns.lmplot(x=x, y=y, hue=hue , data=  df, legend=False)\n",
    "#[(data_pivot_mean['response_rate' ]>=0.8) & (data_pivot_mean['response_rate']<=1.0)]\n",
    "plt.legend( bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "regression_params = {}\n",
    "for b, color in zip(df[hue].unique(), sns.color_palette()):\n",
    "    subset = df[df[hue] == b]\n",
    "    X = subset[x]\n",
    "    Y = subset[y]\n",
    "    slope, intercept, r, p = linregress(X, Y)\n",
    "    regression_params[hue] = {'slope': slope, 'intercept': intercept, 'r_squared': r**2}\n",
    "\n",
    "    # Annotate the plot with regression equations and R-squared values\n",
    "    annotation = f'{hue}: y = {slope:.2f}x + {intercept:.2f}, R² = {r**2:.2f}'\n",
    "    plt.annotate(annotation,\n",
    "                 xy=(max(X), max(Y)),\n",
    "                 color=color,\n",
    "                 fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6GqRa7_DCqI"
   },
   "source": [
    "# Plot Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qni3gs21ZRsh"
   },
   "outputs": [],
   "source": [
    "plt.hist(combined1['response_rate']*100)\n",
    "plt.xlabel ('response rate (%)')\n",
    "plt.ylabel ('quantity (rows data)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShBD0Y0pq6wN"
   },
   "outputs": [],
   "source": [
    "combined1['response_rate'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmuE5Vu7hUxX"
   },
   "outputs": [],
   "source": [
    "combined1[response_counting].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ANYYV5vwGeF"
   },
   "outputs": [],
   "source": [
    "plotcheck1[response_counting].describe()#[response_counting].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find specific date\n",
    "specific_date = combined1[(combined1['school_id']=='brannfjell') & (combined1['rs_noise']>=0) ]['date'].unique()\n",
    "specific_date1 = combined1[(combined1['school_id']=='brannfjell') & (combined1['rs_noise']>=0)& (combined1['room_id']==str(class1)) ]['date'].unique()\n",
    "specific_date2 = combined1[(combined1['school_id']=='brannfjell') & (combined1['rs_noise']>=0)& (combined1['room_id']==str(class2)) ]['date'].unique()\n",
    "specific_date3 = combined1[(combined1['school_id']=='brannfjell') & (combined1['rs_noise']>=0)& (combined1['room_id']==str(class3)) ]['date'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_school = combined1[(combined1['school_id']=='brannfjell') & combined1['date'].isin(specific_date)]\n",
    "data_class1 = combined1[(combined1['school_id']=='brannfjell') & combined1['date'].isin(specific_date1)& (combined1['room_id']==str(class1))]\n",
    "data_class2 = combined1[(combined1['school_id']=='brannfjell') & combined1['date'].isin(specific_date2)& (combined1['room_id']==str(class2))]\n",
    "data_class3 = combined1[(combined1['school_id']=='brannfjell') & combined1['date'].isin(specific_date3)& (combined1['room_id']==str(class3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data_school [is_main_rd], kde=True,  stat='frequency', color='orange', cumulative=True  ,alpha=0.3, label=\"Density\", binwidth=0.1)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histo_cummulative (data, stat_model, variable_measure, extra_title, size  ):\n",
    "    # Create a figure and axis\n",
    "    fig, ax1 = plt.subplots(figsize=(size, size*0.6))\n",
    "\n",
    "    # Plot the distribution (KDE or histogram)\n",
    "    sns.histplot(data, kde=True, ax=ax1, stat=stat_model, color='orange', alpha=0.1, label=\"Density\")\n",
    "\n",
    "    # Create a twin axis to overlay the cumulative plot\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Plot the cumulative distribution function (CDF)\n",
    "    sns.ecdfplot(data, ax=ax2, color='red', label=\"Cumulative\", linewidth=2)\n",
    "\n",
    "    # Customize the labels for both axes\n",
    "    ax1.set_xlabel(variable_measure, fontsize=size*1.5)\n",
    "    ax1.set_ylabel(\"Density\", fontsize=size*1.5, color='blue')\n",
    "    ax2.set_ylabel(\"Cumulative\", fontsize=size*1.5, color='red')\n",
    "\n",
    "    # Align the tick colors with their respective axis\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "    # Add a legend\n",
    "    # ax1.legend(loc='upper left')\n",
    "    # ax2.legend(loc='upper right')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.title(\"Distribution and Cumulative Plot : \" + extra_title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all day data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo_cummulative(data_class1['temp_a'], 'proportion', 'temperature (oC)', ' -room' + str(class1), 5 )\n",
    "histo_cummulative(data_class1['co2_a'], 'proportion', 'CO2 (ppm)', 'all-room' + str(class1), 5 )\n",
    "histo_cummulative(data_class1['voc_a'], 'proportion', 'VOC (ppb)', 'all-room' + str(class1), 5 )\n",
    "histo_cummulative(data_class2['temp_a'], 'proportion', 'temperature (oC)', ' -room' + str(class2), 5 )\n",
    "histo_cummulative(data_class2['co2_a'], 'proportion', 'CO2 (ppm)', 'all-room' + str(class2), 5 )\n",
    "histo_cummulative(data_class2['voc_a'], 'proportion', 'VOC (ppb)', 'all-room' + str(class2), 5 )\n",
    "histo_cummulative(data_class3['temp_a'], 'proportion', 'temperature (oC)', ' -room' + str(class3), 5 )\n",
    "histo_cummulative(data_class3['co2_a'], 'proportion', 'CO2 (ppm)', 'all-room' + str(class3), 5 )\n",
    "histo_cummulative(data_class3['voc_a'], 'proportion', 'VOC (ppb)', 'all-room' + str(class3), 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "morning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo_cummulative(data_class1[data_class1['hour'].between(6,8)]['temp_a'], 'proportion', 'temperature (oC)', 'morning-room' + str(class1), 5 )\n",
    "histo_cummulative(data_class1[data_class1['hour'].between(6,8)]['co2_a'], 'proportion', 'CO2 (ppm)', 'morning-room' + str(class1), 5 )\n",
    "histo_cummulative(data_class1[data_class1['hour'].between(6,8)]['voc_a'], 'proportion', 'CVOC (ppb)', 'morning-room' + str(class1), 5 )\n",
    "histo_cummulative(data_class2[data_class2['hour'].between(6,8)]['temp_a'], 'proportion', 'temperature (oC)', 'morning-room' + str(class2), 5 )\n",
    "histo_cummulative(data_class2[data_class2['hour'].between(6,8)]['co2_a'], 'proportion', 'CO2 (ppm)', 'morning-room' + str(class2), 5 )\n",
    "histo_cummulative(data_class2[data_class2['hour'].between(6,8)]['voc_a'], 'proportion', 'CVOC (ppb)', 'morning-room' + str(class2), 5 )\n",
    "histo_cummulative(data_class3[data_class3['hour'].between(6,8)]['temp_a'], 'proportion', 'temperature (oC)', 'morning-room' + str(class3), 5 )\n",
    "histo_cummulative(data_class3[data_class3['hour'].between(6,8)]['co2_a'], 'proportion', 'CO2 (ppm)', 'morning-room' + str(class3), 5 )\n",
    "histo_cummulative(data_class3[data_class3['hour'].between(6,8)]['voc_a'], 'proportion', 'CVOC (ppb)', 'morning-room' + str(class3), 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo_cummulative(data_class1[data_class1['hour'].between(10,12)]['temp_a'], 'proportion', 'temperature (oC)', 'afternoon-room' + str(class1), 5)\n",
    "histo_cummulative(data_class1[data_class1['hour'].between(10,13)]['co2_a'], 'proportion', 'CO2 (ppm)', 'afternoon-room' + str(class1), 5 )\n",
    "histo_cummulative(data_class1[data_class1['hour'].between(10,13)]['voc_a'], 'proportion', 'VOC (ppb)', 'afternoon-room' + str(class1), 5 )\n",
    "histo_cummulative(data_class2[data_class2['hour'].between(10,12)]['temp_a'], 'proportion', 'temperature (oC)', 'afternoon-room' + str(class2), 5)\n",
    "histo_cummulative(data_class2[data_class2['hour'].between(10,13)]['co2_a'], 'proportion', 'CO2 (ppm)', 'afternoon-room' + str(class2), 5 )\n",
    "histo_cummulative(data_class2[data_class2['hour'].between(10,13)]['voc_a'], 'proportion', 'VOC (ppb)', 'afternoon-room' + str(class2), 5 )\n",
    "histo_cummulative(data_class3[data_class3['hour'].between(10,12)]['temp_a'], 'proportion', 'temperature (oC)', 'afternoon-room' + str(class3), 5)\n",
    "histo_cummulative(data_class3[data_class3['hour'].between(10,13)]['co2_a'], 'proportion', 'CO2 (ppm)', 'afternoon-room' + str(class3), 5 )\n",
    "histo_cummulative(data_class3[data_class3['hour'].between(10,13)]['voc_a'], 'proportion', 'VOC (ppb)', 'afternoon-room' + str(class3), 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the distribution (KDE or histogram)\n",
    "sns.histplot(data_class1['temp_a'], kde=True, ax=ax1, stat='density', color='blue', alpha=0.6, label=\"Density\")\n",
    "\n",
    "# Create a twin axis to overlay the cumulative plot\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the cumulative distribution function (CDF)\n",
    "sns.ecdfplot(data_class1['temp_a'], ax=ax2, color='red', label=\"Cumulative\", linewidth=2)\n",
    "\n",
    "# Customize the labels for both axes\n",
    "ax1.set_xlabel(\"Value\", fontsize=12)\n",
    "ax1.set_ylabel(\"Density\", fontsize=12, color='blue')\n",
    "ax2.set_ylabel(\"Cumulative Probability\", fontsize=12, color='red')\n",
    "\n",
    "# Align the tick colors with their respective axis\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Add a legend\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.title(\"Distribution and Cumulative Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBn5cru65pqO"
   },
   "source": [
    "## 1. Feel Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7N0Picqi-97N"
   },
   "outputs": [],
   "source": [
    "pd_check = 'ad_temp'\n",
    "ps_check = 'as_temp'\n",
    "feel_check = 'feel_temp'\n",
    "input_columns = is_main_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Klpv4BhbQqcF"
   },
   "outputs": [],
   "source": [
    "dfplot1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PrG7BKb9QqcF"
   },
   "outputs": [],
   "source": [
    "input1 = combined1#[(combined1['response_rate'] >= 0.4 )& (combined1['response_rate'] <= 1.3)]\n",
    "# input2 = plotcheck1m\n",
    "# input3 = plotcheck1\n",
    "input2 = plotcheck1m\n",
    "input3 = plotcheck1\n",
    "index = ['date','hour','room_id']\n",
    "values2 = ['responses'] + input_columns + ['response_rate']\n",
    "dfplot1 = pd.pivot_table(input1, values=values_temp2, index=index).round(2).join(pd.pivot_table(input1, values=values2, aggfunc='sum', index=index).round(2), how='outer')\n",
    "dfplot2 = pd.pivot_table(input2, values=values_temp2, index=index).round(2).join(pd.pivot_table(input2, values=values2, aggfunc='sum', index=index).round(2), how='outer')\n",
    "dfplot3 = pd.pivot_table(input3, values=values_temp2, index=index).round(2).join(pd.pivot_table(input3, values=values2, aggfunc='sum', index=index).round(2), how='outer')\n",
    "\n",
    "dissatisfaction_rate(dfplot3, input_columns, 1, pd_check, ps_check)\n",
    "dissatisfaction_rate(dfplot2, input_columns, 1, pd_check, ps_check)\n",
    "dissatisfaction_rate(dfplot1, input_columns, 1, pd_check, ps_check)\n",
    "dissatisfaction_rate2(dfplot1, input_columns, 1, pd_check+'2', ps_check+'2')\n",
    "dfplot1['hura_del'] = dfplot1['hura_a'] - dfplot1['hura_o']\n",
    "dfplot1['enth_del'] = dfplot1['enth_a'] - dfplot1['enth_o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ofal5lBkQqcF"
   },
   "outputs": [],
   "source": [
    "dfplot1[dfplot1['responses']>0].reset_index().to_excel('C:/Users/azimilga/final project/export/'+ time_series_resample + '_'+school_name + '_'  + feel_check +'.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGybsy3jQqcF"
   },
   "outputs": [],
   "source": [
    "y_axis_label = '(Air Temp.) dissatisfaction Rate (%)'\n",
    "plotter (dfplot1, 'temp_a',  temp_a_label ,pd_check,  y_axis_label, response_counting,  'linear',  'linear')\n",
    "plotter (dfplot2, 'temp_a',  temp_a_label ,pd_check,  y_axis_label, response_counting,  'linear',  'linear')\n",
    "plotter (dfplot3, 'temp_a',  temp_a_label ,pd_check,  y_axis_label, response_counting,  'linear',  'linear')\n",
    "plotter (dfplot1, 'enth_a',  enth_a_label ,pd_check,  y_axis_label, response_counting,  'linear',  'linear')\n",
    "plotter (dfplot2, 'enth_a',  enth_a_label ,pd_check,  y_axis_label, response_counting,  'linear',  'linear')\n",
    "plotter (dfplot3, 'enth_a',  enth_a_label ,pd_check,  y_axis_label, response_counting,  'linear',  'linear')\n",
    "plotter (dfplot1, 'hura_a',  hura_a_label ,pd_check,  y_axis_label, response_counting,  'linear',  'linear')\n",
    "plotter (dfplot2, 'hura_a',  hura_a_label ,pd_check,  y_axis_label, response_counting,  'linear',  'linear')\n",
    "plotter (dfplot3, 'hura_a',  hura_a_label ,pd_check,  y_axis_label, response_counting,  'linear',  'linear')\n",
    "plotter (dfplot1, 'rh_a',  rh_a_label ,pd_check,  y_axis_label, response_counting,  'linear',  'linear')\n",
    "plotter (dfplot2, 'rh_a',  rh_a_label ,pd_check,  y_axis_label, response_counting,  'linear',  'linear')\n",
    "plotter (dfplot3, 'rh_a',  rh_a_label ,pd_check,  y_axis_label, response_counting,  'linear',  'linear')\n",
    "#plotter (dfplot1, 'temp_f',  temp_f_label ,pd_check, y_axis_label, response_counting,  'linear',  'linear')\n",
    "#plotter (dfplot2, 'temp_f',  temp_f_label ,pd_check,  y_axis_label, response_counting,  'linear',  'linear')\n",
    "#plotter (dfplot1, 'temp_v',  temp_v_label ,pd_check,  y_axis_label, response_counting,  'linear',  'log')\n",
    "#plotter (dfplot2, 'temp_v',  temp_v_label ,pd_check,  y_axis_label, response_counting,  'linear',  'linear')\n",
    "#plotter (dfplot1, 'ppd',  'ppd_level' ,pd_check,  y_axis_label, response_counting, 'linear',  'linear')\n",
    "#plotter (dfplot2, 'ppd',  'ppd_level' ,pd_check, y_axis_label, response_counting, 'linear',  'linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XItZzSrIdA6"
   },
   "source": [
    "## 2. Feel Air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIB6sWAjIdBJ"
   },
   "outputs": [],
   "source": [
    "pd_check = 'ad_air'\n",
    "ps_check = 'as_air'\n",
    "feel_check = 'feel_air'\n",
    "values_air1 = values_air2 + ['response_rate',pd_check, ps_check]\n",
    "input_columns = is_main_air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YoF9aWEQQqcF"
   },
   "outputs": [],
   "source": [
    "input1 = combined1 #[(combined1['response_rate'] >= 0.4 )& (combined1['response_rate'] <= 1.3)]\n",
    "input2 = plotcheck1m\n",
    "input3 = plotcheck1\n",
    "index = ['date','hour','room_id']\n",
    "values2 = ['responses'] + input_columns + ['response_rate']\n",
    "dfplot1 = pd.pivot_table(input1, values=values_air2, index=index).round(2).join(pd.pivot_table(input1, values=values2, aggfunc='sum', index=index).round(2), how='outer')\n",
    "dfplot2 = pd.pivot_table(input2, values=values_air2, index=index).round(2).join(pd.pivot_table(input2, values=values2, aggfunc='sum', index=index).round(2), how='outer')\n",
    "dfplot3 = pd.pivot_table(input3, values=values_air2, index=index).round(2).join(pd.pivot_table(input3, values=values2, aggfunc='sum', index=index).round(2), how='outer')\n",
    "\n",
    "rate_coefficient = 1\n",
    "dissatisfaction_rate(dfplot3, input_columns, rate_coefficient, pd_check, ps_check)\n",
    "dissatisfaction_rate(dfplot2, input_columns, rate_coefficient, pd_check, ps_check)\n",
    "dissatisfaction_rate(dfplot1, input_columns, rate_coefficient, pd_check, ps_check)\n",
    "dissatisfaction_rate2(dfplot1, input_columns, rate_coefficient, pd_check+'2', ps_check+'2')\n",
    "dfplot1['hura_del'] = dfplot1['hura_a'] - dfplot1['hura_o']\n",
    "dfplot1['enth_del'] = dfplot1['enth_a'] - dfplot1['enth_o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TObsgmu8QqcG"
   },
   "outputs": [],
   "source": [
    "dfplot1[dfplot1['responses']>0].reset_index().to_excel('C:/Users/azimilga/final project/export/'+ time_series_resample + '_'+school_name + '_'  + feel_check +'.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBd8WAYBpBXv"
   },
   "outputs": [],
   "source": [
    "y_axis_label = '(air) Dissatisfaction Rate (%)'\n",
    "plotter (dfplot1, 'co2_a',  co2_a_label ,pd_check, y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot2, 'co2_a',  co2_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot3, 'co2_a',  co2_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'voc_a',  voc_a_label ,pd_check,  y_axis_label, response_counting , 'linear', 'linear')\n",
    "plotter (dfplot2, 'voc_a',  voc_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot3, 'voc_a',  voc_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'temp_a',  temp_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'log')\n",
    "plotter (dfplot2, 'temp_a',  temp_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'temp_v',  temp_v_label ,pd_check,  y_axis_label, response_counting, 'linear', 'log')\n",
    "plotter (dfplot2, 'temp_v',  temp_v_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'temp_f',  temp_f_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot2, 'temp_f',  temp_f_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'rdn_a',  rdn_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot2, 'rdn_a',  rdn_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'bright_a',  bright_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot2, 'bright_a',  bright_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'rh_a',  rh_a_label ,pd_check,  y_axis_label, response_counting, 'log', 'linear')\n",
    "plotter (dfplot2, 'rh_a',  rh_a_label ,pd_check,  y_axis_label, response_counting, 'log', 'linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfZf8MReOMwK"
   },
   "source": [
    "## 3. Feel Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GzNNTQhOMwL"
   },
   "outputs": [],
   "source": [
    "pd_check = 'ad_health'\n",
    "ps_check = 'as_health'\n",
    "feel_check = 'feel_health'\n",
    "input_columns = is_main_health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1ZRyVRuOMwL"
   },
   "outputs": [],
   "source": [
    "input1 = combined1 #[(combined1['response_rate'] >= 0.4 )& (combined1['response_rate'] <= 1.3)]\n",
    "input2 = plotcheck1m\n",
    "imput3 = plotcheck1\n",
    "index = ['date','hour','room_id']\n",
    "values2 = ['responses'] + input_columns + ['response_rate']\n",
    "dfplot1 = pd.pivot_table(input1, values=values_health2, index=index).round(2).join(pd.pivot_table(input1, values=values2, aggfunc='sum', index=index).round(2), how='outer')\n",
    "dfplot2 = pd.pivot_table(input2, values=values_health2, index=index).round(2).join(pd.pivot_table(input2, values=values2, aggfunc='sum', index=index).round(2), how='outer')\n",
    "dfplot3 = pd.pivot_table(input3, values=values_health2, index=index).round(2).join(pd.pivot_table(input3, values=values2, aggfunc='sum', index=index).round(2), how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfcJGNZnQqcG"
   },
   "outputs": [],
   "source": [
    "dissatisfaction_rate(dfplot2, input_columns, 1, pd_check, ps_check)\n",
    "dissatisfaction_rate(dfplot1, input_columns, 1, pd_check, ps_check)\n",
    "dissatisfaction_rate(dfplot3, input_columns, 1, pd_check, ps_check)\n",
    "dissatisfaction_rate2(dfplot1, input_columns, 1, pd_check+'2', ps_check+'2')\n",
    "dfplot1['hura_del'] = dfplot1['hura_a'] - dfplot1['hura_o']\n",
    "dfplot1['enth_del'] = dfplot1['enth_a'] - dfplot1['enth_o']\n",
    "\n",
    "dfplot1[dfplot1['responses']>0].reset_index().to_excel('C:/Users/azimilga/final project/export/'+ time_series_resample + '_'+school_name + '_'  + feel_check +'.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4xLA6ybOMwM"
   },
   "outputs": [],
   "source": [
    "y_axis_label = '(Healthiness) Dissatisfaction Rate (%)'\n",
    "plotter (dfplot1, 'co2_a',  co2_a_label ,pd_check, y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot2, 'co2_a',  co2_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot3, 'co2_a',  co2_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'voc_a',  voc_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot2, 'voc_a',  voc_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot3, 'voc_a',  voc_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'hura_a',  hura_a_label ,pd_check,  y_axis_label, response_counting , 'linear', 'linear')\n",
    "plotter (dfplot2, 'hura_a',  hura_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'temp_a',  temp_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot2, 'temp_a',  temp_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot3, 'temp_a',  temp_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'rh_a',  rh_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot2, 'rh_a',  rh_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "# plotter (dfplot1, 'temp_v',  temp_v_label ,pd_check,  y_axis_label, response_counting, 'linear', 'log')\n",
    "# plotter (dfplot2, 'temp_v',  temp_v_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "# plotter (dfplot1, 'temp_f',  temp_f_label ,pd_check,  y_axis_label, response_counting, 'linear', 'log')\n",
    "# plotter (dfplot2, 'temp_f',  temp_f_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'rdn_a',  rdn_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'log')\n",
    "plotter (dfplot2, 'rdn_a',  rdn_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "\n",
    "\n",
    "plotter (dfplot1, 'bright_a',  bright_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'log')\n",
    "plotter (dfplot2, 'bright_a',  bright_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LPW4_KIJ4zY"
   },
   "source": [
    "## 4. Feel Bright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u31KVstoJ4zY"
   },
   "outputs": [],
   "source": [
    "pd_check = 'ad_bright'\n",
    "ps_check = 'as_bright'\n",
    "feel_check = 'feel_bright'\n",
    "input_columns = is_main_bright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1j_cNsCOphXR"
   },
   "outputs": [],
   "source": [
    "input1 = combined1[combined1['bright_a']>=1]\n",
    "input2 = plotcheck1[plotcheck1['bright_a']>=1]\n",
    "index = ['date','hour','room_id']\n",
    "values2 = ['responses'] + input_columns + ['response_rate']\n",
    "dfplot1 = pd.pivot_table(input1, values=values_air2, index=index).round(2).join(pd.pivot_table(input1, values=values2, aggfunc='sum', index=index).round(2), how='outer')\n",
    "dfplot2 = pd.pivot_table(input2, values=values_air2, index=index).round(2).join(pd.pivot_table(input2, values=values2, aggfunc='sum', index=index).round(2), how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZavegLnpnUN"
   },
   "outputs": [],
   "source": [
    "dissatisfaction_rate(dfplot2, input_columns, 1, pd_check, ps_check)\n",
    "dissatisfaction_rate(dfplot1, input_columns, 1, pd_check, ps_check)\n",
    "dissatisfaction_rate2(dfplot1, input_columns, 1, pd_check+'2', ps_check+'2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSV5YeUlQqcG"
   },
   "outputs": [],
   "source": [
    "dfplot1[dfplot1['responses']>0].reset_index().to_excel('C:/Users/azimilga/final project/export/'+ time_series_resample + '_'+school_name + '_'  + feel_check +'.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LW9ysTPGJ4zY"
   },
   "outputs": [],
   "source": [
    "y_axis_label = '(Room Brightness) dissatisfaction (%)'\n",
    "plotter (dfplot1, 'co2_a',  co2_a_label ,pd_check, y_axis_label, response_counting, 'linear', 'log')\n",
    "plotter (dfplot2, 'co2_a',  co2_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'bright_a' ,  bright_a_label ,pd_check,  y_axis_label, response_counting , 'linear', 'linear')\n",
    "plotter (dfplot2, 'bright_a' ,  bright_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'temp_a',  temp_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'log')\n",
    "plotter (dfplot2, 'temp_a',  temp_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'temp_o',  temp_o_label ,pd_check,  y_axis_label, response_counting, 'linear', 'log')\n",
    "plotter (dfplot2, 'temp_o',  temp_o_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'voc_a',  voc_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'log')\n",
    "plotter (dfplot2, 'voc_a',  voc_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'rh_o',  rh_o_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot2, 'rh_o',  rh_o_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCumLxagLDeO"
   },
   "source": [
    "## 5. Feel Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJPWYfg-LDeV"
   },
   "outputs": [],
   "source": [
    "pd_check = 'ad_noise'\n",
    "ps_check = 'as_noise'\n",
    "feel_check = 'feel_noise'\n",
    "input_columns = is_main_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pi9ekn0qF2Ft"
   },
   "outputs": [],
   "source": [
    "input1 = combined1[combined1['sound_a']>=1]\n",
    "input2 = plotcheck1[plotcheck1['sound_a']>=1]\n",
    "index = ['date','hour','room_id']\n",
    "values2 = ['responses'] + input_columns + ['response_rate']\n",
    "dfplot1 = pd.pivot_table(input1, values=values_noise2, index=index).round(2).join(pd.pivot_table(input1, values=values2, aggfunc='sum', index=index).round(2), how='outer')\n",
    "dfplot2 = pd.pivot_table(input2, values=values_noise2, index=index).round(2).join(pd.pivot_table(input2, values=values2, aggfunc='sum', index=index).round(2), how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpsUYj7QQqcH"
   },
   "outputs": [],
   "source": [
    "combined1[combined1['responses']>0].reset_index().to_excel('C:/Users/azimilga/final project/export/'+ time_series_resample + '_'+school_name + '_' +'combined2.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRf9YUA0LDeV"
   },
   "outputs": [],
   "source": [
    "dissatisfaction_rate(dfplot2, input_columns, 1, pd_check, ps_check)\n",
    "dissatisfaction_rate(dfplot1, input_columns, 1, pd_check, ps_check)\n",
    "dissatisfaction_rate2(dfplot1, input_columns, 1, pd_check+'2', ps_check+'2')\n",
    "\n",
    "dfplot1['hura_del'] = dfplot1['hura_a'] - dfplot1['hura_o']\n",
    "dfplot1['enth_del'] = dfplot1['enth_a'] - dfplot1['enth_o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrT_OymwQqcH"
   },
   "outputs": [],
   "source": [
    "dfplot1[dfplot1['responses']>0].reset_index().to_excel('C:/Users/azimilga/final project/export/'+ time_series_resample + '_'+school_name + '_'  + feel_check +'.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8Jy1YhsQqcH"
   },
   "outputs": [],
   "source": [
    "y_axis_label = '(Room Noise) Dissatisfaction Rate (%)'\n",
    "plotter (dfplot1, 'co2_a',  co2_a_label ,'sound_a', sound_a_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot2, 'co2_a',  co2_a_label ,'sound_a',  sound_a_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'rh_a',  rh_a_label ,'sound_a', sound_a_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot2, 'rh_a',  rh_a_label ,'sound_a',  sound_a_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'sound_a',  sound_a_label ,pd_check, y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot2, 'sound_a',  sound_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'co2_a',  co2_a_label ,pd_check, y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot2, 'co2_a',  co2_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'enth_a',  enth_a_label ,pd_check,  y_axis_label, response_counting , 'linear', 'linear')\n",
    "plotter (dfplot2, 'enth_a',  enth_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'voc_a',  voc_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot2, 'voc_a',  voc_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot1, 'rh_a',  rh_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')\n",
    "plotter (dfplot2, 'rh_a',  rh_a_label ,pd_check,  y_axis_label, response_counting, 'linear', 'linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WZP8-AVEhs5"
   },
   "source": [
    "## 6. Temp Cold Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-kKfwrvEhtK"
   },
   "outputs": [],
   "source": [
    "pd_check = 'ad_temp'\n",
    "ps_check = 'as_temp'\n",
    "feel_check = 'temp_coldhot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTH7ZyPxAR0u"
   },
   "outputs": [],
   "source": [
    "dfplot1 = combined2[combined2[feel_check]<=4] #[[feel_check]+values_temp2]\n",
    "dfplot2 = plotcheck2[plotcheck2[feel_check]<=4]#[[feel_check]+values_temp2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpSStttaQqcH"
   },
   "outputs": [],
   "source": [
    "dfplot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJPaLYRgQqcH"
   },
   "outputs": [],
   "source": [
    "y_axis_label = 'COLD    or     HOT'\n",
    "# plotter_noresponse (dfplot1, 'temp_f',  temp_a_label ,feel_check, y_axis_label, 'linear', 'linear')\n",
    "# plotter_noresponse (dfplot2, 'temp_f',  temp_a_label ,feel_check,  y_axis_label, 'linear', 'linear')\n",
    "# plotter_noresponse (dfplot1, 'temp_v',  temp_a_label ,feel_check, y_axis_label, 'linear', 'linear')\n",
    "# plotter_noresponse (dfplot2, 'temp_v',  temp_a_label ,feel_check,  y_axis_label,  'linear', 'linear')\n",
    "plotter_noresponse (dfplot1, 'temp_a',  temp_a_label ,feel_check, y_axis_label,  'linear', 'linear')\n",
    "plotter_noresponse (dfplot2, 'temp_a',  temp_a_label ,feel_check,  y_axis_label,  'linear', 'linear')\n",
    "plotter_noresponse (dfplot1, 'enth_a',  enth_a_label ,feel_check,  y_axis_label, 'linear', 'linear')\n",
    "plotter_noresponse (dfplot2, 'enth_a',  enth_a_label ,feel_check,  y_axis_label, 'linear', 'linear')\n",
    "plotter_noresponse (dfplot1, 'voc_a',  voc_a_label ,feel_check,  y_axis_label,  'linear', 'linear')\n",
    "plotter_noresponse (dfplot2, 'voc_a',  voc_a_label ,feel_check,  y_axis_label,  'linear', 'linear')\n",
    "plotter_noresponse (dfplot1, 'rh_a',  rh_a_label ,feel_check,  y_axis_label, 'linear', 'linear')\n",
    "plotter_noresponse (dfplot2, 'rh_a',  rh_a_label ,feel_check,  y_axis_label, 'linear', 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPDuZ40n5887"
   },
   "outputs": [],
   "source": [
    "plt.scatter(dfplot[feel_check], dfplot['enth_a'])\n",
    "plt.ylabel('indoor entalphy (kJ/kg)')\n",
    "plt.xlabel(feel_check)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(dfplot[feel_check], dfplot['rh_a'])\n",
    "plt.ylabel(rh_a_label)\n",
    "plt.xlabel(feel_check)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(dfplot[feel_check], dfplot['temp_a'])\n",
    "plt.ylabel(temp_a_label)\n",
    "plt.xlabel(feel_check)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(dfplot[feel_check], dfplot['temp_v'])\n",
    "plt.ylabel(temp_v_label)\n",
    "plt.xlabel(feel_check)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(dfplot[feel_check], dfplot['temp_f'])\n",
    "plt.ylabel(temp_f_label)\n",
    "plt.xlabel(feel_check)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(dfplot[feel_check], dfplot['temp_o'])\n",
    "plt.ylabel(temp_o_label)\n",
    "plt.xlabel(feel_check)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(dfplot[feel_check], dfplot['rh_o'])\n",
    "plt.ylabel(rh_o_label)\n",
    "plt.xlabel(feel_check)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWuRG0PPEhtM"
   },
   "outputs": [],
   "source": [
    "input = plotcheck2[plotcheck2['temp_coldhot']<=4]\n",
    "index = feel_check\n",
    "dfplot= pd.pivot_table(input, values=values_temp2, index=index).round(2).join(pd.pivot_table(input, values=response_counting, aggfunc='count', index=index).round(2))\n",
    "dfplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQ5jkhXtBoih"
   },
   "outputs": [],
   "source": [
    "input = combined2[combined2['temp_coldhot']<=4]\n",
    "index = [feel_check]\n",
    "dfplot = pd.pivot_table(input, values=values_temp2, index=index).round(2).join(pd.pivot_table(input, values=response_counting, aggfunc='count', index=index).round(2))\n",
    "dfplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzLuQlN8H-6H"
   },
   "outputs": [],
   "source": [
    "input = combined2[combined2['temp_coldhot']<=4]\n",
    "index = ['hour',feel_check]\n",
    "dfplot = pd.pivot_table(input, values=values_temp2, index=index).round(2).join(pd.pivot_table(input, values=response_counting, aggfunc='count', index=index).round(2))\n",
    "dfplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOt90JKn_1Ha"
   },
   "outputs": [],
   "source": [
    "input = plotcheck2[plotcheck2['temp_coldhot']<=4]\n",
    "index = ['date','hour',feel_check]\n",
    "dfplot = pd.pivot_table(input, values=values_temp2, index=index).round(2).join(pd.pivot_table(input, values=response_counting, aggfunc='count', index=index).round(2))\n",
    "dfplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Okvb7U71_4o4"
   },
   "outputs": [],
   "source": [
    "input = combined2[combined2['temp_coldhot']<=4]\n",
    "index = ['date','hour',feel_check]\n",
    "dfplot = pd.pivot_table(input, values=values_temp2, index=index).round(2).join(pd.pivot_table(input, values=response_counting, aggfunc='count', index=index).round(2))\n",
    "dfplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mljq0wCsACPU"
   },
   "outputs": [],
   "source": [
    "input = combined2[combined2['temp_coldhot']<=4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCvmY8MMSrof"
   },
   "outputs": [],
   "source": [
    "plt.scatter(dfplot.reset_index()[feel_check], dfplot.reset_index()['temp_a'], s= dfplot.reset_index()[response_counting])\n",
    "plt.scatter(dfplot.reset_index()[feel_check], dfplot.reset_index()['temp_v'], s= dfplot.reset_index()[response_counting])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQ5aGvbcSrol"
   },
   "outputs": [],
   "source": [
    "plt.scatter(dfplot.reset_index()[feel_check], dfplot.reset_index()['temp_f'], s= dfplot.reset_index()[response_counting])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzTAMW7qIFNU"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(dfplot.reset_index(), x=  'temp_v' ,y= feel_check , edgecolors = 'b'  )\n",
    "sns.scatterplot(dfplot.reset_index(), x=  'temp_f' ,y= feel_check , edgecolors = 'r'  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-LBk-WkfjM1"
   },
   "outputs": [],
   "source": [
    "input = plotcheck2\n",
    "index = ['hour',feel_check]\n",
    "dfplot = pd.pivot_table(input, values=values_temp2, index=index).round(2).join(pd.pivot_table(input, values=response_counting, aggfunc='count', index=index).round(2))\n",
    "dfplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t835jgrehi-B"
   },
   "outputs": [],
   "source": [
    "input = plotcheck2\n",
    "index = ['date',feel_check]\n",
    "dfplot = pd.pivot_table(input, values=values_temp2, index=index).round(2).join(pd.pivot_table(input, values=response_counting, aggfunc='count', index=index).round(2))\n",
    "dfplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWcPPOpQfjM2"
   },
   "outputs": [],
   "source": [
    "plt.scatter(dfplot.reset_index()[feel_check], dfplot.reset_index()['temp_a'], s= dfplot.reset_index()[response_counting])\n",
    "plt.scatter(dfplot.reset_index()[feel_check], dfplot.reset_index()['temp_v'], s= dfplot.reset_index()[response_counting])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zAY92T3fw6T"
   },
   "outputs": [],
   "source": [
    "plt.scatter(dfplot.reset_index()[feel_check], dfplot.reset_index()['temp_a'], s= dfplot.reset_index()[response_counting])\n",
    "plt.scatter(dfplot.reset_index()[feel_check], dfplot.reset_index()['temp_f'], s= dfplot.reset_index()[response_counting])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axadpoeFEdqI"
   },
   "source": [
    "## Heatmap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGMzN0TuQqcJ"
   },
   "outputs": [],
   "source": [
    "dropped_columns = ['temp_coldhot', 'pmv', 'ppd','rain_o', 'hura_o', 'enth_o','winds_o','press_a', 'temp_s', 'swc_r', 'swc_f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xelLSuA7xyLT"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=combined_pivot_mean, x= 'co2_a', y='rd_air', hue='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntPPtaWgxyLT"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=combined_pivot_mean, x= 'temp_a', y='rd_temp', hue=room_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgyGPQjiQqcJ"
   },
   "outputs": [],
   "source": [
    "ieq_parameters = [# 'temp_v', #'temp_f' ,'temp_s',\n",
    "                   #'vent_v',\n",
    "                   'temp_a' , 'bright_a' #,'sound_a'\n",
    "                  ,'co2_a','rh_a', 'voc_a' , 'rdn_a'\n",
    "                  , 'enth_a', 'hura_a',\n",
    "                  'temp_o', 'rh_o','enth_o', 'hura_o', 'winds_o', 'rain_o'\n",
    "                  #,'pm25_a','pm1_a'\n",
    "                  ,'pm2.5_o','pm10_o'\n",
    "                  ,'hura_d'#,'enth_d'\n",
    "\n",
    "                  ]+is_main_rd\n",
    "# input_data_heatmap = combined1[combined1['room_id']!=class4][ieq_parameters]\n",
    "input_data_heatmap = combined_pivot_mean[combined_pivot_mean['response_rate']>=0.2][ieq_parameters].dropna()\n",
    "input_data_heatmap.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pWJV5XpznYE"
   },
   "outputs": [],
   "source": [
    "# input_heatmap = combined1[ieq_heatmap+is_col_main3+is_col_main4]#.iloc[:,1:29].drop(dropped_columns, axis=1)\n",
    "heatmap_corrvalue(input_data_heatmap.dropna(), 'All Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4fn8tV-F2Fw"
   },
   "outputs": [],
   "source": [
    "print(it is done okay?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3LCn535QqcJ"
   },
   "source": [
    "# Statistic Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHSZh96OQqcJ"
   },
   "outputs": [],
   "source": [
    "directory_path = platform_directory + '/' + folder_project + '/' + folder_ready + '/'+ folder_year + '/' + school_name + '/' + folder_export\n",
    "\n",
    "folder_path1 = directory_path + '/' + data_for1 + '/' # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path1) if f.endswith('.csv') and f.startswith('5min' +  '_1_')] # get list of csv files\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path1, file) # get full path of csv file\n",
    "    df = pd.read_csv(file_path, delimiter=',') # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list\n",
    "combined3 = pd.concat(dfs1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkLBbI-5QqcJ"
   },
   "outputs": [],
   "source": [
    "#DESCRIPTIVE STATISTIC\n",
    "sdanlegg_parameters = [#'swc_r', 'swc_f',\n",
    "                       'temp_v',  'temp_s',\n",
    "                       #'temp_f',\n",
    "                       'vent_v'\n",
    "                       ]\n",
    "folder_path1 = directory_path + '/' + data_for1 + '/'+ school_name +  '/'\n",
    "desc_test = combined3[sdanlegg_parameters+ airthingsDevice+weather_data]\n",
    "desc_test.describe().round(2).to_csv(directory_path + '/' + data_for1 + '/'+ survey_resample + '_'+school_name + '_'+'descriptive.txt', index=True)\n",
    "desc_test.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8o9tmVqRQqcJ"
   },
   "outputs": [],
   "source": [
    "combined2['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XzwPPJvoQqcJ"
   },
   "outputs": [],
   "source": [
    "print(answer_replace4)\n",
    "feel_confusion_matrix (combined2, 'feel_air', 'feel_temp', answer_replace4) #answer_replace4 #no_replace2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mw3ypeJaQqcK"
   },
   "outputs": [],
   "source": [
    "combined2['room_id'].unique()    #[combined2['room_id']!=class4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q33txP5CQqcK"
   },
   "outputs": [],
   "source": [
    "test_feel   = 'feel_air'\n",
    "test_column = 'co2_a'\n",
    "test_label  = co2_a_label\n",
    "\n",
    "come_in = combined2.copy().dropna( subset=[test_feel]+airthingsDevice, axis=0 ,how= 'all')\n",
    "stat_hist_all(come_in, test_feel,test_column,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNPoetkfxyLU"
   },
   "outputs": [],
   "source": [
    "combined2['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3TC5tsvxyLU"
   },
   "outputs": [],
   "source": [
    "come_in[['co2_a', 'feel_air']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50t0YXtnxyLU"
   },
   "outputs": [],
   "source": [
    "combined2[combined2['school_id']=='øyra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOD7PNoEQqcK"
   },
   "outputs": [],
   "source": [
    "test_feel   = 'feel_noise'\n",
    "test_column = 'sound_a'\n",
    "test_label  = bright_a_label\n",
    "\n",
    "come_in = combined2[combined2['school_id']=='øyra'].copy().dropna( subset=[test_feel]+[test_column], axis=0 ,how= 'any')\n",
    "stat_trial_all(come_in, test_feel,test_column,test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elPjxDubQqcK"
   },
   "outputs": [],
   "source": [
    "input_data = combined2[combined2['school_id']=='brannfjell'] #[combined2['room_id']!=class4].copy()\n",
    "# stat_test (input_data, is_col_main, 'feel_temp', 'enth_a', enth_a_label)\n",
    "stat_test (input_data, is_col_main, 'feel_temp', 'temp_v', temp_v_label)\n",
    "stat_test (input_data, is_col_main, 'feel_temp', 'rh_a', rh_a_label)\n",
    "stat_test (input_data, is_col_main, 'feel_temp', 'enth_a', enth_a_label)\n",
    "stat_test (input_data, is_col_main, 'feel_temp', 'hura_a', hura_a_label)\n",
    "stat_test (input_data, is_col_main, 'feel_air', 'co2_a', co2_a_label)\n",
    "stat_test (input_data, is_col_main, 'feel_air', 'voc_a', voc_a_label)\n",
    "stat_test (input_data, is_col_main, 'feel_air', 'rh_a', rh_a_label)\n",
    "stat_test (input_data, is_col_main, 'feel_air', 'rdn_a', rh_a_label)\n",
    "# stat_test (input_data, is_col_main, 'feel_health', 'co2_a', co2_a_label)\n",
    "# stat_test (input_data, is_col_main, 'feel_health', 'voc_a', voc_a_label)\n",
    "# stat_test (input_data, is_col_main, 'feel_health', 'rh_a', rh_a_label)\n",
    "# stat_test (input_data, is_col_main, 'feel_bright', 'bright_a', bright_a_label)\n",
    "# stat_test (input_data, is_col_main, 'feel_bright', 'rh_o', rh_o_label)\n",
    "# stat_test (input_data, is_col_main, 'feel_bright', 'temp_o', temp_o_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pyKmZJmmxyLU",
    "outputId": "ece84630-a3eb-4889-f1eb-4bfdd8c141b5"
   },
   "outputs": [],
   "source": [
    "combined2[combined2['feel_air']<0]#.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psaHydcTQqcK"
   },
   "outputs": [],
   "source": [
    "input_datax = combined2[(combined2['school_id']=='brannfjell') & (combined2['batch']=='skip')& (combined2['feel_air']<=-1)].copy()\n",
    "stat_test_subquestion(input_datax[input_datax['air_dry']!=np.nan], is_col_air,'air_dry', 'rh_a',rh_a_label )\n",
    "stat_test_subquestion(input_datax[input_datax['air_dry']!=np.nan], is_col_air,'air_dry', 'hura_a',rh_a_label )\n",
    "stat_test_subquestion(input_datax[input_datax['air_heavy']!=np.nan], is_col_air,'air_heavy', 'rh_a',rh_a_label )\n",
    "stat_test_subquestion(input_datax[input_datax['air_heavy']!=np.nan], is_col_air,'air_heavy', 'co2_a',co2_a_label )\n",
    "stat_test_subquestion(input_datax[input_datax['air_smell']!=np.nan], is_col_air,'air_smell', 'voc_a',voc_a_label )\n",
    "stat_test_subquestion(input_datax[input_datax['air_smell']!=np.nan], is_col_air,'air_smell', 'rh_a',rh_a_label )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 516,
     "status": "ok",
     "timestamp": 1726742048565,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "Fj_i9RGoQqcK",
    "outputId": "87b09261-1746-4619-b420-dcd4ae7ad85f"
   },
   "outputs": [],
   "source": [
    "ieq_parameters = [# 'temp_v' ,#'temp_f' ,'temp_s',\n",
    "                   #'vent_v',\n",
    "                  'co2_a', 'temp_a' , 'rh_a'  #,'sound_a'\n",
    "                  , 'enth_a', 'hura_a','voc_a','bright_a', 'press_a' ,'rdn_a',\n",
    "                  'temp_o', 'rh_o','winds_o', 'rain_o','enth_o','hura_o',  'press_o', 'sun_o'\n",
    "                #   ,'pm25_a','pm1_a'\n",
    "                    ,'pm2.5_o','pm10_o'\n",
    "                  ,'hour','hura_d'#,'enth_d'\n",
    "\n",
    "                  ]\n",
    "\n",
    "specific_questions = [\n",
    "    'feel_temp'\n",
    "    ,'feel_air'\n",
    "    ,'feel_bright'\n",
    "    ,'feel_health'\n",
    "    ,'feel_noise'\n",
    "]\n",
    "#[combined2['hour']<=10]   [combined2['room_id']!=class4][combined2['hour']<10]\n",
    "input_data2 = combined2[combined2['school_id']=='brannfjell'].copy().dropna( subset=airthingsPlus , axis=0 , how= 'any' ) #[combined2['room_id']== class4]\n",
    "input_data2.describe()\n",
    "print(input_data2[ieq_parameters + specific_questions].isna().sum())\n",
    "print(input_data2[ieq_parameters + specific_questions].count().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FALDnIwuxyLU"
   },
   "outputs": [],
   "source": [
    "character_columns = ['hour', 'hura_d', 'season']\n",
    "airthingsDevice = airthingsPlus\n",
    "weather_data = weather_oslo\n",
    "ieq_parameters = airthingsDevice + weather_data + character_columns\n",
    "ieq_parameters.remove('ppd')\n",
    "ieq_parameters.remove('pmv')\n",
    "ieq_parameters.remove('rdn_a')\n",
    "ieq_parameters.remove('pm10_o')\n",
    "ieq_parameters.remove('pm2.5_o')\n",
    "# ieq_parameters.remove('pm1_a')\n",
    "# ieq_parameters.remove('pm25_a')\n",
    "# ieq_parameters.remove('sound_a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1726743035427,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "V6k64RYqxyLU"
   },
   "outputs": [],
   "source": [
    "specific_questions_main = is_col_main # ['feel_air', 'feel_temp', 'feel_bright', 'feel_noise']        # is_col_main.copy().remove('feel_health')\n",
    "specific_questions_sub = is_col_subquestion\n",
    "filter_data = combined2[combined2['school_id']=='brannfjell'][combined2['total_feel']>4]#[combined2['season']==2]#[combined2['total_feel']>4]\n",
    "#[combined2['hour']<=10]   [combined2['room_id']!=class4][combined2['hour']<10]\n",
    "input_data_main = filter_data[specific_questions_main+ieq_parameters].dropna(subset='temp_a') #[combined2['room_id']!= class4]\n",
    "# input_data_sub_temp = filter_data[filter_data['feel_temp']<=0][is_col_temp+ieq_parameters].dropna(subset=airthingsDevice) #[combined2['room_id']!= class4]\n",
    "# input_data_sub_air = filter_data[filter_data['feel_air']<=0][is_col_air+ieq_parameters].dropna(subset=airthingsDevice)\n",
    "# input_data_sub_bright = filter_data[filter_data['feel_bright']<=0][is_col_temp+ieq_parameters].dropna(subset=airthingsDevice)\n",
    "# input_data_sub_health = filter_data[filter_data['feel_health']<=0][is_col_health+ieq_parameters].dropna(subset=airthingsDevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Frs8hKIrxyLU"
   },
   "outputs": [],
   "source": [
    "combined2[combined2['school_id']=='brannfjell']['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uM_KXU_mxyLV"
   },
   "outputs": [],
   "source": [
    "combined2[combined2['school_id']=='brannfjell'][is_col_main].describe(percentiles=[0.1,0.25,0.5,0.75,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5574,
     "status": "ok",
     "timestamp": 1726743043118,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "P_QBuAYHQqcK",
    "outputId": "3759758c-69a5-4f5c-c090-8a26d15536ae"
   },
   "outputs": [],
   "source": [
    "print(input_data_main.count().mean())\n",
    "heatmap_pvalue_2cat(input_data_main, specific_questions_main, ieq_parameters, (4,6))\n",
    "heatmap_pvalue_4cat(input_data_main, specific_questions_main, ieq_parameters, (4,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJRlbNjdQqcK"
   },
   "outputs": [],
   "source": [
    "replace_yes_no = {0:'No', 1:'Yes'}\n",
    "heatmap_chitest_2cat(input_data_sub[specific_questions_sub], replace_yes_no, (4,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_wbEUjUQqcK"
   },
   "outputs": [],
   "source": [
    "# specific_questions =  is_col_air#['air_dust']#  #+ is_col_bright #+ is_col_temp + ['health_head'] # [combined2['room_id']!=class4]\n",
    "# input_data2 = combined2.copy().dropna( subset=airthingsPlus+specific_questions, axis=0 ,how= 'any') #[combined2['room_id']== class4]\n",
    "heatmap_pvalue_subquest(input_data_sub[specific_questions_sub+ieq_parameters], specific_questions_sub, ieq_parameters, (13,6))\n",
    "# print(input_data2[specific_questions+ieq_parameters].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZymtRvW-QqcK"
   },
   "outputs": [],
   "source": [
    "def plot_feedback_histograph (data, class_number):\n",
    "    answer_replace4 = {0:'dont know',2:'Best (2)' , 1:'Good (1)', -1:'bad (-1)', -2:'worst (-2)'}\n",
    "    dataframe = data.copy()\n",
    "    dataframe.loc[:,is_col_main] = dataframe.loc[:,is_col_main].replace(answer_replace4)\n",
    "\n",
    "# Count occurrences of each value (1, 2, 3, 4) in each column\n",
    "    value_counts = dataframe[dataframe['room_id']==class_number][is_col_main].apply(pd.Series.value_counts)\n",
    "\n",
    "# Plotting the stacked bar chart\n",
    "    value_counts.plot(kind='bar', stacked=True, figsize=(7, 4))\n",
    "\n",
    "# Customize the plot\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Stacked Column Plot of Value Counts')\n",
    "    plt.legend(title='Columns')\n",
    "    print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOkytaqWQqcK"
   },
   "outputs": [],
   "source": [
    " plot_feedback_histograph (combined2, class2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oT_OEY7uIogp"
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1726741515989,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "hd8esSqxQqcL"
   },
   "outputs": [],
   "source": [
    "answer_replace4 = {0:'dont know',2:'Best (2)' , 1:'Good (1)', -1:'bad (-1)', -2:'worst (-2)'}\n",
    "answer_replace2 = {0:'dont know',2:'Satisfied' , 1:'Satisfied', -1:'Dissatisf.', -2:'Dissatisf.'}\n",
    "hue_4 = ['Best (2)' , 'Good (1)', 'bad (-1)', 'worst (-2)']\n",
    "hue_2 = ['Satisfied','Dissatisf.']\n",
    "\n",
    "def combined_plot (data_measured, data_questionnaire, class_number, col_ieq_measured, col_ieq_measured_unit , col_ieq_perception, hue_ordered_list, answer_replace):\n",
    "# Create figure and primary axis\n",
    "    fig, ax1 = plt.subplots(figsize=(9, 5))\n",
    "    data_questionnaire = data_questionnaire.replace(answer_replace)\n",
    "    # Plot df1 as a boxplot on ax1\n",
    "    sns.histplot(data=data_questionnaire, multiple='fill', hue=col_ieq_perception , hue_order= hue_ordered_list ,x='date',kde=True, stat='percent', palette=\"light:#5A9\", element= 'poly')   #\n",
    "    plt.xticks( horizontalalignment='center',rotation=45)\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Plot df2 as a histplot on ax2\n",
    "    sns.boxenplot(x=data_measured.index, y=col_ieq_measured,  data=data_measured , color= 'grey', width=0.25  , fill=True)\n",
    "\n",
    "    plt.xticks( horizontalalignment='center',rotation=45)\n",
    "    ax2.set_ylabel(col_ieq_measured + ' ' +col_ieq_measured_unit)\n",
    "\n",
    "    # Set common x-axis label\n",
    "    plt.xlabel('date')\n",
    "\n",
    "    plt.xticks(  )\n",
    "    plt.yticks()\n",
    "    plt.title(col_ieq_perception + ' (%) & ' + col_ieq_measured + '| at room: ' + str(class_number))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CqlqsSFQqcL"
   },
   "outputs": [],
   "source": [
    "measurement_plot = 'voc_a'\n",
    "feeling_plot     = 'feel_air'\n",
    "classroom_check  = class1\n",
    "special_date =combined2[combined2[room_col_name]==str(classroom_check)]['date'].unique()\n",
    "# ['2023-03-15', '2023-03-16', '2023-03-17', '2023-03-20',\n",
    "#                 '2023-03-21', '2023-03-22', '2023-03-23', '2023-03-24',\n",
    "#                 '2023-03-27', '2023-03-28', '2023-03-29', '2023-03-30',\n",
    "#                 '2023-03-31', '2023-04-11', '2023-04-12', '2023-04-13',\n",
    "#                 '2023-04-14']\n",
    "measured_data  = combined1[airthingsDevice+['date']][combined1[room_col_name]==str(classroom_check)].set_index('date').loc[special_date] #[combined1[room_col_name]==class1]\n",
    "perceived_data = combined2[is_col_main+['date']][combined2[room_col_name]==str(classroom_check)].set_index('date').loc[special_date] # [combined2[room_col_name]==class1]\n",
    "\n",
    "combined_plot (measured_data, perceived_data, classroom_check, measurement_plot, '(ppb)', feeling_plot,hue_2, answer_replace2)   #['satisfied','Dissatisf.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bi4YcasFxyLV"
   },
   "outputs": [],
   "source": [
    "measurement_plot = 'rh_a'\n",
    "feeling_plot = 'feel_temp'\n",
    "classroom_check = class3\n",
    "special_date = combined2[combined2[room_col_name]==str(classroom_check)]['date'].unique()\n",
    "# special_date = ['2023-03-22', '2023-03-24', '2023-03-28',  '2023-04-13', '2023-04-14', '2023-04-17', '2023-04-21']\n",
    "# '2023-03-22', '2023-03-24', '2023-03-28',  '2023-04-13', '2023-04-14', '2023-04-17', '2023-04-21'\n",
    "# ['2023-03-15', '2023-03-16', '2023-03-17', '2023-03-20',\n",
    "#                 '2023-03-21', '2023-03-22', '2023-03-23', '2023-03-24',\n",
    "#                 '2023-03-27', '2023-03-28', '2023-03-29', '2023-03-30',\n",
    "#                 '2023-03-31', '2023-04-11', '2023-04-12', '2023-04-13',\n",
    "#                 '2023-04-14']\n",
    "measured_data = combined1[[measurement_plot]+['date']][combined1[room_col_name]==str(classroom_check)].set_index('date').loc[special_date] #[combined1[room_col_name]==class1]\n",
    "perceived_data = combined2[[feeling_plot]+['date']][combined2[room_col_name]==str(classroom_check)].set_index('date').loc[special_date] # [combined2[room_col_name]==class1]\n",
    "\n",
    "combined_plot (measured_data, perceived_data, classroom_check, measurement_plot, '(%)', feeling_plot,hue_2, answer_replace2)   #['satisfied','Dissatisf.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtHhmaP2QqcL"
   },
   "outputs": [],
   "source": [
    "combined2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3C6q0fJXWLf"
   },
   "source": [
    "## Preparation & Pre-processsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OD8UYaH1QqcL"
   },
   "outputs": [],
   "source": [
    "timestep_datavisual = '10min'\n",
    "\n",
    "directory_path = platform_directory + '/' + folder_project + '/' + folder_ready + '/' + folder_year+ '/'  + school_name + '/' + folder_export\n",
    "folder_path3 = directory_path + '/' + data_for1 + '/' # replace with the folder path\n",
    "csv_files1 = [f for f in os.listdir(folder_path3) if f.endswith('.csv') and f.startswith( timestep_datavisual + '_1_')] # get list of csv files\n",
    "dfs1 = [] # list to store dataframes\n",
    "\n",
    "for file in csv_files1:\n",
    "    file_path = os.path.join(folder_path3, file) # get full path of csv file\n",
    "    df = pd.read_csv(file_path, delimiter=',') # read csv file into a dataframe\n",
    "    dfs1.append(df) # append dataframe to list\n",
    "combined3 = pd.concat(dfs1, ignore_index=True)\n",
    "\n",
    "combined3 = combined3[combined3['date'].between(start_date,end_date)]\n",
    "combined3['hura_d'] = combined3['hura_a'] - combined3['hura_o']\n",
    "combined3['enth_d'] = combined3['enth_a'] - combined3['enth_o']\n",
    "\n",
    "combined4 = combined3.copy()\n",
    "\n",
    "time_interval = 10\n",
    "hour_interval = time_interval/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdn5QXW1QqcL"
   },
   "outputs": [],
   "source": [
    "combined3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GF7_3Ea2xyLV"
   },
   "outputs": [],
   "source": [
    "combined3['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBBOxTuEQqcL"
   },
   "outputs": [],
   "source": [
    "morning   = 'morning'\n",
    "afternoon = 'afternoon'\n",
    "#morning   = 'kl.07-10'\n",
    "#afternoon = 'kl.10 - end'\n",
    "combined3 = combined3[combined3['time']<'14:00']\n",
    "combined3['time2'] = combined3[['hour']].applymap(lambda x: morning if x <= 9 else afternoon)\n",
    "combined3['time2'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iJlZY8LTF2p"
   },
   "source": [
    "## Data - IAQ Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThSiZsimZoyv"
   },
   "outputs": [],
   "source": [
    "def histo_graph(dataframe,column,column_label):\n",
    "# Plotting the histogram\n",
    "  min =  (dataframe[column].min()*0.9).astype(int)\n",
    "  max =  (dataframe[column].max()*1.1).astype(int)\n",
    "  #plt.hist(dataframe[column], edgecolor='black', alpha=0.7)\n",
    "  plt.hist(dataframe[column], bins=(np.arange(min,max, step=(max-min)/20)), edgecolor='black', alpha=0.7)\n",
    "  plt.xlabel(column_label)\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.title(column +' Histogram: All Rooms', size=10)\n",
    "  #plt.xticks(range(18, 28))  # Set x-ticks to match the temperature range\n",
    "  #plt.xticks(np.arange(min,max, step=(max-min)/12))\n",
    "  plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "  plt.show()\n",
    "\n",
    "def histo_graph_class(dataframe,column,column_label,class_number):\n",
    "# Plotting the histogram\n",
    "  min =  (dataframe[column].min()*0.9).astype(int)\n",
    "  max =  (dataframe[column].max()*1.1).astype(int)\n",
    "  #plt.hist(dataframe[column], edgecolor='black', alpha=0.7)\n",
    "  plt.figure(figsize=(3, 2))\n",
    "  plt.hist(dataframe[column], bins=(np.arange(min,max, step=(max-min)/20)), edgecolor='black', alpha=0.7)\n",
    "  plt.xlabel(column_label)\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.title(column +' Histogram: Classroom ' + str(class_number), size=11)\n",
    "  #plt.xticks(range(18, 28))  # Set x-ticks to match the temperature range\n",
    "  #plt.xticks(np.arange(min,max, step=(max-min)/12))\n",
    "  plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "  plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lua6MEi-liUt"
   },
   "outputs": [],
   "source": [
    "#ieq_describe = ieq_parameters.copy()\n",
    "ieq_describe = [ #'temp_v',# 'temp_f' ,\n",
    "                # 'temp_s',#'co2_v',\n",
    "                 #'vent_v',\n",
    "                   'temp_a'  #, 'sound_a'\n",
    "                   #,'bright_a'\n",
    "                  ,'co2_a','rh_a', 'voc_a' , 'rdn_a'\n",
    "                  , 'enth_a', 'hura_a',\n",
    "                  'temp_o', 'rh_o','enth_o', 'hura_o', 'winds_o', 'rain_o'\n",
    "                  #,'pm25_a','pm1_a'\n",
    "                  #,'pm2.5_o','pm10_o'\n",
    "                  ,'hura_d'#,'enth_d'\n",
    "                  ]\n",
    "print(ieq_describe)\n",
    "combined1[combined1['room_id']!=class4][ieq_describe].describe().round(2)\n",
    "#combined3[['temp_v','rh_a','co2_a','voc_a']].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uymA4YO3adNX"
   },
   "outputs": [],
   "source": [
    "class_number = class1\n",
    "dataset_plot = combined1[(combined1['room_id']==class_number) & (combined1['hour']>=start_hour) & (combined1['hour']<=end_hour) ]\n",
    "\n",
    "#histo_graph(dataset_plot[(dataset_plot['hour']>=start_hour) & (dataset_plot['hour'] <= end_hour) ], 'temp_v', temp_v_label) #/hour_interval\n",
    "#histo_graph(dataset_plot[(dataset_plot['hour']>=start_hour) & (dataset_plot['hour'] <= end_hour) ], 'temp_f', temp_f_label)\n",
    "# histo_graph_class(dataset_plot, 'vent_v', vent_v_label, class_number)\n",
    "# histo_graph_class(dataset_plot, 'pm1_a', pm1_a_label, class_number)\n",
    "# histo_graph_class(dataset_plot, 'pm25_a', pm25_a_label, class_number)\n",
    "histo_graph_class(dataset_plot, 'co2_a', co2_a_label, class_number)\n",
    "histo_graph_class(dataset_plot, 'voc_a', voc_a_label, class_number)\n",
    "histo_graph_class(dataset_plot, 'enth_a', enth_a_label, class_number)\n",
    "histo_graph_class(dataset_plot, 'rh_a', rh_a_label, class_number)\n",
    "histo_graph_class(dataset_plot, 'temp_a', temp_a_label, class_number)\n",
    "histo_graph_class(dataset_plot, 'temp_v', temp_v_label, class_number)\n",
    "histo_graph_class(dataset_plot, 'hura_a', hura_a_label, class_number)\n",
    "histo_graph_class(dataset_plot, 'sound_a', sound_a_label, class_number)\n",
    "\n",
    "#histo_graph(dataset_plot[(dataset_plot['hour']>=start_hour) & (dataset_plot['hour'] <= end_hour) ], 'sound_a', sound_a_label)\n",
    "histo_graph(dataset_plot[(dataset_plot['hour']>=start_hour) & (dataset_plot['hour'] <= end_hour) ], 'bright_a', bright_a_label)\n",
    "histo_graph(dataset_plot[(dataset_plot['hour']>=start_hour) & (dataset_plot['hour'] <= end_hour) ], 'rh_o',rh_o_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rL8bbRbvLoC"
   },
   "source": [
    "### Pie-Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aUterCAwtIG"
   },
   "outputs": [],
   "source": [
    "resuming_df= combined3.groupby(['time2'])[response_counting].sum()\n",
    "\n",
    "# Custom colors for the pie chart segments\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.pie(resuming_df, labels=resuming_df.index, autopct='%1.1f%%', startangle=140, colors= custom_colors, )\n",
    "plt.title(school_name + '_'+response_counting)\n",
    "plt.legend( )\n",
    "print('total cleaned '+ str(response_counting)+': '  + str(resuming_df.sum().round(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5abBlARQqcM"
   },
   "outputs": [],
   "source": [
    "# pie_plot_feel (combined2, 'feel_air' )\n",
    "# pie_plot_feel (combined2, 'feel_bright' )\n",
    "pie_plot_feel_class (combined2, 'feel_temp', class2   )\n",
    "pie_plot_feel_class (combined2, 'feel_air',class2  )\n",
    "pie_plot_feel_class (combined2, 'feel_health', class2  )\n",
    "pie_plot_feel_class (combined2, 'feel_bright', class3 )\n",
    "pie_plot_feel_class (combined2, 'feel_noise', class2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PahR2TzbvS-I"
   },
   "source": [
    "### Boxplot & Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1U9uE9INstLL"
   },
   "outputs": [],
   "source": [
    "what_measure = \"TEMP : airthings and schneider \"\n",
    "print(what_measure + 'CV : '+str((explained_variance_score(combined3['temp_a'],combined3['temp_v']))))\n",
    "print(what_measure + 'R2 : '   +str(r2_score(combined3['temp_a'],combined3['temp_v'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2x4tgy1stKK3"
   },
   "outputs": [],
   "source": [
    "what_measure = \"CO2 : airthings and schneider \"\n",
    "print(what_measure + 'CV : '+str((explained_variance_score(combined3['co2_a'],combined3['co2_v']))))\n",
    "print(what_measure + 'R2 : '   +str(r2_score(combined3['co2_a'],combined3['co2_v'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4adqxaRuIV6g"
   },
   "outputs": [],
   "source": [
    "combined3[airthingsPlus+weather_oslo].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbszepmYTLMS"
   },
   "outputs": [],
   "source": [
    "what_co2 = 'co2_a'\n",
    "what_temp = 'temp_a'\n",
    "co2_limit1 = 1000\n",
    "\n",
    "temp_limit2 = 20.5\n",
    "temp_limit3 = 21.5\n",
    "setpoint = combined3['temp_s']\n",
    "temp_limit2 = setpoint- 1\n",
    "temp_limit3 = setpoint+ 1\n",
    "\n",
    "voc_limit1 = 220\n",
    "sound_limit1 = 50\n",
    "\n",
    "rh_limit1 = 30\n",
    "rh_limit2 = 60\n",
    "\n",
    "pm1_limit = 8\n",
    "pm25_limit = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AElIhPxJxyLW"
   },
   "outputs": [],
   "source": [
    "combined3['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJf4o6e5xyLW"
   },
   "outputs": [],
   "source": [
    "dataset_come['room_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWR-jQl8sJWx"
   },
   "outputs": [],
   "source": [
    "dataset_come = combined3#[combined3['room_id']!=class4]\n",
    "resume = pd.DataFrame(dataset_come['room_id'])\n",
    "a = ((dataset_come[what_co2] <= co2_limit1)).astype(int)\n",
    "b = ((dataset_come[what_co2] > co2_limit1)).astype(int)\n",
    "resume['co2_<'] = (a/ (a+b))\n",
    "resume['co2_>'] = (b/ (a+b))\n",
    "\n",
    "a = ((dataset_come[what_temp] <= temp_limit2)).astype(int)\n",
    "b = ((dataset_come[what_temp] >= temp_limit2) & (dataset_come[what_temp] < temp_limit3)).astype(int)\n",
    "c = ((dataset_come[what_temp] > temp_limit3)).astype(int)\n",
    "resume['temp_<'] = a / (a+b+c)\n",
    "resume['temp_='] = b / (a+b+c)\n",
    "resume['temp_>'] = c / (a+b+c)\n",
    "\n",
    "a = ((dataset_come['voc_a'] <= voc_limit1)).astype(int)\n",
    "b = ((dataset_come['voc_a'] > voc_limit1)).astype(int)\n",
    "resume['voc_<'] = a / (a+b)\n",
    "resume['voc_>'] = b / (a+b)\n",
    "\n",
    "a = ((dataset_come['rh_a'] <= rh_limit1)).astype(int)\n",
    "b = ((dataset_come['rh_a'] >= rh_limit1) & (dataset_come['rh_a'] < rh_limit2)).astype(int)\n",
    "c = ((dataset_come['rh_a'] > rh_limit2)).astype(int)\n",
    "resume['rh_<'] = a / (a+b+c)\n",
    "resume['rh_='] = b / (a+b+c)\n",
    "resume['rh_>'] = c / (a+b+c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WH3nHo2apQNM"
   },
   "outputs": [],
   "source": [
    "a = ((dataset_come['pm1_a'] <= pm1_limit)).astype(int)\n",
    "b = ((dataset_come['pm1_a'] > pm1_limit)).astype(int)\n",
    "resume['pm1_<'] = a / (a+b)\n",
    "resume['pm1_>'] = b / (a+b)\n",
    "a = ((dataset_come['pm25_a'] <= pm25_limit)).astype(int)\n",
    "b = ((dataset_come['pm25_a'] > pm25_limit)).astype(int)\n",
    "resume['pm2.5_<'] = a / (a+b)\n",
    "resume['pm2.5_>'] = b / (a+b)\n",
    "\n",
    "a = ((dataset_come['sound_a'] <= sound_limit1)).astype(int)\n",
    "b = ((dataset_come['sound_a'] > sound_limit1)).astype(int)\n",
    "resume['sound_<'] = a / (a+b)\n",
    "resume['sound_>'] = b / (a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6a9_lhQsqu7M"
   },
   "outputs": [],
   "source": [
    "resume['date'] = dataset_come['date']\n",
    "resume['time'] = dataset_come['time']\n",
    "resume[room_col_name] = dataset_come[room_col_name]\n",
    "\n",
    "resume['datetime'] = pd.to_datetime((dataset_come['date'] + ' ' + dataset_come['time']), format='%Y-%m-%d %H:%M:%S')\n",
    "resume.set_index('datetime', inplace=True)\n",
    "resume .head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQdAtQpTpYcN"
   },
   "outputs": [],
   "source": [
    "# room1 = resume[resume[room_col_name] == np.array(class1).astype(str)].iloc[:,1:-2]  #*hour_interval\n",
    "room2 = resume[resume[room_col_name] == np.array(class2).astype(int)].iloc[:,1:-2]  #*hour_interval\n",
    "room3 = resume[resume[room_col_name] == np.array(class3).astype(int)].iloc[:,1:-2]  #*hour_interval\n",
    "# room4 = resume[resume[room_col_name] == np.array(class4).astype(str)].iloc[:,1:-2]  #*hour_interval\n",
    "result_sum = pd.DataFrame(np.array([#room1.sum()\n",
    "                                    room2.sum()\n",
    "                                    ,room3.sum()\n",
    "                                    # ,room4.sum()\n",
    "                                     ]),columns=room2.columns, index=[#class1,\n",
    "                                                                      class2,class3\n",
    "                                                                      #,class4\n",
    "                                                                     ])   *hour_interval\n",
    "room1.iloc[:,1:8].sum()\n",
    "result_sum\n",
    "#result is as hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csx1S5VVKwJ7"
   },
   "outputs": [],
   "source": [
    "combined4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7slEa1aJB-8a"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "x_col = 'temp_o'\n",
    "y_col = 'rd_temp'\n",
    "z_col = 'temp_a'\n",
    "\n",
    "# For each set of style and range settings, plot n random points in the box\n",
    "xs = combined3[x_col]\n",
    "ys = combined3[y_col]\n",
    "zs = combined3[z_col]\n",
    "ax.scatter(xs, ys, zs )\n",
    "\n",
    "ax.set_xlabel(x_col)\n",
    "ax.set_ylabel(y_col)\n",
    "ax.set_zlabel(z_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OYq55XvrOuW"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "x_col = 'co2_a'\n",
    "y_col = 'pm25_a'\n",
    "z_col = 'feel_air'\n",
    "\n",
    "# For each set of style and range settings, plot n random points in the box\n",
    "xs = combined3[x_col]\n",
    "ys = combined3[y_col]\n",
    "zs = combined3[z_col]\n",
    "ax.scatter(xs, ys, zs)\n",
    "\n",
    "ax.set_xlabel(x_col)\n",
    "ax.set_ylabel(y_col)\n",
    "ax.set_zlabel(z_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRASIt9rrYOS"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "x_col = 'pm25_o'\n",
    "y_col = 'pm25_a'\n",
    "z_col = 'feel_health'\n",
    "\n",
    "# For each set of style and range settings, plot n random points in the box\n",
    "xs = combined4[x_col]\n",
    "ys = combined4[y_col]\n",
    "zs = combined4[z_col]\n",
    "ax.scatter(xs, ys, zs)\n",
    "\n",
    "ax.set_xlabel(x_col)\n",
    "ax.set_ylabel(y_col)\n",
    "ax.set_zlabel(z_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZcSHke_qAqh"
   },
   "source": [
    "## Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFR_4z52qAqh"
   },
   "outputs": [],
   "source": [
    "def threshold_calculator_linear (df, column_check, new_column, threshold_number, duration):\n",
    "    df[new_column] = np.where(df[column_check] > threshold_number, duration, 0)\n",
    "def threshold_calculator_parabolic (df, column_check, new_column, lower_limit, higher_limit, duration):\n",
    "    df[new_column] = np.where((df[column_check] > higher_limit) | (df[column_check] < lower_limit), duration, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZx914dxqAqh"
   },
   "outputs": [],
   "source": [
    "timestep = 5/60\n",
    "# threshold_calculator_linear (measured_resampled, 'S42_pm2.5', 'pm2.5_>', 1,timestep)\n",
    "# threshold_calculator_parabolic (measured_resampled, 'S25_temp', 'temp_>', 21,23,timestep)\n",
    "threshold_calculator_linear (combined1, 'voc_a', 'limit_voc', 220,timestep)\n",
    "threshold_calculator_linear (combined1, 'co2_a', 'limit_co2', 1000,timestep)\n",
    "threshold_calculator_parabolic (combined1, 'temp_a', 'limit_temp', 21,23,timestep)\n",
    "threshold_calculator_parabolic (combined1, 'rh_a', 'limit_rh', 30,60,timestep)\n",
    "threshold_calculator_parabolic (combined1, 'enth_a', 'limit_enth', 32.85,55.45,timestep)\n",
    "threshold_calculator_linear (combined1, 'bright_a', 'limit_bright', 50,timestep)\n",
    "threshold_calculator_linear (combined1, 'hura_a', 'limit_hura', 5,timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ch4nR1BqAqh"
   },
   "outputs": [],
   "source": [
    "#except_date3 = ['2023-11-19', '2023-11-25', '2023-11-26']\n",
    "combined1['datetime'] = pd.to_datetime(combined1['datetime'] )\n",
    "input_data = combined1.copy() [combined1['room_id']!=class4]\n",
    "input_data['fd_air']    = (input_data['feel_air4'] + input_data['feel_air3'])/input_data['students']\n",
    "input_data['fd_temp']   = (input_data['feel_temp4'] + input_data['feel_temp3'])/input_data['students']\n",
    "input_data['fd_health']    = (input_data['feel_health4'] + input_data['feel_health3'])/input_data['students']\n",
    "input_data['fd_bright']   = (input_data['feel_bright4'] + input_data['feel_bright3'])/input_data['students']\n",
    "input_data['fd_noise']    = (input_data['feel_noise4'] + input_data['feel_noise3'])/input_data['students']\n",
    "\n",
    "desired_perception = ['fd_temp', 'fd_air','fd_health', 'fd_bright', 'fd_noise']   #'fd_health', 'fd_bright', 'fd_noise'\n",
    "desired_threshold = ['limit_voc', 'limit_co2', 'limit_temp', 'limit_enth','limit_rh']\n",
    "threshold_data = pd.pivot_table(input_data, values=desired_threshold+desired_perception, index=['date', 'room_id'], aggfunc='sum').round(2)\n",
    "threshold_data = threshold_data.loc[threshold_data['fd_temp'] != 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqo5APkoqAqi"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(threshold_data, x= 'limit_voc', y='fd_air', hue = 'room_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZdgXbcrqAqi"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(threshold_data, x= 'limit_temp', y='fd_temp', hue = 'room_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dgI-7PBqAqi"
   },
   "outputs": [],
   "source": [
    "heatmap_corrvalue(threshold_data.loc[threshold_data['fd_temp'] != 0], 'pearson', 'threshold vs. dissatisfaction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46TPGLro0fIo"
   },
   "source": [
    "## Correlation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0P8rVZQyTO5"
   },
   "outputs": [],
   "source": [
    "#Heatmap Correlation\n",
    "#call the DataFrame\n",
    "#corr = combined.iloc[:,:20].corr().round(2)\n",
    "# corr = combined3[combined3['time']== afternoon].loc[:,is_col_main + pd_col + interpolate_col].drop([ 'temp_s','rain_o','feel_noise'], axis=1).corr()#.round(2)   #, 'feel_bright'\n",
    "corr = combined_pivot_mean[input_variables + ['rd_air', 'rd_temp']].corr()\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(20, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220,10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask,cmap=\"RdYlGn\", vmin=-1, vmax=1, center=0, cbar=True, annot =True,\n",
    "            square=True, linewidths=.5, annot_kws={\"size\": 10})     #cmap= 'coolwarm'\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "\n",
    "plt.title('I '+time_series_resample+'ourly Data : Correlation', fontsize=15)\n",
    "plt.yticks(fontsize=12,rotation=45 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mmb-bTwA9sJY"
   },
   "outputs": [],
   "source": [
    "#Heatmap Correlation\n",
    "#call the DataFrame\n",
    "#corr = combined.iloc[:,:20].corr().round(2)\n",
    "corr = combined1[weather_volda+airthingsPro].drop([ 'ppd', 'pmv'], axis=1).corr().round(2)   #, 'feel_bright'\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(20, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220,10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask,\n",
    "            cmap=\"RdYlGn\", vmin=-1, vmax=1, center=0, cbar=True, annot =True,\n",
    "            square=True, linewidths=.5, annot_kws={\"size\": 10})\n",
    "plt.xticks(rotation=90, fontsize=12)\n",
    "\n",
    "plt.title(#'I '+time_series_resample+ 'ourly\n",
    "          'Data : Correlation', fontsize=15)\n",
    "plt.yticks(fontsize=12,rotation=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSYBFTW8xyLY"
   },
   "outputs": [],
   "source": [
    "combined2[airthingsDevice+is_col_subquestion].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ledHFV7ZchW7"
   },
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1783,
     "status": "ok",
     "timestamp": 1726746116827,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "uPrfG6zec50K"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict\n",
    "from sklearn import metrics, tree\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.naive_bayes import CategoricalNB, GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, ExtraTreesClassifier,StackingRegressor, ExtraTreesRegressor, AdaBoostClassifier, AdaBoostRegressor , GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import xgboost as xg\n",
    "from xgboost.sklearn import XGBClassifier, XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.multioutput import MultiOutputRegressor, MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTENC, SMOTEN\n",
    "import joblib\n",
    "from joblib import load, dump\n",
    "import shap\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1726746117429,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "HmJu39UfbqOj"
   },
   "outputs": [],
   "source": [
    "def conf_matrix_ML(y_test,y_test_pred, model_ML, name_of_MLalgorithm ):\n",
    "    cm_rf = confusion_matrix(y_test,y_test_pred, labels = model_ML.classes_, )\n",
    "    disp  = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=model_ML.classes_)\n",
    "    plt.figure(figsize= (2,2))\n",
    "    disp.plot()\n",
    "    plt.title('Confussion Matrix : '+str(name_of_MLalgorithm))\n",
    "    plt.show()\n",
    "\n",
    "def ML_optimization_process (find_optimum, ML_optimum, x_train, y_train, x_test,y_test):\n",
    "  print(\"Optimum Value : {}\".format(find_optimum))\n",
    "  print(\"Accuracy score (training): {:.3f}\".format(ML_optimum.score(x_train, y_train)))\n",
    "  print(\"Accuracy score (validation): {:.3f}\".format(ML_optimum.score(x_test, y_test)))\n",
    "  print(\"__\")\n",
    "\n",
    "def ML_NN_optimization_process (find_optimum1, find_optimum2, ML_optimum, x_train, y_train, x_test,y_test):\n",
    "  print(\"Optimum Value : {}\".format([find_optimum1,find_optimum2]))\n",
    "  print(\"Accuracy score (training): {:.3f}\".format(ML_optimum.score(x_train, y_train)))\n",
    "  print(\"Accuracy score (validation): {:.3f}\".format(ML_optimum.score(x_test, y_test)))\n",
    "  print(\"__\")\n",
    "\n",
    "def ML_grid_result (grid_search, ML_optimum, x_train, y_train, x_test,y_test):\n",
    "  print(\"Best parameters: \", grid_search.best_params_)\n",
    "  print(\"Best score: \", grid_search.best_score_.round(3))\n",
    "  print(\"Accuracy score (training): {:.3f}\".format(ML_optimum.score(x_train, y_train)))\n",
    "  print(\"Accuracy score (validation): {:.3f}\".format(ML_optimum.score(x_test, y_test)))\n",
    "\n",
    "def pareto_histograph(df):\n",
    "\n",
    "    # Calculate cumulative sum for each column\n",
    "    df_cumsum = df.cumsum()\n",
    "\n",
    "    # Plot\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Plot bar histograms\n",
    "    df.plot(kind='bar', ax=ax1, color=['skyblue', 'salmon'], alpha=0.7, width=0.4, position=1)\n",
    "    ax1.set_ylabel('Value')\n",
    "\n",
    "    # Plot cumulative lines\n",
    "    df_cumsum.plot(kind='line', ax=ax2, marker='o', color=['blue', 'red'], linestyle='-', linewidth=2)\n",
    "\n",
    "    # Add legend\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    # Add labels and title\n",
    "    ax2.set_ylabel('Cumulative Sum')\n",
    "    plt.title('Bar Histograms and Cumulative Lines')\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYPTLMbFo4V9"
   },
   "source": [
    "## Data Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1726746118947,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "OhreJ3rAxyLY"
   },
   "outputs": [],
   "source": [
    "response_rate_min = 0.25\n",
    "school_train = 'brannfjell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1711374965608,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "lVt47aWppIOy",
    "outputId": "718d15c6-4cc2-469a-9c49-496f4bedc406"
   },
   "outputs": [],
   "source": [
    "# REGRESSION\n",
    "target_col   =    ['rd_air']\n",
    "focus_plot = 'co2_a'\n",
    "scene_what = 'C'\n",
    "ML_category = 'regression'\n",
    "school_train = 'brannfjell'\n",
    "batch_selector = 'afternoon'\n",
    "time_after = '06:00'\n",
    "input_variables = [  #'temp_v', #'temp_f' , 'temp_s','co2_v'\n",
    "                   #'vent_v',\n",
    "                        # 'temp_a',\n",
    "                        # 'bright_a' ,  #'sound_a',\n",
    "                        # 'rh_a',\n",
    "                        'co2_a',\n",
    "                         'voc_a' ,\n",
    "                        # 'rdn_a',\n",
    "                        # 'press_a',\n",
    "                        # 'enth_a',\n",
    "                       'hura_a',\n",
    "                      #  'temp_o',\n",
    "                      #  'enth_o',\n",
    "                      #  'press_o',\n",
    "                      #  'rh_o',\n",
    "                       'rain_o',\n",
    "                      #  'hura_o',# 'winds_o',\n",
    "                        #'floor_first',\n",
    "                        # 'hour',\n",
    "                      # 'pm25_a','pm1_a',\n",
    "                    # 'pm2.5_o',\n",
    "                    # 'sun_o',\n",
    "                    # 'pm10_o',\n",
    "                  'moment',\n",
    "                    'hura_d'\n",
    "\n",
    "                  ] #'bright_a', 'rdn_a', 'temp_v','rh_o', 'winds_o',\n",
    "# input_variables = ['feel_temp', 'feel_air', 'feel_bright', 'feel_noise']\n",
    "# input_variables = ['rd_temp', 'rd_air', 'rd_bright', 'rd_noise']\n",
    "\n",
    "\n",
    "rename_func = lambda col: col + '_ave' if col.endswith('_a') or col.endswith('_o') else col\n",
    "input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8hNWv3kxyLY"
   },
   "outputs": [],
   "source": [
    "# REGRESSION\n",
    "\n",
    "is_col_target = is_main_rd.copy()\n",
    "\n",
    "target_col   =  ['rd_temp']    #is_col_target\n",
    "focus_plot = 'co2_a'\n",
    "scene_what = 'C'\n",
    "batch_selector = 'afternoon'\n",
    "time_after = '06:00'\n",
    "input_variables = airthingsSmall + weather_volda + [ 'hura_d','moment'\n",
    "                                                   # ,'floor_first'\n",
    "                                                    ]\n",
    "input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1711380426999,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "y6C4LNfOqAqj",
    "outputId": "1aedde0a-2074-4c99-89fa-3c08fcd1c5b8"
   },
   "outputs": [],
   "source": [
    "# CLASSIFICATION\n",
    "is_col_target = is_col_subquestion.copy()\n",
    "is_col_target.remove('temp_coldhot')\n",
    "\n",
    "ML_category = 'classification'\n",
    "target_col   = is_col_target\n",
    "what_estimate = 'subsquestion'\n",
    "\n",
    "time_after = '06:00'\n",
    "input_variables = airthingsDevice + weather_data #+ [ 'hour']  #'floor_first','hura_d',\n",
    "# input_variables.remove('pmv')\n",
    "# input_variables.remove( 'ppd')\n",
    "# input_variables.remove( 'rdn_a')\n",
    "input_variables.remove( 'winds_o')\n",
    "input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSbGAQrZxyLY",
    "outputId": "747daa8b-a4ba-4ff5-9d70-4043b1fb2f18"
   },
   "outputs": [],
   "source": [
    "# CLASSIFICATION\n",
    "\n",
    "target_col   = 'feel_health'\n",
    "what_estimate = [target_col]\n",
    "\n",
    "ML_category = 'classification'\n",
    "time_after = '06:00'\n",
    "input_variables = airthingsPlus + weather_oslo #+ [ 'hour']  #'floor_first','hura_d',\n",
    "input_variables.remove('pmv')\n",
    "input_variables.remove( 'ppd')\n",
    "input_variables.remove( 'rdn_a')\n",
    "# input_variables = ieq_parameters\n",
    "# input_variables.remove( 'winds_o')\n",
    "input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07C2Eka8QqcN"
   },
   "outputs": [],
   "source": [
    "focus_plot = 'temp_a'\n",
    "time_after = '06:00'\n",
    "input_variables = ['feel_temp','feel_air', 'feel_noise', 'feel_bright'\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDJXiwZJxyLY",
    "outputId": "2be7a317-143b-47e8-c6f1-c239ce9553a9"
   },
   "outputs": [],
   "source": [
    "folder_ML = 'ML'\n",
    "directory_path = platform_directory + '/' + folder_project + '/' + folder_ready + '/'+ folder_year + '/' + school_name + '/' + folder_export\n",
    "folder_model = survey_resample + '_' + school_train + '_'+ folder_year + '_'\n",
    "ML_directory = directory_path + '/'+ folder_ML + '/' + ML_category + '/' + folder_model + \"\".join(map(str, target_col)) #+ '_'  #+scene_what\n",
    "\n",
    "todays_date = '_20240805'\n",
    "ML_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRCpauKgxyLY"
   },
   "source": [
    "## Data Splitting - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkBekhXLxyLY"
   },
   "outputs": [],
   "source": [
    "variable_scale = MinMaxScaler() #StandardScaler()\n",
    "variable_scale2 = MinMaxScaler()\n",
    "train_percent = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEV4HTafxyLY",
    "outputId": "1dcf7e1d-c115-4cfc-d5ed-364cceb8cc8b"
   },
   "outputs": [],
   "source": [
    "index_of_data_ML = ['date', 'school_id','room_id'  ,'batch','response_rate' ]\n",
    "# data_to_ML = combined_pivot_mean_percent.copy()\n",
    "data_to_ML = data_pivot_mean_percent.copy()\n",
    "data_ML= data_to_ML[\n",
    "   (data_to_ML['response_rate']>=response_rate_min) & (data_to_ML['response_rate']<=1.5)\n",
    "                                #  & (data_to_ML['batch']==batch_selector)\n",
    "                                    & (data_to_ML['school_id']=='brannfjell')\n",
    "                                    & (data_to_ML['batch']!='skip')\n",
    "                                  # & (data_to_ML['floor_first']==1)\n",
    "                                ][index_of_data_ML + input_variables +target_col].dropna()\n",
    "data_ML_R = data_ML.drop(index_of_data_ML  , axis=1)\n",
    "# data_ML_R = data_ML_R[data_ML_R['temp_coldhot']!=0]\n",
    "data_ML_R .isna().sum()#.value_counts()  #COUNT NAN VALUES  or NAN CHECKER\n",
    "print(data_ML['room_id'].count())\n",
    "print(data_ML['room_id'].isna().sum())\n",
    "print(data_ML['batch'].value_counts())\n",
    "print(data_ML_R.count().mean())\n",
    "# heatmap_corrvalue(data_ML_R, 'pearson', (8,6), 'Q75', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrriATYbxyLZ",
    "outputId": "2bcca218-8794-4212-9814-3dcd4a59a67c"
   },
   "outputs": [],
   "source": [
    "heatmap_corrvalue( data_ML_R, 'pearson', (12,10), 'brannfjell: pearson correlation', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK & ELIMINATE OUTLIER : HOTELLING T2\n",
    "from pca import pca\n",
    "\n",
    "# Initialize pca to also detected outliers.\n",
    "model = pca(normalize=True, detect_outliers=['ht2', 'spe'], n_std=2 )\n",
    "# Fit and transform\n",
    "results = model.fit_transform(data_ML_R) #.drop(target_col, axis=1)\n",
    "# Grab overlapping outliers\n",
    "hotellings_outlier = results['outliers']['y_bool']==True\n",
    "SPE_outlier = results['outliers']['y_bool_spe']==True\n",
    "all_outlier= np.logical_or(hotellings_outlier, SPE_outlier)\n",
    "print('SPE Outlier :', data_ML_R.loc[SPE_outlier,].count().mean())\n",
    "print('hotellings_outlier :', data_ML_R.loc[hotellings_outlier,].count().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0x-NZ1yxyLZ",
    "outputId": "1ddc9db0-98f0-4ba7-9067-388655633e5a"
   },
   "outputs": [],
   "source": [
    "data_input = data_ML_R.copy().drop( data_ML_R.loc[all_outlier,].index, axis=0)  .sort_values(target_col)#.reset_index(drop=True)   #.dropna(subset='temp_a', how='all').copy()#.dropna(subset=[target_col], axis = 0, inplace=True )\n",
    "# data_input = data_ML.copy() .sort_values(target_col)   .dropna(subset=['temp_a'], how='all')\n",
    "#xb = data_input.loc[:, ['swc_f', 'swc_r', 'temp_v', 'temp_f', 'co2_a', 'temp_a', 'rh_a','voc_a', 'bright_a', 'temp_o', 'rh_o', 'winds_o']] #Predictor\n",
    "xb  = data_input.drop(target_col, axis=1).copy() #.drop(columns=['temp_f', 'pm25_a', 'pm1_a'] ) #.loc[:, ['temp_v', 'co2_a', 'temp_a', 'rh_a','voc_a', 'bright_a', 'temp_o', 'rh_o', 'winds_o']] #Predictor\n",
    "x   = variable_scale.fit_transform(xb)  #xb.copy() #\n",
    "yb  = data_input.loc[:, target_col] #Label\n",
    "print(\"target \" + str(target_col))\n",
    "print(\"total input variables: \", xb.shape[1])\n",
    "print( \"isnan value check, \",  xb.isna().sum())\n",
    "# print(data_input.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linreg_DT_RF_XGBR (find_optimum  , x_train, x_test, y_train, y_test):\n",
    "    # Lin_reg = LinearRegression()\n",
    "    # Lin_reg.fit(x_train, y_train)\n",
    "    print(\"what random use \" + str(find_optimum))\n",
    "    # print(Lin_reg.coef_.flatten().round(4) , Lin_reg.intercept_.flatten().round(4))\n",
    "\n",
    "    DT_reg = DecisionTreeRegressor( random_state= find_optimum )\n",
    "    DT_reg.fit(x_train, y_train    )\n",
    "\n",
    "    RF_reg = RandomForestRegressor( random_state= find_optimum )\n",
    "    RF_reg.fit(x_train, y_train)\n",
    "\n",
    "    XG_reg = XGBRegressor( random_state= find_optimum )\n",
    "    XG_reg.fit(x_train, y_train)\n",
    "\n",
    "    # print(\" \")\n",
    "    print(\"Test \")\n",
    "    # print(\"Linear \", Lin_reg.score(X, yb), Lin_reg.score(x_test, y_test), Lin_reg.score(x_train, y_train) )\n",
    "    print(\"DTR    \", DT_reg.score(x_test, y_test), DT_reg.score(x_train, y_train))\n",
    "    print(\"RFR    \", RF_reg.score(x_test, y_test), RF_reg.score(x_train, y_train) )\n",
    "    print(\"XGB    \", XG_reg.score(x_test, y_test), XG_reg.score(x_train, y_train) )\n",
    "    print(\" \")\n",
    "\n",
    "lr_list = [0,   1,   2,   3, \n",
    "           4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
    "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
    "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
    "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
    "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
    "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
    "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
    "       195, 196, 197, 198, 199\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the most optimum Random_state for train_test_split\n",
    "for find_optimum in lr_list:\n",
    "  x_train, x_test, y_train, y_test = train_test_split(xb, yb, train_size = (train_percent/100),\n",
    "                                                    shuffle = True, random_state=find_optimum )\n",
    "\n",
    "  train_linreg_DT_RF_XGBR (find_optimum  , x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN TEST SPLIT   = REGRESSION\n",
    "random_use_train_test_split_use = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0W7O3HD2xyLb",
    "outputId": "de954e69-4ac7-45a5-c9aa-0a833db360e8"
   },
   "outputs": [],
   "source": [
    "random_use = random_use_train_test_split_use\n",
    "x_train, x_test, y_train, y_test = train_test_split(xb, yb, train_size = (train_percent/100),\n",
    "                                                    shuffle = True, random_state= random_use )\n",
    "Lin_reg = LinearRegression()\n",
    "Lin_reg.fit(x_train, y_train)\n",
    "\n",
    "# print(Lin_reg.coef_)\n",
    "print(\"Linear \", Lin_reg.score(x_train, y_train))\n",
    "\n",
    "\n",
    "model_DTR = DecisionTreeRegressor(random_state= random_use)\n",
    "model_DTR.fit(x_train, y_train)\n",
    "\n",
    "print(\"DTR    \", model_DTR.score(x_train, y_train))\n",
    "\n",
    "\n",
    "model_RFR = RandomForestRegressor(random_state= random_use)\n",
    "model_RFR.fit(x_train, y_train)\n",
    "\n",
    "print(\"RFR    \", model_RFR.score(x_train, y_train))\n",
    "\n",
    "model_ETR = ExtraTreesRegressor(random_state= random_use)\n",
    "model_ETR.fit(x_train, y_train)\n",
    "\n",
    "print(\"ETR    \", model_ETR.score(x_train, y_train))\n",
    "\n",
    "model_XGBR = XGBRegressor(random_state= random_use)\n",
    "model_XGBR.fit(x_train, y_train)\n",
    "\n",
    "print(\"XGB    \", model_XGBR.score(x_train, y_train))\n",
    "print(\" \")\n",
    "print(\"Test \")\n",
    "print(\"Linear \", Lin_reg.score(x_test, y_test))\n",
    "print(\"DTR    \", model_DTR.score(x_test, y_test))\n",
    "print(\"RFR    \", model_RFR.score(x_test, y_test))\n",
    "print(\"ETR    \", model_ETR.score(x_test, y_test))\n",
    "print(\"XGB    \", model_XGBR.score(x_test, y_test))\n",
    "print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTqIZcjmxyLb",
    "outputId": "6dc3b699-f72a-4742-a7e7-5072007ff0db"
   },
   "outputs": [],
   "source": [
    "xb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Jf_ZRzDxyLb"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(xb)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=x_scaled.shape[1])\n",
    "x_pca = pca.fit_transform(x_scaled)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_pca = LinearRegression()\n",
    "model_pca.fit(x_pca, yb)\n",
    "\n",
    "y_pred_pca = model_pca.predict(x_pca)\n",
    "\n",
    "pca_var_ratio = pd.DataFrame( { \"ratio\" : pca.explained_variance_ratio_.round(5) , \"cum_ratio\" : np.cumsum(pca.explained_variance_ratio_)})\n",
    "pca_regression = pd.DataFrame( { \"y_pred_pca\" : y_pred_pca.flatten().round(5) , \"yb\" : yb.values.flatten()})\n",
    "\n",
    "number_of_columns=xb.shape[1]\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=number_of_columns)\n",
    "principal_components = pca.fit_transform(x_scaled)\n",
    "# x_pca_columns=[f'PC{i}' for i in range(1, number_of_columns + 1)]\n",
    "x_pca_columns=xb.columns\n",
    "# Create a DataFrame with the principal components\n",
    "pc_df = pd.DataFrame(data=principal_components, columns=x_pca_columns )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhMTJVj-xyLb"
   },
   "outputs": [],
   "source": [
    "print(r2_score(yb, y_pred_pca))\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(yb, y_pred_pca, alpha=0.4)\n",
    "plt.plot([yb.min(), yb.max()], [yb.min(), yb.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.xticks(np.arange(0,1.1,0.2))\n",
    "plt.yticks(np.arange(0,1.1,0.2))\n",
    "plt.title('Actual vs. PCA-Predicted TSV')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# variance plot\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance by Principal Components')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "# Scree Plot\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)')\n",
    "plt.title('Explained Variance')\n",
    "plt.show()\n",
    "\n",
    "# 2D Scatter Plot\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.scatterplot(x=xb.columns[0], y=xb.columns[1], data=pc_df)\n",
    "plt.title('PCA - 2 Principal Components')\n",
    "plt.show()\n",
    "\n",
    "# Biplot\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(pc_df[xb.columns[0]], pc_df[xb.columns[1]])\n",
    "for i, column in enumerate(xb.columns):\n",
    "    plt.arrow(0, 0, pca.components_[0, i], pca.components_[1, i], color='r', alpha=0.5)\n",
    "    plt.text(pca.components_[0, i], pca.components_[1, i], column, color='g', ha='center', va='center')\n",
    "plt.xlabel(xb.columns[0])\n",
    "plt.ylabel(xb.columns[1])\n",
    "plt.title('PCA Biplot')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Biplot\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(pc_df[xb.columns[1]], pc_df[xb.columns[2]])\n",
    "for i, column in enumerate(xb.columns):\n",
    "    plt.arrow(0, 0, pca.components_[1, i], pca.components_[2, i], color='r', alpha=0.5)\n",
    "    plt.text(pca.components_[1, i], pca.components_[2, i], column, color='g', ha='center', va='center')\n",
    "plt.xlabel(xb.columns[1])\n",
    "plt.ylabel(xb.columns[2])\n",
    "plt.title('PCA Biplot')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# # Biplot\n",
    "# plt.figure(figsize=(4, 3))\n",
    "# plt.scatter(pc_df['PC2'], pc_df['PC3'])\n",
    "# for i, column in enumerate(xb.columns):\n",
    "#     plt.arrow(0, 0, pca.components_[0, i], pca.components_[1, i], color='r', alpha=0.5)\n",
    "#     plt.text(pca.components_[0, i], pca.components_[1, i], column, color='g', ha='center', va='center')\n",
    "# plt.xlabel('PC2')\n",
    "# plt.ylabel('PC3')\n",
    "# plt.title('PCA Biplot')\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# # Biplot\n",
    "# plt.figure(figsize=(4, 3))\n",
    "# plt.scatter(pc_df['PC3'], pc_df['PC4'])\n",
    "# for i, column in enumerate(xb.columns):\n",
    "#     plt.arrow(0, 0, pca.components_[0, i], pca.components_[1, i], color='r', alpha=0.5)\n",
    "#     plt.text(pca.components_[0, i], pca.components_[1, i], column, color='g', ha='center', va='center')\n",
    "# plt.xlabel('PC3')\n",
    "# plt.ylabel('PC4')\n",
    "# plt.title('PCA Biplot')\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# # Biplot\n",
    "# plt.figure(figsize=(4, 3))\n",
    "# plt.scatter(pc_df['PC1'], pc_df['PC4'])\n",
    "# for i, column in enumerate(xb.columns):\n",
    "#     plt.arrow(0, 0, pca.components_[0, i], pca.components_[1, i], color='r', alpha=0.5)\n",
    "#     plt.text(pca.components_[0, i], pca.components_[1, i], column, color='g', ha='center', va='center')\n",
    "# plt.xlabel('PC1')\n",
    "# plt.ylabel('PC4')\n",
    "# plt.title('PCA Biplot')\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# # Biplot\n",
    "# plt.figure(figsize=(4, 3))\n",
    "# plt.scatter(pc_df['PC5'], pc_df['PC9'])\n",
    "# for i, column in enumerate(xb.columns):\n",
    "#     plt.arrow(0, 0, pca.components_[0, i], pca.components_[1, i], color='r', alpha=0.5)\n",
    "#     plt.text(pca.components_[0, i], pca.components_[1, i], column, color='g', ha='center', va='center')\n",
    "# plt.xlabel('PC5')\n",
    "# plt.ylabel('PC9')\n",
    "# plt.title('PCA Biplot')\n",
    "# plt.grid()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjGtE3eGBPLw"
   },
   "source": [
    "## Data Splitting - classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JD-ZxP_BPL_"
   },
   "outputs": [],
   "source": [
    "variable_scale  = MinMaxScaler() #StandardScaler()\n",
    "variable_scale2 = MinMaxScaler()\n",
    "train_percent   = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zQDReYuxyLb",
    "outputId": "79447d34-8ae8-4455-d17f-861f7cd07a4a"
   },
   "outputs": [],
   "source": [
    "index_of_data_ML = [ 'school_id','room_id'  ,'batch', 'moment']\n",
    "combined2['moment'] = combined2.apply(determine_batch_to_moment, axis=1)\n",
    "data_ML = combined2[combined2['time']>=time_after][index_of_data_ML + input_variables+[target_col]]#.dropna()#[combined2[room_col_name]!=class4]#[combined2[room_col_name]!=class4]#[combined2[target_col].between(-2,2)][combined2[focus_plot]>=0].copy()#.loc[:, interpolate_col + is_col_main]#.drop(['ppd', 'pmv'], axis=1) .dropna()  #,'temp_s','rain_o','rh_o', #[combined2['time']>time_after]\n",
    "data_ML[target_col] = data_ML[target_col].fillna(0)\n",
    "data_ML = data_ML[data_ML['school_id']=='brannfjell']\n",
    "# data_ML = data_ML[data_ML['room_id']==str(23)]\n",
    "# index_of_data_ML.remove('hura_d')\n",
    "# index_of_data_ML.remove('hour')\n",
    "# index_of_data_ML.remove('moment')\n",
    "data_ML_C = data_ML.drop (index_of_data_ML, axis=1).dropna()\n",
    "print(data_ML_C .isna().sum())#.value_counts()  #COUNT NAN VALUES  or NAN CHECKER\n",
    "print(data_ML_C[target_col].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUHtAUnlBPMA"
   },
   "outputs": [],
   "source": [
    "data_input = data_ML_C #.dropna(subset='temp_a', how='all').copy()#.dropna(subset=[target_col], axis = 0, inplace=True )\n",
    "# data_input = data_ML.copy() .sort_values([target_col])   .dropna(subset=['temp_a'], how='all')\n",
    "#xb = data_input.loc[:, ['swc_f', 'swc_r', 'temp_v', 'temp_f', 'co2_a', 'temp_a', 'rh_a','voc_a', 'bright_a', 'temp_o', 'rh_o', 'winds_o']] #Predictor\n",
    "xb  = data_input.drop(target_col, axis=1).copy() #.drop(columns=['temp_f', 'pm25_a', 'pm1_a'] ) #.loc[:, ['temp_v', 'co2_a', 'temp_a', 'rh_a','voc_a', 'bright_a', 'temp_o', 'rh_o', 'winds_o']] #Predictor\n",
    "x   = variable_scale.fit_transform(xb)  #xb.copy() #\n",
    "yb  = data_input.loc[:, target_col] #Label\n",
    "xb.isna().sum()\n",
    "\n",
    "answer_replace01   = {0:0,-1:1, -2:1, 1:0, 2:0}\n",
    "answer_replace012  = {0:0,-1:1, -2:2, 1:0, 2:0}\n",
    "answer_replace0123 = {0:0,-1:2, -2:3, 1:1, 2:0}\n",
    "y = np.array(yb.copy() .replace(answer_replace01)).flatten()\n",
    "# y = variable_scale2.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1711381635050,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "uJ-39x39BPMB",
    "outputId": "a614ddee-07df-4f57-dcd8-97210f34d3c3"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = (train_percent/100), shuffle = True)\n",
    "#x_train = pd.DataFrame(x_train,  columns = xb.columns)\n",
    "\n",
    "print(\"Before oversampling train: \", Counter(y_train))\n",
    "print(\"Before oversamplig test: \", Counter(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STeBThj2qAqk",
    "outputId": "aca28c9e-cac0-4cce-a6c7-dafc0bf4ea0d"
   },
   "outputs": [],
   "source": [
    "# define oversampling  strategy\n",
    "oversample_over = SMOTE(sampling_strategy='all')\n",
    "# fit and apply the transform\n",
    "x_new, y_new = oversample_over.fit_resample(x,y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, train_size = (train_percent/100), shuffle = True)\n",
    "print(\"After oversampling train: \", Counter(y_train ))\n",
    "print(\"After oversampling test: \" , Counter(y_test ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bsCDlofYTDz"
   },
   "outputs": [],
   "source": [
    "# Check the number of unique classes in the target variable\n",
    "def PCA_LDA_plotter (x_input, y_input):\n",
    "    num_classes = len(np.unique(y_input))\n",
    "\n",
    "    # Check the number of features in the dataset\n",
    "    num_features = x_input.shape[1]\n",
    "\n",
    "    # If either number of classes or number of features is less than or equal to 1, print an error message\n",
    "    if num_classes <= 1 or num_features <= 1:\n",
    "        print(\"Error: Insufficient data for LDA.\")\n",
    "    else:\n",
    "        # Standardize the features\n",
    "        scaler = MinMaxScaler()\n",
    "        X_scaled = scaler.fit_transform(x_input)\n",
    "\n",
    "        # Perform PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "        # Perform LDA with the maximum number of components allowed\n",
    "        n_components_lda = min(X_scaled.shape[1], len(np.unique(y_input)) - 1)\n",
    "        lda = LinearDiscriminantAnalysis(n_components=n_components_lda)\n",
    "        X_lda = lda.fit_transform(X_scaled, y_input)\n",
    "\n",
    "        # Plot PCA results\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_input, cmap='viridis')\n",
    "        plt.title('PCA')\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.legend()\n",
    "        plt.ylabel('Principal Component 2')\n",
    "\n",
    "        # Plot LDA results\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if n_components_lda > 1:\n",
    "            plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y_input, cmap='viridis')\n",
    "            plt.title('LDA')\n",
    "            plt.xlabel('Linear Discriminant 1')\n",
    "            plt.ylabel('Linear Discriminant 2')\n",
    "            plt.legend()\n",
    "        else:\n",
    "            print(\"Error: Insufficient data for LDA.\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rE3sgu9XxyLc",
    "outputId": "8876e4a6-d629-41c6-d983-887d07766d16"
   },
   "outputs": [],
   "source": [
    "PCA_LDA_plotter (x_new, y_new)\n",
    "# PCA_LDA_plotter (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXS_4OCSGrQ1"
   },
   "source": [
    "## 1A. Train Classification ML *GridCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCU-7FfhhG2b"
   },
   "source": [
    "### Logistic Regression Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwU8FT_5qAql",
    "outputId": "80d3bc76-56f7-42df-d1bf-958fb631429c"
   },
   "outputs": [],
   "source": [
    "# #data_input = data_ML.dropna(subset=airthingsPlus, how='all').copy()#.dropna(subset=[target_col], axis = 0, inplace=True )\n",
    "# data_input = data_ML.sort_values([target_col])   #.dropna(subset=['temp_a'], how='all')\n",
    "# #xb = data_input.loc[:, ['swc_f', 'swc_r', 'temp_v', 'temp_f', 'co2_a', 'temp_a', 'rh_a','voc_a', 'bright_a', 'temp_o', 'rh_o', 'winds_o']] #Predictor\n",
    "# xb  = data_input.drop(target_col, axis=1).copy() #.drop(columns=['temp_f', 'pm25_a', 'pm1_a'] ) #.loc[:, ['temp_v', 'co2_a', 'temp_a', 'rh_a','voc_a', 'bright_a', 'temp_o', 'rh_o', 'winds_o']] #Predictor\n",
    "# x   = xb.copy() #variable_scale.fit_transform(xb)  #\n",
    "# yb  = data_input.loc[:, target_col] #Label\n",
    "# xb.isna().sum()\n",
    "\n",
    "# all parameters not specified are set to their defaults\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train, y_train)\n",
    "y_train_pred_logreg = logreg.predict(x_train)\n",
    "y_test_pred_logreg = logreg.predict(x_test)\n",
    "y_all_pred_logreg = logreg.predict(x)\n",
    "\n",
    "\n",
    "train_acc_logreg = accuracy_score(y_train, y_train_pred_logreg)#.round(3)\n",
    "test_acc_logreg = accuracy_score(y_test, y_test_pred_logreg)#.round(3)\n",
    "test_precision_logreg = precision_score(y_test, y_test_pred_logreg, average= 'weighted' )#.round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "rmse_logreg =np.sqrt(mean_squared_error(y_test,y_test_pred_logreg))#.round(3)\n",
    "f1_logreg = f1_score(y_test, y_test_pred_logreg, average= 'weighted' )#.round(3)\n",
    "\n",
    "print('Target : ' + str(target_col))\n",
    "print(\"Training Accuracy:\", train_acc_logreg)\n",
    "print(\"Testing Accuracy:\", test_acc_logreg)\n",
    "print(\"Testing Precision:\", test_precision_logreg)\n",
    "print('RMSE : ', rmse_logreg)\n",
    "print('f1 score :', f1_logreg)\n",
    "print(classification_report(y_test, y_test_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QvWsYghQqcP",
    "outputId": "4d6681bf-5088-4f88-8ec4-634831a13195"
   },
   "outputs": [],
   "source": [
    "conf_matrix_ML( y_test, y_test_pred_logreg, logreg, 'Log.Regression (Testing)' )\n",
    "conf_matrix_ML( y_train, y_train_pred_logreg, logreg, 'Log.Regression (Training)' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6pnAFWSGrRF"
   },
   "source": [
    "### Decision Tree Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 78547,
     "status": "ok",
     "timestamp": 1709104062633,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "_utuf1HLGw-r",
    "outputId": "6aed78be-7e07-4735-979c-820b587cd230"
   },
   "outputs": [],
   "source": [
    "model =DecisionTreeClassifier(   )\n",
    "param_grid = {\n",
    "    'criterion'    : ['gini' ,'entropy','log_loss'],\n",
    "    'max_depth'    : [1,2,3,4,5,8,10,13,21,34,42,55,89,100, 120,150,180,144,200,500,700],\n",
    "    'splitter'     : ['best', 'random'],\n",
    "    'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "    'random_state' : [1,2,5,10,50,100,200]\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(x_train, y_train )\n",
    "model_DTC = grid_search.best_estimator_\n",
    "ML_grid_result (grid_search, model_DTC, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUru1vasxyLc"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=combined_pivot_mean  , y='rd_temp', x='temp_a'  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_5DoaMOGrRG"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_DTC = model_DTC.predict(x_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_test_pred_DTC = model_DTC.predict(x_test)\n",
    "\n",
    "train_acc_DTC = accuracy_score(y_train, y_train_pred_DTC).round(3)\n",
    "test_acc_DTC = accuracy_score(y_test, y_test_pred_DTC).round(3)\n",
    "test_precision_DTC = precision_score(y_test, y_test_pred_DTC, average= 'weighted' ).round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "rmse_DTC =np.sqrt(mean_squared_error(y_test,y_test_pred_DTC)).round(3)\n",
    "f1_DTC = f1_score(y_test, y_test_pred_DTC, average= 'weighted' ).round(3)\n",
    "\n",
    "print('Target : ' + target_col)\n",
    "print(\"Training Accuracy:\", train_acc_DTC)\n",
    "print(\"Testing Accuracy:\", test_acc_DTC)\n",
    "print(\"Testing Precision:\", test_precision_DTC)\n",
    "print('RMSE : ', rmse_DTC)\n",
    "print('f1 score :', f1_DTC)\n",
    "print(classification_report(y_test, y_test_pred_DTC))\n",
    "\n",
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                \"Dec.Tree.Class\":model_DTC.feature_importances_.round(4),\n",
    "                                }).sort_values( by=\"Dec.Tree.Class\" ,ascending = False)\n",
    "data_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5h5J-TJSqAql"
   },
   "outputs": [],
   "source": [
    "# fn = model_DTC.feature_names_in_\n",
    "# cn= model_DTC.feature_names_in_\n",
    "# figsize = (50,10)\n",
    "\n",
    "# fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = figsize, dpi=1200)\n",
    "# for index in range(0, 1):\n",
    "#     tree.plot_tree(model_DTC,\n",
    "#                    feature_names = fn,\n",
    "#                    class_names=target_col,\n",
    "#                    filled = True,\n",
    "#                    ax = axes);\n",
    "\n",
    "#     # axes.set_title('Estimator: ' + str(index), fontsize = 11)\n",
    "# fig.savefig(directory_path + '/'+ folder_ML + '/' + 'model_DTC_' + str(target_col) + '.pdf', format= 'pdf')\n",
    "# fig.savefig(directory_path + '/'+ folder_ML + '/' + 'model_DTC_' + str(target_col) + '.png', format= 'png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blUfhgxlGrRH"
   },
   "outputs": [],
   "source": [
    "conf_matrix_ML( y_test, y_test_pred_DTC, model_DTC, 'Decision Tree (Testing)' )\n",
    "conf_matrix_ML( y_train, y_train_pred_DTC, model_DTC, 'Decision Tree (Training)' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ma5EnKvVQqcR"
   },
   "source": [
    "### Extra Trees Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IO4EP_H_QqcR"
   },
   "outputs": [],
   "source": [
    "model =ExtraTreesClassifier( )\n",
    "param_grid = {\n",
    "    'criterion'    : ['gini','entropy','log_loss'],\n",
    "    'max_depth'    : [1,2,3,4,5,8,10,13,21,34,42,55,89,100, 120,150,180,144,200,500,700],\n",
    "    # 'splitter'     : ['best', 'random'],\n",
    "    'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "    'random_state' : [1,2,10,100],\n",
    "    'n_estimators'  : [10,50,100,200,300,500]\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=6)\n",
    "grid_search.fit(x_train, y_train )\n",
    "\n",
    "model_ETC = grid_search.best_estimator_\n",
    "ML_grid_result (grid_search, model_ETC, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-W_y_RfQqcS"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_ETC = model_ETC.predict(x_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_test_pred_ETC = model_ETC.predict(x_test)\n",
    "\n",
    "train_acc_ETC = accuracy_score(y_train, y_train_pred_ETC).round(3)\n",
    "test_acc_ETC = accuracy_score(y_test, y_test_pred_ETC).round(3)\n",
    "test_precision_ETC = precision_score(y_test, y_test_pred_ETC, average= 'weighted' ).round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "rmse_ETC =np.sqrt(mean_squared_error(y_test,y_test_pred_ETC)).round(3)\n",
    "f1_ETC = f1_score(y_test, y_test_pred_ETC, average= 'weighted' ).round(3)\n",
    "\n",
    "print('Target : ' + target_col)\n",
    "print(\"Training Accuracy:\", train_acc_ETC)\n",
    "print(\"Testing Accuracy:\", test_acc_ETC)\n",
    "print(\"Testing Precision:\", test_precision_ETC)\n",
    "print('RMSE : ', rmse_ETC)\n",
    "print('f1 score :', f1_ETC)\n",
    "print(classification_report(y_test, y_test_pred_ETC))\n",
    "\n",
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                \"Dec.Tree.Class\":model_ETC.feature_importances_.round(4),\n",
    "                                }).sort_values( by=\"Dec.Tree.Class\" ,ascending = False)\n",
    "data_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-BAXWe3QqcS"
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(25, 12))\n",
    "#plot_tree(ETC)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01mNYrHSQqcS"
   },
   "outputs": [],
   "source": [
    "conf_matrix_ML( y_test, y_test_pred_ETC, model_ETC, 'Extras Tree (Testing)' )\n",
    "conf_matrix_ML( y_train, y_train_pred_ETC, model_ETC, 'Extras Tree (Training)' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCXuLweGQqcS"
   },
   "source": [
    "### Gaussian Process Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyecQ6jnQqcS"
   },
   "outputs": [],
   "source": [
    "# model = GaussianProcessClassifier( )\n",
    "\n",
    "# param_grid = {\n",
    "    # 'kernel'    : [1.0 * RBF(1.0)]\n",
    "    # ,'n_restarts_optimizer' : [0,1,2,3,4,10]\n",
    "    # ,'max_iter_predict'   : [100]\n",
    "    # ,'random_state'       : [1,2,5,10,20,30,50,80,130,210,440]\n",
    "# }\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "# grid_search.fit(x_train, y_train )\n",
    "\n",
    "# model_GPC = grid_search.best_estimator_\n",
    "# ML_grid_result (grid_search, model_GPC, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WV_V2yfGQqcS"
   },
   "outputs": [],
   "source": [
    "# # Predict train set labels\n",
    "# y_train_pred_GPC = model_GPC.predict(x_train)\n",
    "\n",
    "# # Predict test set labels\n",
    "# y_test_pred_GPC = model_GPC.predict(x_test)\n",
    "\n",
    "# train_acc_GPC = accuracy_score(y_train, y_train_pred_GPC).round(3)\n",
    "# test_acc_GPC = accuracy_score(y_test, y_test_pred_GPC).round(3)\n",
    "# test_precision_GPC = precision_score(y_test, y_test_pred_GPC, average= 'weighted' ).round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "# rmse_GPC =np.sqrt(mean_squared_error(y_test,y_test_pred_GPC)).round(3)\n",
    "# f1_GPC = f1_score(y_test, y_test_pred_GPC, average= 'weighted' ).round(3)\n",
    "\n",
    "# print('Target : ' + target_col)\n",
    "# print(\"Training Accuracy:\", train_acc_GPC)\n",
    "# print(\"Testing Accuracy:\", test_acc_GPC)\n",
    "# print(\"Testing Precision:\", test_precision_GPC)\n",
    "# print('RMSE : ', rmse_GPC)\n",
    "# print('f1 score :', f1_GPC)\n",
    "# print(classification_report(y_test, y_test_pred_GPC))\n",
    "\n",
    "# cm_dt = confusion_matrix(y_test,y_test_pred_GPC, labels = model_GPC.classes_ )\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm_dt, display_labels=model_GPC.classes_)\n",
    "# disp.plot(  )\n",
    "# plt.title('Confussion Matrix : Gaussian Process (Testing)')\n",
    "# plt.show(  )\n",
    "\n",
    "# cm_dt = confusion_matrix(y_train,y_train_pred_GPC, labels = model_GPC.classes_ )\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm_dt, display_labels=model_GPC.classes_)\n",
    "# disp.plot(  )\n",
    "# plt.title('Confussion Matrix : Gaussian Process (Training)')\n",
    "# plt.show(  )\n",
    "\n",
    "# conf_matrix_ML(y_test,y_test_pred_GPC, model_GPC, 'Gaussian P. Class. (Testing)')\n",
    "# conf_matrix_ML(y_train,y_train_pred_GPC, model_GPC, 'Gaussian P. Class. (Training)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kJOr58KQqcS"
   },
   "source": [
    "### KNN Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkxA5lOcQqcS"
   },
   "outputs": [],
   "source": [
    "model =KNeighborsClassifier(   )\n",
    "param_grid = {\n",
    "    'n_neighbors'    : [1,2,3,4,5,8,10,13,21,34,55,89,100, 120,150,180,144,200,500]\n",
    "    ,'weights'  : ['uniform', 'distance']\n",
    "    ,'algorithm'   : [ 'ball_tree', 'kd_tree', 'brute']\n",
    "    ,'leaf_size' : [10,20,30,50,80,130,210,440]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(x_train, y_train )\n",
    "\n",
    "model_KNN = grid_search.best_estimator_\n",
    "ML_grid_result (grid_search, model_KNN, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yy_Uz_6hQqcS"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_KNN = model_KNN.predict(x_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_test_pred_KNN = model_KNN.predict(x_test)\n",
    "\n",
    "train_acc_KNN = accuracy_score(y_train, y_train_pred_KNN).round(3)\n",
    "test_acc_KNN = accuracy_score(y_test, y_test_pred_KNN).round(3)\n",
    "test_precision_KNN = precision_score(y_test, y_test_pred_KNN, average= 'weighted' ).round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "rmse_KNN =np.sqrt(mean_squared_error(y_test,y_test_pred_KNN)).round(3)\n",
    "f1_KNN = f1_score(y_test, y_test_pred_KNN, average= 'weighted' ).round(3)\n",
    "\n",
    "print('Target : ' + target_col)\n",
    "print(\"Training Accuracy:\", train_acc_KNN)\n",
    "print(\"Testing Accuracy:\", test_acc_KNN)\n",
    "print(\"Testing Precision:\", test_precision_KNN)\n",
    "print('RMSE : ', rmse_KNN)\n",
    "print('f1 score :', f1_KNN)\n",
    "print(classification_report(y_test, y_test_pred_KNN))\n",
    "\n",
    "conf_matrix_ML(y_test,y_test_pred_KNN, model_KNN, 'KNN Class. (Testing)')\n",
    "conf_matrix_ML(y_train,y_train_pred_KNN, model_KNN, 'KNN Class. (Training)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMO3DtTaQqcS"
   },
   "source": [
    "### SVM Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHYydNCCQqcS"
   },
   "outputs": [],
   "source": [
    "model =SVC(   )\n",
    "param_grid = {\n",
    "    'C'        : [1,1.1,1.25,1.5],\n",
    "    'kernel'   : ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree'   : [ 1,2,3,4,5,6,7,8,9,10],\n",
    "    'gamma'    :  ['scale','auto'],\n",
    "    'random_state' : [1,2,5,10]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(x_train, y_train )\n",
    "\n",
    "model_SVC = grid_search.best_estimator_\n",
    "ML_grid_result (grid_search, model_SVC, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwqLDrn9QqcT"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_SVC = model_SVC.predict(x_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_test_pred_SVC = model_SVC.predict(x_test)\n",
    "\n",
    "train_acc_SVC = accuracy_score(y_train, y_train_pred_SVC).round(3)\n",
    "test_acc_SVC = accuracy_score(y_test, y_test_pred_SVC).round(3)\n",
    "test_precision_SVC = precision_score(y_test, y_test_pred_SVC, average= 'weighted' ).round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "rmse_SVC =np.sqrt(mean_squared_error(y_test,y_test_pred_SVC)).round(3)\n",
    "f1_SVC = f1_score(y_test, y_test_pred_SVC, average= 'weighted' ).round(3)\n",
    "\n",
    "print('Target : ' + target_col)\n",
    "print(\"Training Accuracy:\", train_acc_SVC)\n",
    "print(\"Testing Accuracy:\", test_acc_SVC)\n",
    "print(\"Testing Precision:\", test_precision_SVC)\n",
    "print('RMSE : ', rmse_SVC)\n",
    "print('f1 score :', f1_SVC)\n",
    "print(classification_report(y_test, y_test_pred_SVC))\n",
    "\n",
    "conf_matrix_ML(y_test,y_test_pred_SVC, model_SVC, 'Support Vector Class. (Testing)')\n",
    "conf_matrix_ML(y_train,y_train_pred_SVC, model_SVC, 'Support Vector Class. (Training)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x61xYj5nGrRH"
   },
   "source": [
    "### Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20VaChHMKvWR"
   },
   "outputs": [],
   "source": [
    "model =RandomForestClassifier()\n",
    "param_grid = {\n",
    "    'criterion'    : ['gini','entropy','log_loss'],\n",
    "    'max_depth'    : [1,2,3,4,5,8,10,13,21,34,55,89,100, 120,150,180,144,200,500],\n",
    "    'random_state' : [1,2,5,10,50,100],\n",
    "    'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "    'n_estimator'  : [10,50,100,200,500,1000]\n",
    "\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(x_train, y_train )\n",
    "model_RFC = grid_search.best_estimator_\n",
    "ML_grid_result (grid_search, model_RFC, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVoqviO4GrRI"
   },
   "outputs": [],
   "source": [
    "y_train_pred_RFC = model_RFC.predict(x_train)\n",
    "# Predict test set labels\n",
    "y_test_pred_RFC = model_RFC.predict(x_test)\n",
    "\n",
    "train_acc_RFC = accuracy_score(y_train, y_train_pred_RFC).round(3)\n",
    "test_acc_RFC = accuracy_score(y_test, y_test_pred_RFC).round(3)\n",
    "test_precision_RFC = precision_score(y_test, y_test_pred_RFC, average= 'weighted' ).round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "f1_RFC = f1_score(y_test, y_test_pred_RFC, average= 'weighted' ).round(3)\n",
    "rmse_RFC =np.sqrt(mean_squared_error(y_test,y_test_pred_RFC)).round(3)\n",
    "\n",
    "print('Target : ' + target_col)\n",
    "print(\"Training Accuracy:\", train_acc_RFC)\n",
    "print(\"Testing Accuracy:\", test_acc_RFC)\n",
    "print(\"Testing Precision:\", test_precision_RFC)\n",
    "print('f1 score :', f1_RFC)\n",
    "print('RMSE : ', rmse_RFC)\n",
    "print(classification_report(y_test, y_test_pred_RFC))\n",
    "\n",
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                \"RanFor.Class\":model_RFC.feature_importances_.round(4),\n",
    "                                })#.sort_values(ascending = False)\n",
    "data_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DE9NQzqsQqcT"
   },
   "outputs": [],
   "source": [
    "conf_matrix_ML(y_test,y_test_pred_RFC, model_RFC, 'Random Forest (Testing)')\n",
    "conf_matrix_ML(y_train,y_train_pred_RFC, model_RFC, 'Random Forest (Training)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJ_Z3FGkGrRJ"
   },
   "source": [
    "### Gradient Boost Classifier (Multi-Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dI-uQ8ZjLQeg"
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier( )\n",
    "param_grid = {\n",
    "    'loss'         : ['log_loss', 'exponential'],\n",
    "    'n_estimators' :  [1,3, 5,8,10,20,50,100,110,120, 130, 150,250],\n",
    "    'max_depth'    : [1,2,3,5,8,10,12,15,20,25,50,100,150,200],\n",
    "    'criterion'    : ['friedman_mse', 'squared_error'],\n",
    "    'learning_rate': [0.01,0.05,0.1,0.25,0.6,0.8,1]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=6)\n",
    "grid_search.fit(x_train, y_train )\n",
    "model_GBC = grid_search.best_estimator_\n",
    "\n",
    "ML_grid_result (grid_search, model_GBC, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJltBbikGrRK"
   },
   "outputs": [],
   "source": [
    "y_train_pred_GBC = model_GBC.predict(x_train)\n",
    "y_test_pred_GBC = model_GBC.predict(x_test)\n",
    "\n",
    "train_acc_GBC = accuracy_score(y_train, y_train_pred_GBC).round(3)\n",
    "test_acc_GBC = accuracy_score(y_test, y_test_pred_GBC).round(3)\n",
    "test_precision_GBC = precision_score(y_test, y_test_pred_GBC, average= 'weighted' ).round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "f1_GBC = f1_score(y_test,y_test_pred_GBC, average= 'weighted' ).round(3)\n",
    "rmse_GBC =np.sqrt(mean_squared_error(y_test,y_test_pred_GBC)).round(3)\n",
    "\n",
    "print('Target : ' + target_col)\n",
    "print(\"Training Accuracy:\", train_acc_GBC)\n",
    "print(\"Testing Accuracy:\", test_acc_GBC)\n",
    "print(\"Testing Precision:\", test_precision_GBC)\n",
    "print('f1 score :', f1_GBC)\n",
    "print('RMSE : ', rmse_GBC)\n",
    "print(classification_report(y_test, y_test_pred_GBC))\n",
    "\n",
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                \"GradBoost.Class\":model_GBC.feature_importances_.round(4),\n",
    "                                }).sort_values(by='features',ascending = False)\n",
    "data_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGk3EQqdGrRL"
   },
   "outputs": [],
   "source": [
    "conf_matrix_ML(y_test,y_test_pred_GBC, model_GBC, 'GradBoost Class. (Testing)')\n",
    "conf_matrix_ML(y_train,y_train_pred_GBC, model_GBC, 'GradBoost Class. (Training)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fnTe-P8QqcT"
   },
   "source": [
    "### XGBoost Classifier (Multi-Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhyUGCICQqcT",
    "outputId": "ca027706-ec3f-41a7-c8b7-8968e5ad28f3"
   },
   "outputs": [],
   "source": [
    "model = xg.XGBClassifier(   )\n",
    "param_grid = {\n",
    "    'n_estimators' : [10,20,50,100,110,120, 130, 150,250],\n",
    "    'learning_rate': [0.01,0.05,0.1,0.25,0.6,1],\n",
    "    'max_depth'    : [2,3,5,8,10,12,15,20,25,50,100,150,200],\n",
    "    'random_state' : [1,2,10,50,100]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(x_train, y_train )\n",
    "model_XGBC = grid_search.best_estimator_\n",
    "ML_grid_result (grid_search, model_XGBC, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYUem668QqcT"
   },
   "outputs": [],
   "source": [
    "y_train_pred_XGBC = model_XGBC.predict(x_train)\n",
    "# Predict test set labels\n",
    "y_test_pred_XGBC = model_XGBC.predict(x_test)\n",
    "\n",
    "train_acc_XGBC = accuracy_score(y_train, y_train_pred_XGBC).round(3)\n",
    "test_acc_XGBC = accuracy_score(y_test, y_test_pred_XGBC).round(3)\n",
    "test_precision_XGBC = precision_score(y_test, y_test_pred_XGBC, average= 'weighted' ).round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "f1_XGBC = f1_score(y_test, y_test_pred_XGBC, average= 'weighted' ).round(3)\n",
    "rmse_XGBC =np.sqrt(mean_squared_error(y_test,y_test_pred_XGBC)).round(3)\n",
    "\n",
    "print('Target : ' + target_col)\n",
    "print(\"Training Accuracy:\", train_acc_XGBC)\n",
    "print(\"Testing Accuracy:\", test_acc_XGBC)\n",
    "print(\"Testing Precision:\", test_precision_XGBC)\n",
    "print('f1 score :', f1_XGBC)\n",
    "print('RMSE : ', rmse_XGBC)\n",
    "print(classification_report(y_test, y_test_pred_XGBC))\n",
    "\n",
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                \"model_XGBC\":model_XGBC.feature_importances_.round(4),\n",
    "                                })#.sort_values(ascending = False)\n",
    "data_importance\n",
    "\n",
    "conf_matrix_ML(y_test,y_test_pred_XGBC, model_XGBC, 'XGBoost Class. (Testing)')\n",
    "conf_matrix_ML(y_train,y_train_pred_XGBC, model_XGBC, 'XGBoost Class. (Training)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sW5ZDGcRxeBb"
   },
   "source": [
    "### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnCMiG0thBNf"
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier()\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(1144, 272)], #(12,3), (12,5), (12, 8), (12, 13),(24,5) , (24,8), (24, 13), (24, 21), (36,8), (36,13), (36, 21), (108, 42), (108, 68), (236, 68),(236, 136),(572, 136),(572, 272), (1144, 272)\n",
    "    'activation'        : ['logistic'],  #'relu', 'logistic',  , 'tanh'\n",
    "    'solver'            : ['lbfgs'],    #'adam', 'sgd',\n",
    "    'learning_rate'     : ['constant'],     #,'invscaling','adaptive'\n",
    "    'alpha'             : [ 0.01],  # ,0.1\n",
    "    'max_iter'          : [1000]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=6)\n",
    "grid_search.fit(x_train, y_train )\n",
    "model_MLPC = grid_search.best_estimator_\n",
    "ML_grid_result (grid_search, model_MLPC, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDxHTSeexi_s"
   },
   "outputs": [],
   "source": [
    "#ALTERNATIVE\n",
    "model_MPLC = MLPClassifier( hidden_layer_sizes=(1144,272), activation='tanh', alpha=0.1, learning_rate='adaptive', solver='adam')\n",
    "model_MLPC.fit(x_train, y_train )\n",
    "print(\"Accuracy score (training)  : {:.3f}\".format(model_MLPC.score(x_train, y_train)))\n",
    "print(\"Accuracy score (validation): {:.3f}\".format(model_MLPC.score(x_test, y_test)))\n",
    "\n",
    "y_train_pred_MLPC = model_MLPC.predict(x_train)\n",
    "y_test_pred_MLPC = model_MLPC.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSQ65zVUx589"
   },
   "outputs": [],
   "source": [
    "train_acc_MLPC = accuracy_score(y_train, y_train_pred_MLPC).round(3)\n",
    "test_acc_MLPC = accuracy_score(y_test, y_test_pred_MLPC).round(3)\n",
    "test_precision_MLPC = precision_score(y_test, y_test_pred_MLPC, average= 'weighted' ).round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "rmse_MLPC =np.sqrt(mean_squared_error(y_test,y_test_pred_MLPC)).round(3)\n",
    "f1_MLPC = f1_score(y_test, y_test_pred_MLPC, average= 'weighted' ).round(3)\n",
    "\n",
    "print('Target : ' + target_col)\n",
    "print(\"Training Accuracy:\", train_acc_MLPC)\n",
    "print(\"Testing Accuracy:\", test_acc_MLPC)\n",
    "print(\"Testing Precision:\", test_precision_MLPC)\n",
    "print('RMSE : ', rmse_MLPC)\n",
    "print('f1 score :', f1_MLPC)\n",
    "print(classification_report(y_test, y_test_pred_MLPC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRcchdF7QqcU"
   },
   "outputs": [],
   "source": [
    "conf_matrix_ML(y_test,y_test_pred_MLPC, model_MLPC, 'MLPC Class. (Testing)')\n",
    "conf_matrix_ML(y_train,y_train_pred_MLPC, model_MLPC, 'MLPC Class. (Training)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aBBlM2xQqcU"
   },
   "source": [
    "### ANN custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxJs-9op1IIl"
   },
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcccUoJYQqcU"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_6SE1mx1NQJ"
   },
   "outputs": [],
   "source": [
    "xb.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7GDZmOc4QZ-"
   },
   "outputs": [],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AU_5Vr3VQqcU"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load your DataFrame\n",
    "# Replace \"your_dataframe.csv\" with the actual filename or use your data loading mechanism\n",
    "# df = pd.read_csv(\"your_dataframe.csv\")\n",
    "\n",
    "# Assume columns 1, 2, and 6 are the input features and column \"target\" is the target variable\n",
    "X = xb.astype(float).copy()\n",
    "# y = df['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwRMC16y190v"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Custom parabolic activation function\n",
    "def parabolic_activation(x):\n",
    "    return tf.where(x > 0, x, 0.01 * x)\n",
    "\n",
    "\n",
    "# Define a custom model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_dim=9, activation=parabolic_activation))  # Column 1\n",
    "model.add(Dense(128, activation=parabolic_activation))  # Column 2\n",
    "model.add(Dense(128, activation='sigmoid'))  # Column 3\n",
    "model.add(Dense(128, activation='sigmoid'))  # Column 4\n",
    "model.add(Dense(128, activation=parabolic_activation))  # Column 5\n",
    "model.add(Dense(128, activation=parabolic_activation))  # Column 6\n",
    "model.add(Dense(128, activation=parabolic_activation))  # Column 7\n",
    "model.add(Dense(128, activation=parabolic_activation))  # Column 8\n",
    "model.add(Dense(128, activation='sigmoid'))  # Column 9\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='hard_sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ae7Pc-ke7kYd"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred_DL_Seq = model.predict(X_test)\n",
    "# train_acc_MLPC = accuracy_score(y_train, y_train_pred_MLPC).round(3)\n",
    "# accuracy_DL = accuracy_score(y_test, y_pred_DL_Seq).round(3)\n",
    "# f1 = f1_score(y_test, y_pred_DL_Seq)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBbLt2kT9Qya"
   },
   "outputs": [],
   "source": [
    "plt.plot(y_pred_DL_Seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgNTbl-U91z9"
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-47CyOMeX5wD"
   },
   "source": [
    "### Classification ML - Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPWhGtJpX5wK"
   },
   "outputs": [],
   "source": [
    "data = np.array([['Algorithm', 'Train Acc', 'Test ACC', 'Test Precision', 'f1_score', 'RMSE'],\n",
    "                ['Log.Regression', train_acc_logreg, test_acc_logreg, test_precision_logreg, f1_logreg, rmse_logreg],\n",
    "                ['Decision Tree', train_acc_DTC, test_acc_DTC, test_precision_DTC, f1_DTC,  rmse_DTC],\n",
    "                ['Extra Trees', train_acc_ETC, test_acc_ETC, test_precision_ETC, f1_ETC,  rmse_ETC],\n",
    "                #['Gaussian Process', train_acc_GPC, test_acc_GPC, test_precision_GPC, f1_GPC, rmse_GPC],\n",
    "                #['K-Nearest N', train_acc_KNN, test_acc_KNN, test_precision_KNN, f1_KNN,  rmse_KNN],\n",
    "                #['Support Vector', train_acc_SVC, test_acc_SVC, test_precision_SVC, f1_SVC,  rmse_SVC],\n",
    "                ['Random Forest', train_acc_RFC, test_acc_RFC, test_precision_RFC, f1_RFC,  rmse_RFC],\n",
    "                #['XGBoost', train_acc_XGBC, test_acc_XGBC, test_precision_XGBC,   f1_XGBC,   rmse_XGBC],\n",
    "                # ['GradBoost', train_acc_GBC, test_acc_GBC, test_precision_GBC,   f1_GBC,   rmse_GBC],\n",
    "                # ['MultiLayer NN', train_acc_MLPC, test_acc_MLPC, test_precision_MLPC, f1_MLPC,  rmse_MLPC],\n",
    "                 ])\n",
    "\n",
    "table_classification = pd.DataFrame(data=data[1:, 1:],\n",
    "                     index = data[1:,0],\n",
    "                     columns=(data[0,1:])).sort_values('Test Precision', ascending=False)\n",
    "print('Target : ' + target_col)\n",
    "table_classification      .drop(['Test Precision', 'RSME'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gv07A-z6X5wL"
   },
   "outputs": [],
   "source": [
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                'Log.Regress' : logreg.coef_.flatten().round(4),\n",
    "                                #'Support Vector' : model_SVC.coef_.flatten().round(4),\n",
    "                                 \"Dec.Tree.Class\":model_DTC.feature_importances_.round(4),\n",
    "                                 \"Extra.Trees.Class\":model_ETC.feature_importances_.round(4),\n",
    "                                 \"RanFor.Class\" : model_RFC.feature_importances_.round(4),\n",
    "                                #\"GradBoost.Class\": model_GBC.feature_importances_.round(4),\n",
    "                                #\"XGBoost.Class\": model_XGBC.feature_importances_.round(4)\n",
    " #                               ,\"AdaBoost.Class\": model_ADABC.feature_importances_ .round(4)\n",
    "                                }).sort_values('features', ascending = False)\n",
    "print('Target : ' + target_col)\n",
    "data_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSVeWUD5X5wL"
   },
   "outputs": [],
   "source": [
    "def features_plotter (df):\n",
    "    df.plot(kind='barh', stacked=False, figsize=(6, 4))\n",
    "    # Customize labels and title\n",
    "    plt.ylabel('measured IAQ parameters')\n",
    "    plt.xlabel('(Cummulative) feature of importances')\n",
    "    plt.title('ML Result : importances of features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yw_nGIsBX5wL"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "data = {\n",
    "    'temp_a': [0.1384, 0.1644, None],\n",
    "    'rh_a': [None, 0.1176, None],\n",
    "    'co2_a': [0.1563, 0.1174, None],\n",
    "    'voc_a': [0.1844, 0.1007, None],\n",
    "    'enth_a': [0.1515, 0.1349, None],\n",
    "    'hura_a': [0.189, 0.1, None],\n",
    "    'temp_o': [0.0918, 0.1032, None],\n",
    "    'rh_o': [0.0614, None, None],\n",
    "    'enth_o': [None, 0.1007, None],\n",
    "    'hour (time)': [0.0272, 0.0611, None]}\n",
    "data_importance = pd.DataFrame(data, index = [ 'feel_temp', 'feel_air', 'None']).T.drop(columns='None',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubhAh_FMX5wL"
   },
   "outputs": [],
   "source": [
    "def pareto_histograph(df):\n",
    "\n",
    "\n",
    "    # Calculate cumulative sum for each column\n",
    "    df_cumsum = df.cumsum()\n",
    "\n",
    "    # Plot\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Plot bar histograms\n",
    "    df.plot(kind='bar', ax=ax1, color=['skyblue', 'salmon'], alpha=0.7, width=0.4, position=1)\n",
    "    ax1.set_ylabel('Value')\n",
    "\n",
    "    # Plot cumulative lines\n",
    "    df_cumsum.plot(kind='line', ax=ax2, marker='o', color=['blue', 'red'], linestyle='-', linewidth=2)\n",
    "\n",
    "    # Add legend\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    # Add labels and title\n",
    "    ax2.set_ylabel('Cumulative Sum')\n",
    "    plt.title('Bar Histograms and Cumulative Lines')\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpl-GenAX5wL"
   },
   "outputs": [],
   "source": [
    "pareto_histograph(data_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1sO280jsOcQ"
   },
   "source": [
    "## 1B. Train Classification ML *manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzAoQSwTsi6M"
   },
   "source": [
    "### Logistic Regression Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1701272702634,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "Aex5eztAsi6T",
    "outputId": "8b06a889-b8f7-4e53-a302-cc48944dd855"
   },
   "outputs": [],
   "source": [
    "# all parameters not specified are set to their defaults\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train, y_train)\n",
    "y_train_pred_logreg = logreg.predict(x_train)\n",
    "y_test_pred_logreg = logreg.predict(x_test)\n",
    "f1_score(y_test, y_test_pred_logreg, average= 'weighted' )#.round(3)\n",
    "\n",
    "train_acc_logreg = accuracy_score(y_train, y_train_pred_logreg)#.round(3)\n",
    "test_acc_logreg = accuracy_score(y_test, y_test_pred_logreg)#.round(3)\n",
    "test_precision_logreg = precision_score(y_test, y_test_pred_logreg, average= 'weighted' )#.round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "rmse_logreg =np.sqrt(mean_squared_error(y_test,y_test_pred_logreg))#.round(3)\n",
    "f1_logreg = f1_score(y_test, y_test_pred_logreg, average= 'weighted' )#.round(3)\n",
    "\n",
    "print('Target : ' + str(target_col))\n",
    "print(\"Training Accuracy:\", train_acc_logreg)\n",
    "print(\"Testing Accuracy:\", test_acc_logreg)\n",
    "print(\"Testing Precision:\", test_precision_logreg)\n",
    "print('RMSE : ', rmse_logreg)\n",
    "print('f1 score :', f1_logreg)\n",
    "print(classification_report(y_test, y_test_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWth57sbsi6T",
    "outputId": "21b88582-c79e-44e4-f999-0ad71318732c"
   },
   "outputs": [],
   "source": [
    "print('Target : ' + str(target_col))\n",
    "conf_matrix_ML( y_test, y_test_pred_logreg, logreg, 'Log.Regression (Testing)' )\n",
    "conf_matrix_ML( y_train, y_train_pred_logreg, logreg, 'Log.Regression (Training)' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ntd2VtccjH2"
   },
   "source": [
    "### Extra Trees Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbXhz1BYcjH-",
    "outputId": "9f9bee76-4073-4e2a-b3f2-86e97ae759fb"
   },
   "outputs": [],
   "source": [
    "#optimum MAX_DEPTH\n",
    "lr_list = [1,2,3,4,5,8,10,13,21,34,38,42,55,89,100, 120,150,180,144,200,500,700]\n",
    "for find_optimum in lr_list:\n",
    "  ETC_optimum = ExtraTreesClassifier(max_depth=find_optimum, random_state=10)\n",
    "  ETC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, ETC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I59d3PZZcjH-"
   },
   "outputs": [],
   "source": [
    "max_depth_use = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xEiWno6cjH-",
    "outputId": "7fb0e22b-ca77-4f4a-ef0f-2d30a061881f"
   },
   "outputs": [],
   "source": [
    "#optimum RANDOM_STATE\n",
    "lr_list = [1,2,5,10,50,100,200,400]\n",
    "for find_optimum in lr_list:\n",
    "  ETC_optimum = ExtraTreesClassifier(max_depth=max_depth_use, random_state=find_optimum)\n",
    "  ETC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, ETC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MwyRFVYcjH-"
   },
   "outputs": [],
   "source": [
    "random_use = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdHOc9zLcjH_",
    "outputId": "57c4d05e-baa3-45a8-dff7-bc9b11bd2624"
   },
   "outputs": [],
   "source": [
    "#optimum CRITERION\n",
    "lr_list = ['gini','entropy','log_loss']\n",
    "for find_optimum in lr_list:\n",
    "  ETC_optimum = ExtraTreesClassifier(max_depth=max_depth_use, random_state=random_use, criterion= find_optimum)\n",
    "  ETC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, ETC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiFjpmswcjH_"
   },
   "outputs": [],
   "source": [
    "criterion_use = 'gini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laUPbaNCcjH_",
    "outputId": "d168e827-ce73-4e27-fb19-003265f6229a"
   },
   "outputs": [],
   "source": [
    "#optimum SPLITTER\n",
    "lr_list = [2,5,10,20,50,100,110,120, 130, 150,250]\n",
    "for find_optimum in lr_list:\n",
    "  ETC_optimum = ExtraTreesClassifier(max_depth=max_depth_use, random_state=random_use,\n",
    "                                       criterion= criterion_use, n_estimators= find_optimum )\n",
    "  ETC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, ETC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RuPw39l7cjH_"
   },
   "outputs": [],
   "source": [
    "n_estimator_use = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JANt0LYjcjH_"
   },
   "outputs": [],
   "source": [
    "#optimum SPLITTER\n",
    "lr_list = ['auto', 'sqrt', 'log2']\n",
    "for find_optimum in lr_list:\n",
    "  ETC_optimum = ExtraTreesClassifier(max_depth=max_depth_use, random_state=random_use,\n",
    "                                       criterion= criterion_use, n_estimators=n_estimator_use, max_features= find_optimum)\n",
    "  ETC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, ETC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thYpeUz_cjH_"
   },
   "outputs": [],
   "source": [
    "max_features_use = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_sNXRWzcjH_"
   },
   "outputs": [],
   "source": [
    "# Fit decision tree model\n",
    "ETC = ExtraTreesClassifier(max_depth=max_depth_use, random_state=random_use,\n",
    "                                       criterion= criterion_use, n_estimators=n_estimator_use,\n",
    "                                    #    max_features= max_features_use\n",
    "                                       )\n",
    "ETC.fit(x_train, y_train)\n",
    "\n",
    "# Fit decision tree input-output\n",
    "model_ETC = ETC.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1697631197442,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "eHaR00hCcjH_",
    "outputId": "e441361e-490c-461a-e1df-4bb5a81a017c"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_ETC = model_ETC.predict(x_train)\n",
    "y_test_pred_ETC = model_ETC.predict(x_test)\n",
    "\n",
    "train_acc_ETC = accuracy_score(y_train, y_train_pred_ETC)#.round(3)\n",
    "test_acc_ETC = accuracy_score(y_test, y_test_pred_ETC)#.round(3)\n",
    "test_precision_ETC = precision_score(y_test, y_test_pred_ETC, average= 'weighted' )#.round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "rmse_ETC =np.sqrt(mean_squared_error(y_test,y_test_pred_ETC))#.round(3)\n",
    "f1_ETC = f1_score(y_test, y_test_pred_ETC, average= 'weighted' )#.round(3)\n",
    "\n",
    "# print('Target : ' + target_col)\n",
    "print(\"Training Accuracy:\", train_acc_ETC)\n",
    "print(\"Testing Accuracy:\", test_acc_ETC)\n",
    "print(\"Testing Precision:\", test_precision_ETC)\n",
    "print('RMSE : ', rmse_ETC)\n",
    "print('f1 score :', f1_ETC)\n",
    "print(classification_report(y_test, y_test_pred_ETC))\n",
    "\n",
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                \"Extra.Trees.Class\":model_ETC.feature_importances_#.round(4),\n",
    "                                }).sort_values( by=\"Extra.Trees.Class\" ,ascending = False)\n",
    "data_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udPaui41cjIA",
    "outputId": "49ab6b53-5214-40d1-d9b4-eec51f300a6a"
   },
   "outputs": [],
   "source": [
    "conf_matrix_ML( y_test, y_test_pred_ETC, model_ETC, 'Extra Trees Class. (Testing)' )\n",
    "conf_matrix_ML( y_train, y_train_pred_ETC, model_ETC, 'Extra Trees Class.  (Training)' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tmehvJ1qAqp"
   },
   "outputs": [],
   "source": [
    "# fn = model_ETC.feature_names_in_\n",
    "# cn= model_ETC.feature_names_in_\n",
    "# figsize = (n_estimator_use*5,5)\n",
    "\n",
    "# fig, axes = plt.subplots(nrows = 1,ncols = n_estimator_use,figsize = figsize, dpi=900)\n",
    "# for index in range(0, n_estimator_use):\n",
    "#     tree.plot_tree(model_ETC.estimators_[index],\n",
    "#                    feature_names = fn,\n",
    "#                    class_names=target_col,\n",
    "#                    filled = True,\n",
    "#                    ax = axes[index]);\n",
    "\n",
    "#     axes[index].set_title('Estimator: ' + str(index), fontsize = 11)\n",
    "# fig.savefig(directory_path + '/'+ folder_ML + '/' + 'model_ETC_' + str(target_col) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlJ1cyjQqAqp"
   },
   "outputs": [],
   "source": [
    "# fn = model_ETC.feature_names_in_\n",
    "# cn= model_ETC.feature_names_in_\n",
    "# figsize = (20,10 )\n",
    "# fig, axes = plt.subplots(figsize = figsize, dpi=800)\n",
    "# tree.plot_tree(model_ETC.estimators_[0],\n",
    "#                feature_names = fn,\n",
    "#                class_names=target_col,\n",
    "#                filled = True);\n",
    "# fig.savefig(directory_path + '/'+ folder_ML + '/' + 'model_ETC_' + str(target_col) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hqkMsrjXf08"
   },
   "source": [
    "### Decision Tree Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimum CRITERION\n",
    "lr_list = ['gini','entropy','log_loss']\n",
    "for find_optimum in lr_list:\n",
    "  DTC_optimum = DecisionTreeClassifier( criterion= find_optimum)\n",
    "  DTC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, DTC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_use = 'gini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimum MAX_DEPTH\n",
    "lr_list = [1,2,3,4,5,6,7,8,10,13,21,34,42,55,89,100#, 120,150,180,144,200,500,700\n",
    "           ]\n",
    "for find_optimum in lr_list:\n",
    "  DTC_optimum = DecisionTreeClassifier(max_depth=find_optimum, criterion=   criterion_use)\n",
    "  DTC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, DTC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_use = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimum max_features\n",
    "lr_list = [None, 'auto', 'sqrt']\n",
    "for find_optimum in lr_list:\n",
    "  DTC_optimum = DecisionTreeClassifier(max_depth=max_depth_use, max_features= find_optimum, criterion= criterion_use)\n",
    "  DTC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, DTC_optimum, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features_use = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimum min_samples_split\n",
    "lr_list = [1,2,5,7, 10,20,50]\n",
    "for find_optimum in lr_list:\n",
    "  DTC_optimum = DecisionTreeClassifier(max_depth=max_depth_use, max_features= max_features_use, criterion= criterion_use,\n",
    "                                       min_samples_split= find_optimum)\n",
    "  DTC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, DTC_optimum, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_split_use = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimum min_samples_leaf\n",
    "lr_list = [1,2,3,4,5,7,10]\n",
    "for find_optimum in lr_list:\n",
    "  DTC_optimum = DecisionTreeClassifier(max_depth=max_depth_use, max_features= max_features_use, criterion= criterion_use,\n",
    "                                       min_samples_split =min_samples_split_use   , min_samples_leaf= find_optimum )\n",
    "  DTC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, DTC_optimum, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf_use = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimum min_samples_leaf\n",
    "lr_list = ['best', 'random']\n",
    "for find_optimum in lr_list:\n",
    "  DTC_optimum = DecisionTreeClassifier(max_depth=max_depth_use, max_features= max_features_use, criterion= criterion_use,\n",
    "                                       min_samples_split =min_samples_split_use   , min_samples_leaf= min_samples_leaf_use,\n",
    "                                        splitter= find_optimum )\n",
    "  DTC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, DTC_optimum, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter_use = 'best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit decision tree model\n",
    "DTC = DecisionTreeClassifier(max_depth=max_depth_use, max_features= max_features_use, criterion= criterion_use,\n",
    "                                       min_samples_split =min_samples_split_use   , min_samples_leaf= min_samples_leaf_use,\n",
    "                                        splitter= splitter_use  )\n",
    "                                      \n",
    "DTC.fit(x_train, y_train)\n",
    "\n",
    "# Fit decision tree input-output\n",
    "model_DTC = DTC.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1697631197442,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -120
    },
    "id": "Rtoc6lGOjBDd",
    "outputId": "e441361e-490c-461a-e1df-4bb5a81a017c"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_DTC = model_DTC.predict(x_train)\n",
    "y_test_pred_DTC = model_DTC.predict(x_test)\n",
    "\n",
    "train_acc_DTC = accuracy_score(y_train, y_train_pred_DTC).round(3)\n",
    "test_acc_DTC = accuracy_score(y_test, y_test_pred_DTC).round(3)\n",
    "test_precision_DTC = precision_score(y_test, y_test_pred_DTC, average= 'weighted' ).round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "rmse_DTC =np.sqrt(mean_squared_error(y_test,y_test_pred_DTC)).round(3)\n",
    "f1_DTC = f1_score(y_test, y_test_pred_DTC, average= 'weighted' ).round(3)\n",
    "\n",
    "print('Target : ' , target_col)\n",
    "print(\"Training Accuracy:\", train_acc_DTC)\n",
    "print(\"Testing Accuracy:\", test_acc_DTC)\n",
    "print(\"Testing Precision:\", test_precision_DTC)\n",
    "print('RMSE : ', rmse_DTC)\n",
    "print('f1 score :', f1_DTC)\n",
    "print(classification_report(y_test, y_test_pred_DTC))\n",
    "\n",
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                \"Dec.Tree.Class\":model_DTC.feature_importances_.round(4),\n",
    "                                }).sort_values( by=\"Dec.Tree.Class\" ,ascending = False)\n",
    "data_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olrz9vgBs6Dm"
   },
   "outputs": [],
   "source": [
    "conf_matrix_ML( y_test, y_test_pred_DTC, model_DTC, 'Decision Tree (Testing)' )\n",
    "conf_matrix_ML( y_train, y_train_pred_DTC, model_DTC, 'Decision Tree (Training)' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOto02wybEB5"
   },
   "source": [
    "### Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimum N_ESTIMATORS\n",
    "lr_list = [10,25,50,75,100,200,500,1000,1500]\n",
    "for find_optimum in lr_list:\n",
    "  RFC_optimum = RandomForestClassifier(n_estimators=find_optimum)\n",
    "  RFC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, RFC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimator_use = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimum MAX_DEPTH\n",
    "lr_list = [1,2,3,4,5,6,7,8,10,13,21,34,42,55,89,100]\n",
    "for find_optimum in lr_list:\n",
    "  RFC_optimum = RandomForestClassifier(max_depth=find_optimum, n_estimators= n_estimator_use )\n",
    "  RFC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, RFC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_use = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimum MAX FEATURES\n",
    "lr_list = [1,2,5,7,8,'auto', 'sqrt']\n",
    "for find_optimum in lr_list:\n",
    "  RFC_optimum = RandomForestClassifier(max_depth=max_depth_use, n_estimators = n_estimator_use,\n",
    "                                       max_features= find_optimum)\n",
    "  RFC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, RFC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features_use = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimum MIN SAMPLES SPLIT\n",
    "lr_list = [1,2,5,8,10,15]\n",
    "for find_optimum in lr_list:\n",
    "  RFC_optimum = RandomForestClassifier(max_depth=max_depth_use, n_estimators = n_estimator_use, \n",
    "                                       max_features= max_features_use, min_samples_split = find_optimum)\n",
    "  RFC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, RFC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sample_split_use = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimum MIN SAMPLES SPLIT\n",
    "lr_list = [1,2,5,8,10,15]\n",
    "for find_optimum in lr_list:\n",
    "  RFC_optimum = RandomForestClassifier(max_depth=max_depth_use, n_estimators = n_estimator_use, \n",
    "                                       max_features= max_features_use, min_samples_split = min_sample_split_use,\n",
    "                                       min_samples_leaf= find_optimum  )\n",
    "  RFC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, RFC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf_use = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimum MIN SAMPLES SPLIT\n",
    "lr_list = [True, False]\n",
    "for find_optimum in lr_list:\n",
    "  RFC_optimum = RandomForestClassifier(max_depth=max_depth_use, n_estimators = n_estimator_use, \n",
    "                                       max_features= max_features_use, min_samples_split = min_sample_split_use,\n",
    "                                       min_samples_leaf= min_samples_leaf_use, bootstrap= find_optimum )\n",
    "  RFC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, RFC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_use = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit decision tree model\n",
    "RFC = RandomForestClassifier(max_depth=max_depth_use, n_estimators = n_estimator_use, \n",
    "                                       max_features= max_features_use, min_samples_split = min_sample_split_use,\n",
    "                                       min_samples_leaf= min_samples_leaf_use, bootstrap= bootstrap_use )\n",
    "RFC.fit(x_train, y_train)\n",
    "\n",
    "# Fit decision tree input-output\n",
    "model_RFC = RFC.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1709131827006,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "7hWeHXcYbECE",
    "outputId": "38402e03-8d03-487f-fdec-18139017c6e1"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_RFC = model_RFC.predict(x_train)\n",
    "y_test_pred_RFC = model_RFC.predict(x_test)\n",
    "\n",
    "train_acc_RFC = np.array(accuracy_score(y_train, y_train_pred_RFC)).round(3)\n",
    "test_acc_RFC =  np.array(accuracy_score(y_test, y_test_pred_RFC)).round(3)\n",
    "test_precision_RFC = precision_score(y_test, y_test_pred_RFC, average= 'weighted' ).round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "rmse_RFC =np.sqrt(mean_squared_error(y_test,y_test_pred_RFC)).round(3)\n",
    "f1_RFC = f1_score(y_test, y_test_pred_RFC, average= 'weighted' ).round(3)\n",
    "\n",
    "print('Target : ' + target_col)\n",
    "print(model_RFC.get_params)\n",
    "print(\"Training Accuracy:\", train_acc_RFC)\n",
    "print(\"Testing Accuracy:\", test_acc_RFC)\n",
    "print(\"Testing Precision:\", test_precision_RFC)\n",
    "print('RMSE : ', rmse_RFC)\n",
    "print('f1 score :', f1_RFC)\n",
    "print(classification_report(y_test, y_test_pred_RFC))\n",
    "\n",
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                \"RandomForest.Class\":model_RFC.feature_importances_.round(4),\n",
    "                                }).sort_values( by=\"RandomForest.Class\" ,ascending = False)\n",
    "data_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kpvN4C1bECE"
   },
   "outputs": [],
   "source": [
    "conf_matrix_ML( y_test, y_test_pred_RFC, model_RFC, 'RandomForest.Class (Testing)' )\n",
    "conf_matrix_ML( y_train, y_train_pred_RFC, model_RFC, 'RandomForest.Class Tree (Training)' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-ddhi0VqAqt"
   },
   "outputs": [],
   "source": [
    "fn = model_RFC.feature_names_in_\n",
    "cn= model_RFC.feature_names_in_\n",
    "figsize = (n_estimator_use*5,5)\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = n_estimator_use,figsize = figsize, dpi=900)\n",
    "for index in range(0, n_estimator_use):\n",
    "    tree.plot_tree(model_RFC.estimators_[index],\n",
    "                   feature_names = fn,\n",
    "                   class_names=target_col,\n",
    "                   filled = True,\n",
    "                   ax = axes[index]);\n",
    "\n",
    "    axes[index].set_title('Estimator: ' + str(index), fontsize = 11)\n",
    "fig.savefig(directory_path + '/'+ folder_ML + '/' + 'model_RFC_' + str(target_col) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAz7IRtTqAqt"
   },
   "outputs": [],
   "source": [
    "fn = model_RFC.feature_names_in_\n",
    "cn= model_RFC.feature_names_in_\n",
    "figsize = (n_estimator_use*5,5)\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = n_estimator_use,figsize = figsize, dpi=900)\n",
    "for index in range(0, n_estimator_use):\n",
    "    tree.plot_tree(model_RFC.estimators_[index],\n",
    "                   feature_names = fn,\n",
    "                   class_names=target_col,\n",
    "                   filled = True,\n",
    "                   ax = axes[index]);\n",
    "\n",
    "    axes[index].set_title('Estimator: ' + str(index), fontsize = 11)\n",
    "fig.savefig(directory_path + '/'+ folder_ML + '/' + 'model_RFC_' + str(target_col) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0VsYHf-mIqf"
   },
   "source": [
    "### KNN Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9558,
     "status": "ok",
     "timestamp": 1709131896875,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "ae9JZ0xymQSc",
    "outputId": "400ea2dd-ec45-40b8-ea10-f8dae43be5f8"
   },
   "outputs": [],
   "source": [
    "#optimum MAX_DEPTH\n",
    "lr_list = [1,2,3,4,5,8,10,13,18,21,34,55,89,100, 120,150,180,144,200,500]\n",
    "for find_optimum in lr_list:\n",
    "  KNN_optimum = KNeighborsClassifier(n_neighbors=find_optimum)\n",
    "  KNN_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, KNN_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bCwYGZ1mhpZ"
   },
   "outputs": [],
   "source": [
    "n_neighbors_use = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBOszGqAmkJy"
   },
   "outputs": [],
   "source": [
    "#optimum MAX_DEPTH\n",
    "lr_list = ['uniform', 'distance']\n",
    "for find_optimum in lr_list:\n",
    "  KNN_optimum = KNeighborsClassifier(n_neighbors=n_neighbors_use, weights = find_optimum)\n",
    "  KNN_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, KNN_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuWuaavzmqcO"
   },
   "outputs": [],
   "source": [
    "weights_use = 'distance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcio-DFemwzi"
   },
   "outputs": [],
   "source": [
    "#optimum MAX_DEPTH\n",
    "lr_list = [ 'ball_tree', 'kd_tree', 'brute']\n",
    "for find_optimum in lr_list:\n",
    "  KNN_optimum = KNeighborsClassifier(n_neighbors=n_neighbors_use, weights = weights_use, algorithm = find_optimum)\n",
    "  KNN_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, KNN_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lee3KU7ym2XU"
   },
   "outputs": [],
   "source": [
    "algorithm_use = 'ball_tree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQtSQTsSm5Ih"
   },
   "outputs": [],
   "source": [
    "#optimum LEAF_SIZE\n",
    "lr_list = [1,2,5,10,20,30,50,80,130,210,440]\n",
    "for find_optimum in lr_list:\n",
    "  KNN_optimum = KNeighborsClassifier(n_neighbors=n_neighbors_use, weights = weights_use,\n",
    "                                     algorithm = algorithm_use, leaf_size = find_optimum)\n",
    "  KNN_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, KNN_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W34sLvf1m4_J"
   },
   "outputs": [],
   "source": [
    "leaf_size_use = 210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlo0UgXAnJzd"
   },
   "outputs": [],
   "source": [
    "# Fit decision tree model\n",
    "KNN = KNeighborsClassifier(n_neighbors=n_neighbors_use, weights = weights_use,\n",
    "                                     algorithm = algorithm_use, leaf_size = leaf_size_use)\n",
    "KNN.fit(x_train, y_train)\n",
    "\n",
    "# Fit decision tree input-output\n",
    "model_KNN = KNN.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cA-JTt53njj8"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_KNN = model_KNN.predict(x_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_test_pred_KNN = model_KNN.predict(x_test)\n",
    "\n",
    "train_acc_KNN = accuracy_score(y_train, y_train_pred_KNN).round(3)\n",
    "test_acc_KNN = accuracy_score(y_test, y_test_pred_KNN).round(3)\n",
    "test_precision_KNN = precision_score(y_test, y_test_pred_KNN, average= 'weighted' ).round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "rmse_KNN =np.sqrt(mean_squared_error(y_test,y_test_pred_KNN)).round(3)\n",
    "f1_KNN = f1_score(y_test, y_test_pred_KNN, average= 'weighted' ).round(3)\n",
    "\n",
    "print('Target : ' + target_col)\n",
    "print(\"Training Accuracy:\", train_acc_KNN)\n",
    "print(\"Testing Accuracy:\", test_acc_KNN)\n",
    "print(\"Testing Precision:\", test_precision_KNN)\n",
    "print('RMSE : ', rmse_KNN)\n",
    "print('f1 score :', f1_KNN)\n",
    "print(classification_report(y_test, y_test_pred_KNN))\n",
    "\n",
    "conf_matrix_ML(y_test,y_test_pred_KNN, model_KNN, 'KNN Class. (Testing)')\n",
    "conf_matrix_ML(y_train,y_train_pred_KNN, model_KNN, 'KNN Class. (Training)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPhdSNLOoD4z"
   },
   "source": [
    "### SVM Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33625,
     "status": "ok",
     "timestamp": 1709132118693,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "SW8Q1CQWoGhQ",
    "outputId": "9fa95736-1a87-47e8-8adc-352ff8d46cf5"
   },
   "outputs": [],
   "source": [
    "#optimum MAX_DEPTH\n",
    "lr_list = [1,1.25,1.5,2,5,10,50,100,200,500,10000,50000]\n",
    "for find_optimum in lr_list:\n",
    "  SVC_optimum = SVC(C = find_optimum)\n",
    "  SVC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, SVC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJCU3Hioocmb"
   },
   "outputs": [],
   "source": [
    "C_use = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gO1Tke31ogYz"
   },
   "outputs": [],
   "source": [
    "#optimum MAX_DEPTH\n",
    "lr_list = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "for find_optimum in lr_list:\n",
    "  SVC_optimum = SVC(C = C_use, kernel = find_optimum)\n",
    "  SVC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, SVC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hP8RvDO_oops"
   },
   "outputs": [],
   "source": [
    "kernel_use = 'poly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4Fnv_dMomzo"
   },
   "outputs": [],
   "source": [
    "#optimum MAX_DEPTH\n",
    "lr_list = [ 1,2,3,4,5,6,7,8,9,10,20,50,100]\n",
    "for find_optimum in lr_list:\n",
    "  SVC_optimum = SVC(C = C_use, kernel = kernel_use, degree = find_optimum)\n",
    "  SVC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, SVC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHuspT8dowWX"
   },
   "outputs": [],
   "source": [
    "degree_use = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-VvtrM3o3v4"
   },
   "outputs": [],
   "source": [
    "#optimum MAX_DEPTH\n",
    "lr_list = ['scale','auto']\n",
    "for find_optimum in lr_list:\n",
    "  SVC_optimum = SVC(C = C_use, kernel = kernel_use, degree = degree_use, gamma = find_optimum)\n",
    "  SVC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, SVC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dmivx5Jqo-uz"
   },
   "outputs": [],
   "source": [
    "gamma_use = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URu-m7rEpO2k"
   },
   "outputs": [],
   "source": [
    "# Fit decision tree model\n",
    "SVC = SVC(C = C_use, kernel = kernel_use, degree = degree_use, gamma = gamma_use)\n",
    "SVC.fit(x_train, y_train)\n",
    "\n",
    "# Fit decision tree input-output\n",
    "model_SVC = SVC.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fO12kdyMpede"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_SVC = model_SVC.predict(x_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_test_pred_SVC = model_SVC.predict(x_test)\n",
    "\n",
    "train_acc_SVC = accuracy_score(y_train, y_train_pred_SVC).round(3)\n",
    "test_acc_SVC = accuracy_score(y_test, y_test_pred_SVC).round(3)\n",
    "test_precision_SVC = precision_score(y_test, y_test_pred_SVC, average= 'weighted' ).round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "rmse_SVC =np.sqrt(mean_squared_error(y_test,y_test_pred_SVC)).round(3)\n",
    "f1_SVC = f1_score(y_test, y_test_pred_SVC, average= 'weighted' ).round(3)\n",
    "\n",
    "print('Target : ' + target_col)\n",
    "print(\"Training Accuracy:\", train_acc_SVC)\n",
    "print(\"Testing Accuracy:\", test_acc_SVC)\n",
    "print(\"Testing Precision:\", test_precision_SVC)\n",
    "print('RMSE : ', rmse_SVC)\n",
    "print('f1 score :', f1_SVC)\n",
    "print(classification_report(y_test, y_test_pred_SVC))\n",
    "\n",
    "conf_matrix_ML(y_test,y_test_pred_SVC, model_SVC, 'Support Vector Class. (Testing)')\n",
    "conf_matrix_ML(y_train,y_train_pred_SVC, model_SVC, 'Support Vector Class. (Training)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0L9WYRiYsm1b"
   },
   "source": [
    "### Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cf1kry7qAqu"
   },
   "outputs": [],
   "source": [
    "#optimum LOSS_FUNCTION\n",
    "lr_list = ['squared_error', 'absolute_error', 'huber', 'quantile']\n",
    "# lr_list = ['log_loss', 'exponential']\n",
    "for find_optimum in lr_list:\n",
    "  GBC_optimum = GradientBoostingRegressor(loss=find_optimum, random_state=10)\n",
    "  GBC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, GBC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8KB8bLgqAqu"
   },
   "outputs": [],
   "source": [
    "loss_use = 'squared_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SKuzCrWqAqu"
   },
   "outputs": [],
   "source": [
    "#optimum LEARNING_RATE\n",
    "lr_list = [0.01,0.05,0.1,0.25,0.6,1]\n",
    "for find_optimum in lr_list:\n",
    "  GBC_optimum = GradientBoostingRegressor(loss=loss_use, learning_rate= find_optimum,  random_state=10)\n",
    "  GBC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, GBC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnGlpih-qAqu"
   },
   "outputs": [],
   "source": [
    "learning_rate_use = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cD4zwoICqAqu"
   },
   "outputs": [],
   "source": [
    "#optimum LEARNING_RATE\n",
    "lr_list = [2,5,10,25,50,75,100,200,500,1000,1500]\n",
    "for find_optimum in lr_list:\n",
    "  GBC_optimum = GradientBoostingClassifier(loss=loss_use, learning_rate= learning_rate_use , n_estimators= find_optimum,  random_state=10)\n",
    "  GBC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, GBC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vklJpSVsqAqu"
   },
   "outputs": [],
   "source": [
    "n_estimator_use = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSt2j45esm1j"
   },
   "outputs": [],
   "source": [
    "#optimum MAX_DEPTH\n",
    "lr_list = [1,2,3,4,5,8,10,13,21,34,42,55,89,100, 120,150,180,144,200,500,700]\n",
    "for find_optimum in lr_list:\n",
    "  RFC_optimum = GradientBoostingClassifier(max_depth=find_optimum, random_state=10)\n",
    "  RFC_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, RFC_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpomY3ULsm1k"
   },
   "outputs": [],
   "source": [
    "y_train_pred_GBC = model_GBC.predict(x_train)\n",
    "y_test_pred_GBC = model_GBC.predict(x_test)\n",
    "\n",
    "train_acc_GBC = accuracy_score(y_train, y_train_pred_GBC).round(3)\n",
    "test_acc_GBC = accuracy_score(y_test, y_test_pred_GBC).round(3)\n",
    "test_precision_GBC = precision_score(y_test, y_test_pred_GBC, average= 'weighted' ).round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "f1_GBC = f1_score(y_test,y_test_pred_GBC, average= 'weighted' ).round(3)\n",
    "rmse_GBC =np.sqrt(mean_squared_error(y_test,y_test_pred_GBC)).round(3)\n",
    "\n",
    "print('Target : ' + target_col)\n",
    "print(\"Training Accuracy:\", train_acc_GBC)\n",
    "print(\"Testing Accuracy:\", test_acc_GBC)\n",
    "print(\"Testing Precision:\", test_precision_GBC)\n",
    "print('f1 score :', f1_GBC)\n",
    "print('RMSE : ', rmse_GBC)\n",
    "print(classification_report(y_test, y_test_pred_GBC))\n",
    "\n",
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                \"GradBoost.Class\":model_GBC.feature_importances_.round(4),\n",
    "                                }).sort_values(by='features',ascending = False)\n",
    "data_importance\n",
    "\n",
    "conf_matrix_ML(y_test,y_test_pred_GBC, model_GBC, 'GradBoost Class. (Testing)')\n",
    "conf_matrix_ML(y_train,y_train_pred_GBC, model_GBC, 'GradBoost Class. (Training)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-MdlsD9qN0L"
   },
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rsgz5qcjqonu",
    "outputId": "dd3a4948-cd20-402f-e2ba-a5930db8e78c"
   },
   "outputs": [],
   "source": [
    "#optimum MAX_DEPTH\n",
    "lr_list = [1,2,3,4,5,8,10,13,21,34,42,55,89,100, 120,150,180,144,200,500,700]\n",
    "for find_optimum in lr_list:\n",
    "  XGB_optimum = xg.XGBClassifier( n_estimators=find_optimum, random_state= 10)\n",
    "  XGB_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, XGB_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ceVfflUat0Z4"
   },
   "outputs": [],
   "source": [
    "n_estimators_use = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZT9Lq5I6t3xI",
    "outputId": "862eabac-5abc-4f0f-a425-306f1cccd582"
   },
   "outputs": [],
   "source": [
    "#optimum MAX_DEPTH\n",
    "lr_list = [0.01,0.05,0.1,0.25,0.6,1]\n",
    "for find_optimum in lr_list:\n",
    "  XGB_optimum = xg.XGBClassifier( n_estimators=n_estimators_use, learning_rate = find_optimum, random_state= 10)\n",
    "  XGB_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, XGB_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHU4bj8zuFCN"
   },
   "outputs": [],
   "source": [
    "learning_rate_use = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vltr3u_Mt-SK",
    "outputId": "883955d6-8315-43db-b6b8-76e363b3e205"
   },
   "outputs": [],
   "source": [
    "#optimum MAX_DEPTH\n",
    "lr_list = [2,3,5,6,7,8,10,12,15,20,25,50,100,150,200]\n",
    "for find_optimum in lr_list:\n",
    "  XGB_optimum = xg.XGBClassifier( n_estimators=n_estimators_use, learning_rate = learning_rate_use,\n",
    "                                 max_depth=find_optimum, random_state= 10)\n",
    "  XGB_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, XGB_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqTBanwXuWX7"
   },
   "outputs": [],
   "source": [
    "max_depth_use = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0jIhPP5uUIp",
    "outputId": "60699e09-d980-436e-a6e1-d575d2700d36"
   },
   "outputs": [],
   "source": [
    "#optimum MAX_DEPTH\n",
    "lr_list = [1,2,5,10,50,100]\n",
    "for find_optimum in lr_list:\n",
    "  XGB_optimum = xg.XGBClassifier( n_estimators=n_estimators_use, learning_rate = learning_rate_use,\n",
    "                                 max_depth=max_depth_use, random_state= find_optimum)\n",
    "  XGB_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, XGB_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fob7W9OIudht"
   },
   "outputs": [],
   "source": [
    "random_state_use = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxscklkAunjO"
   },
   "outputs": [],
   "source": [
    "# Fit decision tree model\n",
    "XGBC = xg.XGBClassifier( n_estimators=n_estimators_use, learning_rate = learning_rate_use,\n",
    "                                 max_depth=max_depth_use, random_state= find_optimum)\n",
    "XGBC.fit(x_train, y_train)\n",
    "\n",
    "model_XGBC = XGBC.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YSC7dHitXPK"
   },
   "outputs": [],
   "source": [
    "y_train_pred_XGBC = model_XGBC.predict(x_train)\n",
    "y_test_pred_XGBC = model_XGBC.predict(x_test)\n",
    "\n",
    "train_acc_XGBC = accuracy_score(y_train, y_train_pred_XGBC).round(3)\n",
    "test_acc_XGBC = accuracy_score(y_test, y_test_pred_XGBC).round(3)\n",
    "test_precision_XGBC = precision_score(y_test, y_test_pred_XGBC, average= 'weighted' ).round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "f1_XGBC = f1_score(y_test, y_test_pred_XGBC, average= 'weighted' ).round(3)\n",
    "rmse_XGBC =np.sqrt(mean_squared_error(y_test,y_test_pred_XGBC)).round(3)\n",
    "\n",
    "print('Target : ' + target_col)\n",
    "print(\"Training Accuracy:\", train_acc_XGBC)\n",
    "print(\"Testing Accuracy:\", test_acc_XGBC)\n",
    "print(\"Testing Precision:\", test_precision_XGBC)\n",
    "print('f1 score :', f1_XGBC)\n",
    "print('RMSE : ', rmse_XGBC)\n",
    "print(classification_report(y_test, y_test_pred_XGBC))\n",
    "\n",
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                \"model_XGBC\":model_XGBC.feature_importances_.round(4),\n",
    "                                })#.sort_values(ascending = False)\n",
    "data_importance\n",
    "\n",
    "conf_matrix_ML(y_test,y_test_pred_XGBC, model_XGBC, 'XGBoost Class. (Testing)')\n",
    "conf_matrix_ML(y_train,y_train_pred_XGBC, model_XGBC, 'XGBoost Class. (Training)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDfhIOnEwThb",
    "outputId": "9ec73472-2368-40f6-8a6c-e39ff4284b70"
   },
   "outputs": [],
   "source": [
    "#optimum HIDDEN LAYER\n",
    "lr_list = [6,12,24,50,75,100,150,200,300,400,600,800,1200]\n",
    "lr_list2 = [6,12,24,50,75,100,200,300,400]\n",
    "for find_optimum1  in lr_list :\n",
    "  for find_optimum2 in lr_list2 :\n",
    "    MLP_optimum = MLPClassifier(hidden_layer_sizes=(find_optimum, find_optimum2),max_iter=500)\n",
    "    MLP_optimum.fit(x_train, y_train)\n",
    "    ML_NN_optimization_process (find_optimum, find_optimum2, MLP_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_INU48Dw_1x"
   },
   "outputs": [],
   "source": [
    "hidden_layer_sizes_use = (236, 136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qFSTn7lw5at"
   },
   "outputs": [],
   "source": [
    "#optimum ACTIVATION FUNCTION\n",
    "lr_list = ['relu',\n",
    "            'logistic', 'tanh',\n",
    "           'identity'#, 'softmax'\n",
    "           ]\n",
    "for find_optimum in lr_list:\n",
    "  MLP_optimum = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes_use, activation= find_optimum, max_iter=500)\n",
    "  MLP_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, MLP_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uM1hdkVdxFKJ"
   },
   "outputs": [],
   "source": [
    "activation_use = 'tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VTCrPrSxH8v"
   },
   "outputs": [],
   "source": [
    "#optimum SOLVER\n",
    "lr_list = ['adam', 'sgd','lbfgs']\n",
    "for find_optimum in lr_list:\n",
    "  MLP_optimum = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes_use, solver= find_optimum,\n",
    "                              activation= activation_use, max_iter=500)\n",
    "  MLP_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, MLP_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zi3SHYBwxkCk"
   },
   "outputs": [],
   "source": [
    "solver_use = 'lbfgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNhpAkUaxjdC"
   },
   "outputs": [],
   "source": [
    "#optimum LEARNING RATE\n",
    "lr_list = ['constant','invscaling','adaptive']\n",
    "for find_optimum in lr_list:\n",
    "  MLP_optimum = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes_use, solver= solver_use,\n",
    "                              activation= activation_use, learning_rate= find_optimum, max_iter=500)\n",
    "  MLP_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, MLP_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBOW8BNtx_Gg"
   },
   "outputs": [],
   "source": [
    "learning_rate_use = 'invscaling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKE-bIkyyCHS"
   },
   "outputs": [],
   "source": [
    "#optimum ALPHA\n",
    "lr_list = [ 0.01,0.1]\n",
    "for find_optimum in lr_list:\n",
    "  MLP_optimum = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes_use, solver= solver_use,\n",
    "                              activation= activation_use, learning_rate= learning_rate_use, alpha=find_optimum, max_iter=500)\n",
    "  MLP_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, MLP_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBtxdFUOyNDv"
   },
   "outputs": [],
   "source": [
    "alpha_use = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30BhLXTLy8mW"
   },
   "outputs": [],
   "source": [
    "MLPC = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes_use, solver= solver_use,\n",
    "                              activation= activation_use, learning_rate= learning_rate_use, alpha=alpha_use, max_iter=1000)\n",
    "model_MLPC = MLPC.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUR506nJzFfw"
   },
   "outputs": [],
   "source": [
    "y_train_pred_MLPC = model_MLPC.predict(x_train)\n",
    "y_test_pred_MLPC = model_MLPC.predict(x_test)\n",
    "train_acc_MLPC = accuracy_score(y_train, y_train_pred_MLPC).round(3)\n",
    "test_acc_MLPC = accuracy_score(y_test, y_test_pred_MLPC).round(3)\n",
    "test_precision_MLPC = precision_score(y_test, y_test_pred_MLPC, average= 'weighted' ).round(3) #average : {'micro', 'macro', 'samples', 'weighted', 'binary'\n",
    "rmse_MLPC =np.sqrt(mean_squared_error(y_test,y_test_pred_MLPC)).round(3)\n",
    "f1_MLPC = f1_score(y_test, y_test_pred_MLPC, average= 'weighted' ).round(3)\n",
    "\n",
    "print('Target : ' + target_col)\n",
    "print(\"Training Accuracy:\", train_acc_MLPC)\n",
    "print(\"Testing Accuracy:\", test_acc_MLPC)\n",
    "print(\"Testing Precision:\", test_precision_MLPC)\n",
    "print('RMSE : ', rmse_MLPC)\n",
    "print('f1 score :', f1_MLPC)\n",
    "print(classification_report(y_test, y_test_pred_MLPC))\n",
    "\n",
    "conf_matrix_ML(y_test,y_test_pred_MLPC, model_MLPC, 'MLPC Class. (Testing)')\n",
    "conf_matrix_ML(y_train,y_train_pred_MLPC, model_MLPC, 'MLPC Class. (Training)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lO3o57NZxyLn"
   },
   "outputs": [],
   "source": [
    "# Extract coefs_ and intercepts_\n",
    "coefs_show = model_MLPC.coefs_\n",
    "intercepts_show = model_MLPC.intercepts_\n",
    "\n",
    "# Create DataFrame for coefficients\n",
    "coef_df_list = []\n",
    "for i, coef_matrix in enumerate(coefs_show):\n",
    "    layer_df = pd.DataFrame(coef_matrix, columns=[f'Layer_{i+1}_Neuron_{j+1}' for j in range(coef_matrix.shape[1])])\n",
    "    layer_df['Input_Variable'] = xb.columns if i == 0 else [f'Layer_{i}_Neuron_{j+1}' for j in range(coefs_show[i-1].shape[1])]\n",
    "    coef_df_list.append(layer_df.set_index('Input_Variable'))\n",
    "\n",
    "# Combine coefficients into a single DataFrame\n",
    "MLPC_coef_df = pd.concat(coef_df_list, axis=1)\n",
    "\n",
    "# Create DataFrame for intercepts\n",
    "intercept_df_list = []\n",
    "for i, intercept_vector in enumerate(intercepts_show):\n",
    "    intercept_df = pd.DataFrame(intercept_vector, columns=[f'Intercept_Layer_{i+1}'])\n",
    "    intercept_df['Neuron'] = [f'Layer_{i+1}_Neuron_{j+1}' for j in range(len(intercept_vector))]\n",
    "    intercept_df_list.append(intercept_df.set_index('Neuron'))\n",
    "\n",
    "# Combine intercepts into a single DataFrame\n",
    "MLPC_intercept_df = pd.concat(intercept_df_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_klRBfzYUaW"
   },
   "source": [
    "### Classification ML - Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ZAEb1zPjiMg"
   },
   "outputs": [],
   "source": [
    "data = np.array([['Algorithm', 'Train Acc', 'Test ACC', 'Test Precision', 'f1_score', 'RMSE'],\n",
    "                ['Log.Regression', train_acc_logreg, test_acc_logreg, test_precision_logreg, f1_logreg, rmse_logreg],\n",
    "                ['Decision Tree', train_acc_DTC, test_acc_DTC, test_precision_DTC, f1_DTC,  rmse_DTC],\n",
    "                ['Extra Trees', train_acc_ETC, test_acc_ETC, test_precision_ETC, f1_ETC,  rmse_ETC],\n",
    "                #['Gaussian Process', train_acc_GPC, test_acc_GPC, test_precision_GPC, f1_GPC, rmse_GPC],\n",
    "                #['K-Nearest N', train_acc_KNN, test_acc_KNN, test_precision_KNN, f1_KNN,  rmse_KNN],\n",
    "                #['Support Vector', train_acc_SVC, test_acc_SVC, test_precision_SVC, f1_SVC,  rmse_SVC],\n",
    "                ['Random Forest', train_acc_RFC, test_acc_RFC, test_precision_RFC, f1_RFC,  rmse_RFC],\n",
    "                # ['XGBoost', train_acc_XGBC, test_acc_XGBC, test_precision_XGBC,   f1_XGBC,   rmse_XGBC],\n",
    "                # ['GradBoost', train_acc_GBC, test_acc_GBC, test_precision_GBC,   f1_GBC,   rmse_GBC],\n",
    "                # ['MultiLayer NN', train_acc_MLPC, test_acc_MLPC, test_precision_MLPC, f1_MLPC,  rmse_MLPC],\n",
    "                 ])\n",
    "\n",
    "table_classification_performance = pd.DataFrame(data=data[1:, 1:],\n",
    "                     index = data[1:,0],\n",
    "                     columns=(data[0,1:])).sort_values('Test Precision', ascending=False)\n",
    "print('Target : ' + target_col)\n",
    "table_classification_performance      #.drop(['Test Precision'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWidHrTfHFXS"
   },
   "outputs": [],
   "source": [
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "\n",
    "                                #'Support Vector' : model_SVC.coef_.flatten().round(4),\n",
    "                                 \"Dec.Tree.Class\":model_DTC.feature_importances_.round(4),\n",
    "                                 \"Extra.Trees.Class\":model_ETC.feature_importances_.round(4),\n",
    "                                 \"RanFor.Class\" : model_RFC.feature_importances_.round(4),\n",
    "                                #\"GradBoost.Class\": model_GBC.feature_importances_.round(4),\n",
    "                                # \"XGBoost.Class\": model_XGBC.feature_importances_.round(4),\n",
    " #                               \"AdaBoost.Class\": model_ADABC.feature_importances_ .round(4)\n",
    "                                #  'Log.Regress' : logreg.coef_.flatten().round(4),\n",
    "                                }).sort_values('Extra.Trees.Class', ascending = False)\n",
    "print('Target : ' + target_col)\n",
    "data_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STZPJ_QLxyLn"
   },
   "outputs": [],
   "source": [
    "train_result = pd.DataFrame({\"y_train\" : y_train,\n",
    "                             \"Linear Class.\":y_train_pred_logreg.flatten(),\n",
    "                                # \"SV Class.\":y_train_pred_SVC.flatten(),\n",
    "                                \"Dec.Tree Class.\" : y_train_pred_DTC.flatten(),\n",
    "                                \"RandForest Class.\": y_train_pred_RFC.flatten(),\n",
    "                                \"Extra Trees Class.\": y_train_pred_ETC.flatten(),\n",
    "                                # \"GradBoost Class.\":y_train_pred_GBC.flatten(),\n",
    "                                # \"XGBoost Class.\":y_train_pred_XGBC.flatten(),\n",
    "                                \"MLP Class.\": y_train_pred_MLPC.flatten()\n",
    "                                }\n",
    "                               )\n",
    "\n",
    "test_result = pd.DataFrame({\"y_test\" : y_test,\n",
    "                            \"Linear Class.\":y_test_pred_logreg.flatten(),\n",
    "                            # \"SV Class.\":y_test_pred_SVC.flatten(),\n",
    "                            \"Dec.Tree Class.\" : y_test_pred_DTC.flatten(),\n",
    "                            \"RandForest Class.\": y_test_pred_RFC.flatten(),\n",
    "                            \"Extra Trees Class.\": y_test_pred_ETC.flatten(),\n",
    "                            # \"GradBoost Reg.\":y_test_pred_GBC.flatten(),\n",
    "                            # \"XGBoost Reg.\":y_test_pred_XGBC.flatten(),\n",
    "                            \"MLP Class.\": y_test_pred_MLPC.flatten()\n",
    "                                }\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWIIrUQVQqcV"
   },
   "outputs": [],
   "source": [
    "def features_plotter (df):\n",
    "    df.plot(kind='barh', stacked=False, figsize=(6, 4))\n",
    "    # Customize labels and title\n",
    "    plt.ylabel('measured IAQ parameters')\n",
    "    plt.xlabel('(Cummulative) feature of importances')\n",
    "    plt.title('ML Result : importances of features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKMQnmzdQqcV"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "data = {\n",
    "    'temp_a': [0.1384, 0.1644, None],\n",
    "    'rh_a': [None, 0.1176, None],\n",
    "    'co2_a': [0.1563, 0.1174, None],\n",
    "    'voc_a': [0.1844, 0.1007, None],\n",
    "    'enth_a': [0.1515, 0.1349, None],\n",
    "    'hura_a': [0.189, 0.1, None],\n",
    "    'temp_o': [0.0918, 0.1032, None],\n",
    "    'rh_o': [0.0614, None, None],\n",
    "    'enth_o': [None, 0.1007, None],\n",
    "    'hour (time)': [0.0272, 0.0611, None]}\n",
    "data_importance = pd.DataFrame(data, index = [ 'feel_temp', 'feel_air', 'None']).T.drop(columns='None',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmIc8w45QqcV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQ2fXXbEQqcV"
   },
   "outputs": [],
   "source": [
    "pareto_histograph(data_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXoSNciyu9-_"
   },
   "source": [
    "## 2A. Train Regression ML *GridCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vvmy3X7r9K6K"
   },
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1711381366194,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "-Mt_A8vS9K6S",
    "outputId": "a787ea67-da71-45d9-e078-964fe2a281cf"
   },
   "outputs": [],
   "source": [
    "Lin_reg = LinearRegression()\n",
    "Lin_reg.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred_LinReg = Lin_reg.predict(x_train)\n",
    "y_test_pred_LinReg = Lin_reg.predict(x_test)\n",
    "print(Lin_reg.intercept_)\n",
    "print(Lin_reg.coef_)\n",
    "print(Lin_reg.score(x_train, y_train))\n",
    "print(Lin_reg.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 326,
     "status": "ok",
     "timestamp": 1711381369869,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "Otbxb5YY9K6S",
    "outputId": "b038180c-32f6-4676-a8ee-55c92101071b"
   },
   "outputs": [],
   "source": [
    "r2_LinReg = r2_score(y_test,y_test_pred_LinReg).round(3)\n",
    "mse_LinReg=mean_squared_error(y_test,y_test_pred_LinReg).round(3)\n",
    "mae_LinReg=mean_absolute_error(y_test,y_test_pred_LinReg).round(3)\n",
    "rmse_LinReg=np.sqrt(mean_squared_error(y_test,y_test_pred_LinReg)).round(3)\n",
    "CV_value_LinReg= explained_variance_score(y_test,y_test_pred_LinReg).round(3)\n",
    "print('Linear Regression Performance')\n",
    "print('R2 : ', r2_LinReg)\n",
    "print('MSE : ', mse_LinReg)\n",
    "print('MAE : ', mae_LinReg)\n",
    "print('RMSE : ', rmse_LinReg)\n",
    "print('CV : ', CV_value_LinReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UHpYEOJxyLn",
    "outputId": "5b83d129-32c7-44d1-83f7-db4b5473bb7b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the base models and the meta-model\n",
    "base_models = [\n",
    "    ('decision_tree', DecisionTreeRegressor(random_state=42)),\n",
    "    ('linear_regression', RandomForestRegressor())\n",
    "]\n",
    "\n",
    "meta_model = RandomForestRegressor()\n",
    "\n",
    "# Create and train the stacking regressor\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5  # Use 5-fold cross-validation\n",
    ")\n",
    "\n",
    "stacking_regressor.fit(x_train, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = stacking_regressor.predict(x_test)\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjH6ZfEY9K6S"
   },
   "source": [
    "### Lasso Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1711381376554,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "mOQrzQNC9K6T",
    "outputId": "af3f33f4-0e83-4eb9-bd52-1654bfec479d"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "#Lasso = Lasso(alpha=0.2, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
    "#Lasso = Lasso(alpha=3, fit_intercept=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
    "Lasso = Lasso(alpha=0.1, max_iter=1000, selection= 'random' )\n",
    "Lasso.fit(x_train, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1711381378521,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "2v-TTems9K6T",
    "outputId": "8de620d7-4566-4fe1-d604-8b15c5f958e8"
   },
   "outputs": [],
   "source": [
    "y_train_pred_Lasso = Lasso.predict(x_train)\n",
    "y_test_pred_Lasso = Lasso.predict(x_test)\n",
    "print(Lasso.intercept_)\n",
    "print(Lasso.coef_)\n",
    "print(Lasso.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1678091789118,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "yurmHksW9K6T",
    "outputId": "98126a78-ea0d-4c4c-832f-554548347bbc"
   },
   "outputs": [],
   "source": [
    "r2_Lasso = r2_score(y_test,y_test_pred_Lasso).round(3)\n",
    "mse_Lasso=mean_squared_error(y_test,y_test_pred_Lasso).round(3)\n",
    "mae_Lasso=mean_absolute_error(y_test,y_test_pred_Lasso).round(3)\n",
    "rmse_Lasso=np.sqrt(mean_squared_error(y_test,y_test_pred_Lasso)).round(3)\n",
    "CV_value_Lasso= explained_variance_score(y_test,y_test_pred_Lasso).round(3)\n",
    "print('Lasso Regression Performance')\n",
    "print('R2 : ', r2_Lasso)\n",
    "print('MSE : ', mse_Lasso)\n",
    "print('MAE : ', mae_Lasso)\n",
    "print('RMSE : ', rmse_Lasso)\n",
    "print('CV : ', CV_value_Lasso)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oK_XCaB9K6T"
   },
   "source": [
    "### Ridge Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1711381389492,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "-aJ4QHJJ9K6T",
    "outputId": "1ef98343-5781-456d-b05f-e0bc7e78c2ce"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "Ridge = Ridge()\n",
    "Ridge.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1711381390044,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "_PDYFM-g9K6T",
    "outputId": "b8c66ddc-3207-4fdf-83b6-4ada3c2f24b4"
   },
   "outputs": [],
   "source": [
    "y_train_pred_Ridge = Ridge.predict(x_train)\n",
    "y_test_pred_Ridge = Ridge.predict(x_test)\n",
    "print(Ridge.intercept_)\n",
    "print(Ridge.coef_)\n",
    "print(Ridge.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1711381394031,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "7B3JKos99K6U",
    "outputId": "7fa44c5d-1805-40e0-d319-699cd2db9363"
   },
   "outputs": [],
   "source": [
    "r2_Ridge = r2_score(y_test,y_test_pred_Ridge).round(3)\n",
    "mse_Ridge=mean_squared_error(y_test,y_test_pred_Ridge).round(3)\n",
    "mae_Ridge=mean_absolute_error(y_test,y_test_pred_Ridge).round(3)\n",
    "rmse_Ridge=np.sqrt(mean_squared_error(y_test,y_test_pred_Ridge)).round(3)\n",
    "CV_value_Ridge= explained_variance_score(y_test,y_test_pred_Ridge).round(3)\n",
    "print('Ridge Regression Performance')\n",
    "print('R2 : ', r2_Ridge)\n",
    "print('MSE : ', mse_Ridge)\n",
    "print('MAE : ', mae_Ridge)\n",
    "print('RMSE : ', rmse_Ridge)\n",
    "print('CV : ', CV_value_Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvPSzeliFA6u"
   },
   "source": [
    "### SGD Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvjEKlTuFCvs"
   },
   "outputs": [],
   "source": [
    "model = SGDRegressor( max_iter=1000 , validation_fraction = 0.2, early_stopping=True, tol=0.001  )\n",
    "param_grid = {\n",
    "    'loss'    : ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "    'penalty' : [ 'l2', 'l1', 'elasticnet'  ],\n",
    "    'alpha'   : [0.0001, 0.0005, 0.001, 0.005]\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=6, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train )\n",
    "\n",
    "model_SGR = grid_search.best_estimator_\n",
    "ML_grid_result (grid_search, model_SGR, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DAaEFOUGR9h"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_SGR = model_SGR.predict(x_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_test_pred_SGR = model_SGR.predict(x_test)\n",
    "\n",
    "r2_SGR = r2_score(y_test,y_test_pred_SGR).round(3)\n",
    "mse_SGR=mean_squared_error(y_test,y_test_pred_SGR).round(3)\n",
    "mae_SGR=mean_absolute_error(y_test,y_test_pred_SGR).round(3)\n",
    "rmse_SGR=np.sqrt(mean_squared_error(y_test,y_test_pred_SGR)).round(3)\n",
    "CV_value_SGR= explained_variance_score(y_test,y_test_pred_SGR).round(3)\n",
    "print('Stoc.Gauss.D Reg. Performance')\n",
    "print('target :', target_col )\n",
    "print('R2 : ', r2_SGR)\n",
    "print('MSE : ', mse_SGR)\n",
    "print('MAE : ', mae_SGR)\n",
    "print('RMSE : ', rmse_SGR)\n",
    "print('CV : ', CV_value_SGR)\n",
    "\n",
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                \"Stoc.Gauss.D Reg.\":model_SGR.feature_importances_.round(4),\n",
    "                                }).sort_values( by=\"Stoc.Gauss.D Reg.\" ,ascending = False)\n",
    "data_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVI1OJaku9_A"
   },
   "source": [
    "### Decision Tree Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56397,
     "status": "ok",
     "timestamp": 1711380701860,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "iak_Nb0du9_A",
    "outputId": "b828e16f-9092-4dee-eba8-a6016afbca88"
   },
   "outputs": [],
   "source": [
    "model =DecisionTreeRegressor(   )\n",
    "param_grid = {\n",
    "    'max_depth'    : [1,2,3,4,5,8,10,13,21,34,42,55,89,100, 120,150,180,144,200,500,700],\n",
    "    'criterion'    : ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "    'splitter'     : ['best', 'random'],\n",
    "    'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "    'random_state' : [1,2,5,10,50,100,200]\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=6, scoring='neg_root_mean_squared_error' #, n_jobs=-1\n",
    "                           )\n",
    "grid_search.fit(x_train, y_train )\n",
    "\n",
    "model_DTR = grid_search.best_estimator_\n",
    "ML_grid_result (grid_search, model_DTR, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1711375644254,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "rgkqg5Vyu9_A",
    "outputId": "4fea9ca1-6148-435f-8270-a0964942fe6d"
   },
   "outputs": [],
   "source": [
    "y_train_pred_DTR = model_DTR.predict(x_train)\n",
    "y_test_pred_DTR = model_DTR.predict(x_test)\n",
    "y_pred_DTR = (model_DTR.predict(x)).reshape(-1,1)\n",
    "r2_DTR = r2_score(y_test,y_test_pred_DTR).round(3)\n",
    "mse_DTR=mean_squared_error(y_test,y_test_pred_DTR).round(3)\n",
    "mae_DTR=mean_absolute_error(y_test,y_test_pred_DTR).round(3)\n",
    "rmse_DTR=np.sqrt(mean_squared_error(y_test,y_test_pred_DTR)).round(3)\n",
    "CV_value_DTR= (rmse_DTR/np.array(y_test).flatten().mean(axis=0)).round(3)\n",
    "\n",
    "train_r2_DTR = r2_score(y_train,y_train_pred_DTR).round(3)\n",
    "train_mse_DTR=mean_squared_error(y_train,y_train_pred_DTR).round(3)\n",
    "train_mae_DTR=mean_absolute_error(y_train,y_train_pred_DTR).round(3)\n",
    "train_rmse_DTR=np.sqrt(mean_squared_error(y_train,y_train_pred_DTR)).round(3)\n",
    "train_CV_value_DTR= (train_rmse_DTR/np.array(y_train).flatten().mean(axis=0)).round(3)\n",
    "\n",
    "print('Decision Tree Regression Performance')\n",
    "print('target :', target_col )\n",
    "print('R2 : ', r2_DTR)\n",
    "print('MSE : ', mse_DTR)\n",
    "print('MAE : ', mae_DTR)\n",
    "print('RMSE : ', rmse_DTR)\n",
    "print('CV : ', CV_value_DTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbO0zH29u9_B"
   },
   "outputs": [],
   "source": [
    "# fn = model_DTR.feature_names_in_\n",
    "# cn= model_DTR.feature_names_in_\n",
    "# figsize = (50,10)\n",
    "\n",
    "# fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = figsize, dpi=1200)\n",
    "# for index in range(0, 1):\n",
    "#     tree.plot_tree(model_DTR,\n",
    "#                    feature_names = fn,\n",
    "#                    class_names=target_col,\n",
    "#                    filled = True,\n",
    "#                    ax = axes);\n",
    "\n",
    "#     # axes.set_title('Estimator: ' + str(index), fontsize = 11)\n",
    "# fig.savefig(directory_path + '/'+ folder_ML + '/' + 'model_DTR_' + str(target_col) + '.pdf', format= 'pdf')\n",
    "# fig.savefig(directory_path + '/'+ folder_ML + '/' + 'model_DTR_' + str(target_col) + '.png', format= 'png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYVPpnuMu9_B"
   },
   "source": [
    "### Extra Trees Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkIF2Iinu9_B",
    "outputId": "d3fc43d9-e314-40ff-fb38-0cf1ab4d4e90"
   },
   "outputs": [],
   "source": [
    "model =ExtraTreesRegressor( )\n",
    "param_grid = {\n",
    "    'n_estimators'  : [10,50,100,200], #,300,500\n",
    "    # 'criterion'    : ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "    'max_depth'    : [1,2,3,4,5,8,10,13,21,34], #,42,55,89,100, 120,150,180,144,200,500,700\n",
    "    # 'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "    # 'random_state' : [1,2,5,10,100]\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=6, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train )\n",
    "\n",
    "model_ETR = grid_search.best_estimator_\n",
    "ML_grid_result (grid_search, model_ETR, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNPLp2owu9_B",
    "outputId": "ee3912b8-f6a2-446c-8637-a95c008b60b9"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_ETR = model_ETR.predict(x_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_test_pred_ETR = model_ETR.predict(x_test)\n",
    "\n",
    "r2_ETR = r2_score(y_test,y_test_pred_ETR).round(3)\n",
    "mse_ETR=mean_squared_error(y_test,y_test_pred_ETR).round(3)\n",
    "mae_ETR=mean_absolute_error(y_test,y_test_pred_ETR).round(3)\n",
    "rmse_ETR=np.sqrt(mean_squared_error(y_test,y_test_pred_ETR)).round(3)\n",
    "CV_value_ETR= explained_variance_score(y_test,y_test_pred_ETR).round(3)\n",
    "print('Extra Trees Regression Performance')\n",
    "print('target :', target_col )\n",
    "print('R2 : ', r2_ETR)\n",
    "print('MSE : ', mse_ETR)\n",
    "print('MAE : ', mae_ETR)\n",
    "print('RMSE : ', rmse_ETR)\n",
    "print('CV : ', CV_value_ETR)\n",
    "\n",
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                \"Extra.Trees.Class\":model_ETR.feature_importances_.round(4),\n",
    "                                }).sort_values( by=\"Extra.Trees.Class\" ,ascending = False)\n",
    "data_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgZUOF_Hu9_B"
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(25, 12))\n",
    "#plot_tree(ETR)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4x01_u04u9_C"
   },
   "source": [
    "### Gaussian Process Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msYQHhrnu9_C"
   },
   "outputs": [],
   "source": [
    "# model = GaussianProcessRegressor( )\n",
    "\n",
    "# param_grid = {\n",
    "    # 'kernel'    : [1.0 * RBF(1.0)]\n",
    "    # ,'n_restarts_optimizer' : [0,1,2,3,4,10]\n",
    "    # ,'max_iter_predict'   : [100]\n",
    "    # ,'random_state'       : [1,2,5,10,20,30,50,80,130,210,440]\n",
    "# }\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "# grid_search.fit(x_train, y_train )\n",
    "\n",
    "# model_GPR = grid_search.best_estimator_\n",
    "# ML_grid_result (grid_search, model_GPR, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6w0M7qxu9_C"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_GPR = model_GPR.predict(x_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_test_pred_GPR = model_GPR.predict(x_test)\n",
    "\n",
    "r2_GPR = r2_score(y_test,y_test_pred_GPR).round(3)\n",
    "mse_GPR=mean_squared_error(y_test,y_test_pred_GPR).round(3)\n",
    "mae_GPR=mean_absolute_error(y_test,y_test_pred_GPR).round(3)\n",
    "rmse_GPR=np.sqrt(mean_squared_error(y_test,y_test_pred_GPR)).round(3)\n",
    "CV_value_GPR= explained_variance_score(y_test,y_test_pred_GPR).round(3)\n",
    "print('Gaussian Process Regression Performance')\n",
    "print('target :', target_col )\n",
    "print('R2 : ', r2_GPR)\n",
    "print('MSE : ', mse_GPR)\n",
    "print('MAE : ', mae_GPR)\n",
    "print('RMSE : ', rmse_GPR)\n",
    "print('CV : ', CV_value_GPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44Splvsqu9_C"
   },
   "source": [
    "### KNN Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244349,
     "status": "ok",
     "timestamp": 1709104364380,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "1UV8ALqXu9_C",
    "outputId": "582512d2-ef1c-46ce-90c4-afe3031ab5e9"
   },
   "outputs": [],
   "source": [
    "model = KNeighborsRegressor(   )\n",
    "param_grid = {\n",
    "    'n_neighbors'    : [1,2,3,4,5,8,10,13,21,34,55,89,100, 120,150,180,144,200,500]\n",
    "    ,'weights'  : ['uniform', 'distance']\n",
    "    ,'algorithm'   : [ 'ball_tree', 'kd_tree', 'brute']\n",
    "    ,'leaf_size' : [10,20,30,50,80,130,210,440]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train )\n",
    "\n",
    "model_KNR = grid_search.best_estimator_\n",
    "ML_grid_result (grid_search, model_KNR, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzGW9R-uu9_D"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_KNR = model_KNR.predict(x_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_test_pred_KNR = model_KNR.predict(x_test)\n",
    "\n",
    "r2_KNR  = r2_score(y_test,y_test_pred_KNR ).round(3)\n",
    "mse_KNR =mean_squared_error(y_test,y_test_pred_KNR ).round(3)\n",
    "mae_KNR =mean_absolute_error(y_test,y_test_pred_KNR ).round(3)\n",
    "rmse_KNR =np.sqrt(mean_squared_error(y_test,y_test_pred_KNR )).round(3)\n",
    "CV_value_KNR = explained_variance_score(y_test,y_test_pred_KNR ).round(3)\n",
    "print('KNN Regression Performance')\n",
    "print('target :', target_col )\n",
    "print('R2 : ', r2_KNR )\n",
    "print('MSE : ', mse_KNR )\n",
    "print('MAE : ', mae_KNR )\n",
    "print('RMSE : ', rmse_KNR )\n",
    "print('CV : ', CV_value_KNR )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XesRU3RHu9_D"
   },
   "source": [
    "### SVM Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTGkVgFyu9_D"
   },
   "outputs": [],
   "source": [
    "model =SVR(   )\n",
    "param_grid = {\n",
    "    'C'        : [1,1.1,1.25,1.5],\n",
    "    'kernel'   : ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree'   : [ 1,2,3,4,5,6,7,8,9,10],\n",
    "    'gamma'    :  ['scale','auto'],\n",
    "    'random_state' : [1,2,5,10]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train )\n",
    "\n",
    "model_SVR = grid_search.best_estimator_\n",
    "ML_grid_result (grid_search, model_SVR, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gi7MEMLau9_D"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_SVR = model_SVR.predict(x_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_test_pred_SVR = model_SVR.predict(x_test)\n",
    "\n",
    "r2_SVR = r2_score(y_test,y_test_pred_SVR).round(3)\n",
    "mse_SVR=mean_squared_error(y_test,y_test_pred_SVR).round(3)\n",
    "mae_SVR=mean_absolute_error(y_test,y_test_pred_SVR).round(3)\n",
    "rmse_SVR=np.sqrt(mean_squared_error(y_test,y_test_pred_SVR)).round(3)\n",
    "CV_value_SVR= explained_variance_score(y_test,y_test_pred_SVR).round(3)\n",
    "print('Extra Trees Regression Performance')\n",
    "print('target :', target_col )\n",
    "print('R2 : ', r2_SVR)\n",
    "print('MSE : ', mse_SVR)\n",
    "print('MAE : ', mae_SVR)\n",
    "print('RMSE : ', rmse_SVR)\n",
    "print('CV : ', CV_value_SVR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_8py2sQu9_D"
   },
   "source": [
    "### Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0MY4biJu9_D"
   },
   "outputs": [],
   "source": [
    "model =RandomForestRegressor()\n",
    "param_grid = {\n",
    "    'criterion'    : ['squared_error','absolute_error','friedman_mse', 'poisson'],\n",
    "    'max_depth'    : [1,2,3,4,5,8,10,13,21,34,55,89,100, 120,150,180,144,200,500],\n",
    "    'random_state' : [1,2,5,10,50,100],\n",
    "    'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "    'n_estimators'  : [10,50,100,200,500]\n",
    "    }\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train )\n",
    "model_RFR = grid_search.best_estimator_\n",
    "ML_grid_result (grid_search, model_RFR, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akTkqIiPu9_E"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_RFR = model_RFR.predict(x_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_test_pred_RFR = model_RFR.predict(x_test)\n",
    "\n",
    "r2_RFR = r2_score(y_test,y_test_pred_RFR).round(3)\n",
    "mse_RFR=mean_squared_error(y_test,y_test_pred_RFR).round(3)\n",
    "mae_RFR=mean_absolute_error(y_test,y_test_pred_RFR).round(3)\n",
    "rmse_RFR=np.sqrt(mean_squared_error(y_test,y_test_pred_RFR)).round(3)\n",
    "CV_value_RFR= explained_variance_score(y_test,y_test_pred_RFR).round(3)\n",
    "print('Decision Tree Regression Performance')\n",
    "print('target :', target_col )\n",
    "print('R2 : ', r2_RFR)\n",
    "print('MSE : ', mse_RFR)\n",
    "print('MAE : ', mae_RFR)\n",
    "print('RMSE : ', rmse_RFR)\n",
    "print('CV : ', CV_value_RFR)\n",
    "\n",
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                \"Ran.For.Class\":model_RFR.feature_importances_.round(4),\n",
    "                                }).sort_values( by=\"Ran.For.Class\" ,ascending = False)\n",
    "data_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jfymy0Pgu9_E"
   },
   "outputs": [],
   "source": [
    "conf_matrix_ML(y_test,y_test_pred_RFC, model_RFC, 'Random Forest (Testing)')\n",
    "conf_matrix_ML(y_train,y_train_pred_RFC, model_RFC, 'Random Forest (Training)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrsFoC3su9_F"
   },
   "source": [
    "### Gradient Boost Regression (Multi-Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0OorThOu9_F"
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor( )\n",
    "param_grid = {\n",
    "    'loss'         : ['log_loss', 'exponential'],\n",
    "    'n_estimators' :  [1,3, 5,8,10,20,50,100,110,120, 130, 150,250],\n",
    "    'max_depth'    : [1,2,3,5,8,10,12,15,20,25,50,100,150,200],\n",
    "    'criterion'    : ['friedman_mse', 'squared_error'],\n",
    "    'learning_rate': [0.01,0.05,0.1,0.25,0.6,0.8,1]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=6, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train )\n",
    "model_GBR = grid_search.best_estimator_\n",
    "\n",
    "ML_grid_result (grid_search, model_GBR, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufADJtGwu9_F"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_GBR = model_GBR.predict(x_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_test_pred_GBR = model_GBR.predict(x_test)\n",
    "\n",
    "r2_GBR = r2_score(y_test,y_test_pred_GBR).round(3)\n",
    "mse_GBR=mean_squared_error(y_test,y_test_pred_GBR).round(3)\n",
    "mae_GBR=mean_absolute_error(y_test,y_test_pred_GBR).round(3)\n",
    "rmse_GBR=np.sqrt(mean_squared_error(y_test,y_test_pred_GBR)).round(3)\n",
    "CV_value_GBR= explained_variance_score(y_test,y_test_pred_GBR).round(3)\n",
    "print('Grad. Boost Regression Performance')\n",
    "print('target :', target_col )\n",
    "print('R2 : ', r2_GBR)\n",
    "print('MSE : ', mse_GBR)\n",
    "print('MAE : ', mae_GBR)\n",
    "print('RMSE : ', rmse_GBR)\n",
    "print('CV : ', CV_value_GBR)\n",
    "\n",
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                \"Grad.Boost.Class\":model_GBR.feature_importances_.round(4),\n",
    "                                }).sort_values( by=\"Grad.Boost.Class\" ,ascending = False)\n",
    "data_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ykdt6hqau9_F"
   },
   "source": [
    "### XGBoost Regression (Multi-Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vr88wnsUu9_G"
   },
   "outputs": [],
   "source": [
    "model = xg.XGBRegressor(   )\n",
    "param_grid = {\n",
    "    'n_estimators' : [10,20,50,100,110,120, 130, 150,250],\n",
    "    'learning_rate': [0.01,0.05,0.1,0.25,0.6,1],\n",
    "    'max_depth'    : [2,3,5,8,10,12,15,20,25,50,100,150,200],\n",
    "    'random_state' : [1,2,10,50,100]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, \n",
    "                           scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train )\n",
    "model_XGBR = grid_search.best_estimator_\n",
    "ML_grid_result (grid_search, model_XGBR, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zL-hAP6Eu9_G"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_XGBR = model_XGBR.predict(x_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_test_pred_XGBR = model_XGBR.predict(x_test)\n",
    "\n",
    "r2_XGBR = r2_score(y_test,y_test_pred_XGBR).round(3)\n",
    "mse_XGBR = mean_squared_error(y_test,y_test_pred_XGBR).round(3)\n",
    "mae_XGBR = mean_absolute_error(y_test,y_test_pred_XGBR).round(3)\n",
    "rmse_XGBR = np.sqrt(mean_squared_error(y_test,y_test_pred_XGBR)).round(3)\n",
    "CV_value_XGBR = explained_variance_score(y_test,y_test_pred_XGBR).round(3)\n",
    "print('XGBoost Regression Performance')\n",
    "print('target :', target_col )\n",
    "print('R2 : ', r2_XGBR)\n",
    "print('MSE : ', mse_XGBR)\n",
    "print('MAE : ', mae_XGBR)\n",
    "print('RMSE : ', rmse_XGBR)\n",
    "print('CV : ', CV_value_XGBR)\n",
    "\n",
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                \"XGBoost.Class\":model_XGBR.feature_importances_.round(4),\n",
    "                                }).sort_values( by=\"XGBoost.Class\" ,ascending = False)\n",
    "data_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPjuvlWcu9_G"
   },
   "source": [
    "### MLP Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UA5HzeVu9_G"
   },
   "outputs": [],
   "source": [
    "model = MLPRegressor()\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(1144, 272)], #(12,3), (12,5), (12, 8), (12, 13),(24,5) , (24,8), (24, 13), (24, 21), (36,8), (36,13), (36, 21), (108, 42), (108, 68), (236, 68),(236, 136),(572, 136),(572, 272), (1144, 272)\n",
    "    'activation'        : ['logistic'],  #'relu', 'logistic',  , 'tanh'\n",
    "    'solver'            : ['lbfgs'],    #'adam', 'sgd',\n",
    "    'learning_rate'     : ['constant'],     #,'invscaling','adaptive'\n",
    "    'alpha'             : [ 0.01],  # ,0.1\n",
    "    'max_iter'          : [1000]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=6, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train )\n",
    "model_MLPR = grid_search.best_estimator_\n",
    "ML_grid_result (grid_search, model_MLPR, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8huY1aylu9_G"
   },
   "outputs": [],
   "source": [
    "#ALTERNATIVE\n",
    "model_MPLR = MLPRegressor( hidden_layer_sizes=(1144,272), activation='tanh', alpha=0.1, learning_rate='adaptive', solver='adam')\n",
    "model_MLPR.fit(x_train, y_train )\n",
    "print(\"Accuracy score (training)  : {:.3f}\".format(model_MLPR.score(x_train, y_train)))\n",
    "print(\"Accuracy score (validation): {:.3f}\".format(model_MLPR.score(x_test, y_test)))\n",
    "\n",
    "y_train_pred_MLPR = model_MLPR.predict(x_train)\n",
    "y_test_pred_MLPR = model_MLPR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Lrt_8SYu9_G"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_MLPR = model_MLPR.predict(x_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_test_pred_MLPR = model_MLPR.predict(x_test)\n",
    "\n",
    "r2_MLPR = r2_score(y_test,y_test_pred_MLPR).round(3)\n",
    "mse_MLPR=mean_squared_error(y_test,y_test_pred_MLPR).round(3)\n",
    "mae_MLPR=mean_absolute_error(y_test,y_test_pred_MLPR).round(3)\n",
    "rmse_MLPR=np.sqrt(mean_squared_error(y_test,y_test_pred_MLPR)).round(3)\n",
    "CV_value_MLPR= explained_variance_score(y_test,y_test_pred_MLPR).round(3)\n",
    "print('Multilayer Perceptron Regression Performance')\n",
    "print('target :', target_col )\n",
    "print('R2 : ', r2_MLPR)\n",
    "print('MSE : ', mse_MLPR)\n",
    "print('MAE : ', mae_MLPR)\n",
    "print('RMSE : ', rmse_MLPR)\n",
    "print('CV : ', CV_value_MLPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CG1v5Hhiu9_H"
   },
   "source": [
    "### ANN custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sDi96q3u9_H"
   },
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOlwIk_pu9_H"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pS4jqahGu9_H"
   },
   "outputs": [],
   "source": [
    "xb.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NAi53L9u9_I"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Custom parabolic activation function\n",
    "def parabolic_activation(x):\n",
    "    return tf.where(x > 0, x, 0.01 * x)\n",
    "\n",
    "\n",
    "# Define a custom model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_dim=9, activation=parabolic_activation))  # Column 1\n",
    "model.add(Dense(128, activation=parabolic_activation))  # Column 2\n",
    "model.add(Dense(128, activation='sigmoid'))  # Column 3\n",
    "model.add(Dense(128, activation='sigmoid'))  # Column 4\n",
    "model.add(Dense(128, activation=parabolic_activation))  # Column 5\n",
    "model.add(Dense(128, activation=parabolic_activation))  # Column 6\n",
    "model.add(Dense(128, activation=parabolic_activation))  # Column 7\n",
    "model.add(Dense(128, activation=parabolic_activation))  # Column 8\n",
    "model.add(Dense(128, activation='sigmoid'))  # Column 9\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='hard_sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3P0Z2VMu9_I"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred_DL_Seq = model.predict(X_test)\n",
    "# train_acc_MLPC = accuracy_score(y_train, y_train_pred_MLPC).round(3)\n",
    "# accuracy_DL = accuracy_score(y_test, y_pred_DL_Seq).round(3)\n",
    "# f1 = f1_score(y_test, y_pred_DL_Seq)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mtrvkwsvu9_I"
   },
   "outputs": [],
   "source": [
    "plt.plot(y_pred_DL_Seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuV7XYfcu9_I"
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Co3XASijqgFh"
   },
   "source": [
    "## 2B. Train Regression ML *Manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNzJ1QTABg9s"
   },
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwDCyrp0Bg9s",
    "outputId": "40563426-2980-4c81-984e-1d75b24e9694"
   },
   "outputs": [],
   "source": [
    "Lin_reg = LinearRegression()\n",
    "Lin_reg.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred_LinReg = Lin_reg.predict(x_train)#.reshape(1,-1)\n",
    "y_test_pred_LinReg = Lin_reg.predict(x_test)#.reshape(1,-1)\n",
    "print(Lin_reg.intercept_)\n",
    "print(Lin_reg.coef_)\n",
    "print(Lin_reg.score(x_train, y_train))\n",
    "print(Lin_reg.score(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89OCydIrBg9s",
    "outputId": "04b50ba4-86df-4798-fdde-7695227028e8"
   },
   "outputs": [],
   "source": [
    "r2_LinReg = r2_score(y_test,y_test_pred_LinReg).round(3)\n",
    "mse_LinReg=mean_squared_error(y_test,y_test_pred_LinReg).round(3)\n",
    "mae_LinReg=mean_absolute_error(y_test,y_test_pred_LinReg).round(3)\n",
    "rmse_LinReg=np.sqrt(mean_squared_error(y_test,y_test_pred_LinReg)).round(3)\n",
    "CV_value_LinReg= explained_variance_score(y_test,y_test_pred_LinReg).round(3)\n",
    "\n",
    "train_r2_LinReg = r2_score(y_train,y_train_pred_LinReg).round(3)\n",
    "train_mse_LinReg=mean_squared_error(y_train,y_train_pred_LinReg).round(3)\n",
    "train_mae_LinReg=mean_absolute_error(y_train,y_train_pred_LinReg).round(3)\n",
    "train_rmse_LinReg=np.sqrt(mean_squared_error(y_train,y_train_pred_LinReg)).round(3)\n",
    "train_CV_value_LinReg= explained_variance_score(y_train,y_train_pred_LinReg).round(3)\n",
    "\n",
    "print('Linear Regression Performance')\n",
    "print('R2 : ', r2_LinReg)\n",
    "print('MSE : ', mse_LinReg)\n",
    "print('MAE : ', mae_LinReg)\n",
    "print('RMSE : ', rmse_LinReg)\n",
    "print('CV : ', CV_value_LinReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xt8z_famBg9s"
   },
   "source": [
    "### Lasso Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1711381518495,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "IJEVcnZ-Bg9s",
    "outputId": "4f08a252-743a-4dad-bd05-6f2422ad5eaf"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "#Lasso = Lasso(alpha=0.2, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
    "#Lasso = Lasso(alpha=3, fit_intercept=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
    "Lasso_reg = Lasso(alpha=0.1, max_iter=1000, selection= 'random' )\n",
    "Lasso_reg.fit(x_train, y_train )\n",
    "print(Lasso_reg.intercept_)\n",
    "print(Lasso_reg.coef_)\n",
    "print(Lasso_reg.score(x_test, y_test))\n",
    "print(Lasso_reg.score(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1711381518876,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "c9HATyucBg9s",
    "outputId": "eaa14180-2abc-461e-88a1-fb379831e25a"
   },
   "outputs": [],
   "source": [
    "y_train_pred_Lasso = Lasso_reg.predict(x_train)#.reshape(-1,1)\n",
    "y_test_pred_Lasso = Lasso_reg.predict(x_test)#.reshape(-1,1)\n",
    "\n",
    "r2_Lasso = r2_score(y_test,y_test_pred_Lasso).round(3)\n",
    "mse_Lasso=mean_squared_error(y_test,y_test_pred_Lasso).round(3)\n",
    "mae_Lasso=mean_absolute_error(y_test,y_test_pred_Lasso).round(3)\n",
    "rmse_Lasso=np.sqrt(mean_squared_error(y_test,y_test_pred_Lasso)).round(3)\n",
    "CV_value_Lasso= explained_variance_score(y_test,y_test_pred_Lasso).round(3)\n",
    "\n",
    "train_r2_Lasso = r2_score(y_train,y_train_pred_Lasso).round(3)\n",
    "train_mse_Lasso=mean_squared_error(y_train,y_train_pred_Lasso).round(3)\n",
    "train_mae_Lasso=mean_absolute_error(y_train,y_train_pred_Lasso).round(3)\n",
    "train_rmse_Lasso=np.sqrt(mean_squared_error(y_train,y_train_pred_Lasso)).round(3)\n",
    "train_CV_value_Lasso= explained_variance_score(y_train,y_train_pred_Lasso).round(3)\n",
    "\n",
    "print('Lasso Regression Performance')\n",
    "print('R2 : ', r2_Lasso)\n",
    "print('MSE : ', mse_Lasso)\n",
    "print('MAE : ', mae_Lasso)\n",
    "print('RMSE : ', rmse_Lasso)\n",
    "print('CV : ', CV_value_Lasso)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFY8fVTxBg9t"
   },
   "source": [
    "### Ridge Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1711376938706,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "xOGIrZAhBg9t",
    "outputId": "2a47ca45-f086-46fe-e48a-f78695a577d6"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "Ridge_reg = Ridge()\n",
    "Ridge_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1711376938707,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "x5Gu2NIsBg9t",
    "outputId": "f9793022-c0a1-4b4c-bde5-91233e642cab"
   },
   "outputs": [],
   "source": [
    "y_train_pred_Ridge = Ridge_reg.predict(x_train)#.reshape(-1,1)\n",
    "y_test_pred_Ridge = Ridge_reg.predict(x_test)#.reshape(-1,1)\n",
    "print(Ridge_reg.intercept_)\n",
    "print(Ridge_reg.coef_)\n",
    "print(Ridge_reg.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1711376938707,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "nA3VeSkwBg9t",
    "outputId": "a1535445-4488-45a7-fdb7-6f8b66115bd4"
   },
   "outputs": [],
   "source": [
    "r2_Ridge = r2_score(y_test,y_test_pred_Ridge).round(3)\n",
    "mse_Ridge=mean_squared_error(y_test,y_test_pred_Ridge).round(3)\n",
    "mae_Ridge=mean_absolute_error(y_test,y_test_pred_Ridge).round(3)\n",
    "rmse_Ridge=np.sqrt(mean_squared_error(y_test,y_test_pred_Ridge)).round(3)\n",
    "CV_value_Ridge= explained_variance_score(y_test,y_test_pred_Ridge).round(3)\n",
    "\n",
    "train_r2_Ridge = r2_score(y_train,y_train_pred_Ridge).round(3)\n",
    "train_mse_Ridge=mean_squared_error(y_train,y_train_pred_Ridge).round(3)\n",
    "train_mae_Ridge=mean_absolute_error(y_train,y_train_pred_Ridge).round(3)\n",
    "train_rmse_Ridge=np.sqrt(mean_squared_error(y_train,y_train_pred_Ridge)).round(3)\n",
    "train_CV_value_Ridge= explained_variance_score(y_train,y_train_pred_Ridge).round(3)\n",
    "\n",
    "print('Ridge Regression Performance')\n",
    "print('R2 : ', r2_Ridge)\n",
    "print('MSE : ', mse_Ridge)\n",
    "print('MAE : ', mae_Ridge)\n",
    "print('RMSE : ', rmse_Ridge)\n",
    "print('CV : ', CV_value_Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dO4G4PLEBg9t"
   },
   "source": [
    "### Support Vector Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VIcHvfrhi7C",
    "outputId": "6d0403c8-b7b0-47fd-9f57-4e928c3c0ed7"
   },
   "outputs": [],
   "source": [
    "lr_list = [1, 2, 3, 5, 8, 13, 21, 34, ]\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  SVR_optimum = SVR( kernel = 'linear', gamma= 'auto' , degree=6, tol=0.001, C=find_optimum, epsilon= 0.1, cache_size=200)\n",
    "#find the most optimum C-value for SVRegression\n",
    "\n",
    "  SVR_optimum.fit(x_train, y_train)\n",
    "\n",
    "  ML_optimization_process (find_optimum, SVR_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjPX5MBT02Xb"
   },
   "outputs": [],
   "source": [
    "C_use = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 968,
     "status": "ok",
     "timestamp": 1711381543640,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "COplhlB2Bg9t",
    "outputId": "4037cd58-9c60-4ac7-86b9-ab48f7fbfa29"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "model_SVR = SVR_optimum = SVR( kernel = 'linear', gamma= 'auto' , degree=6, tol=0.001, C=C_use, epsilon= 0.1, cache_size=200)\n",
    "model_SVR.fit(x_train, y_train)\n",
    "\n",
    "print(model_SVR.intercept_)\n",
    "print(model_SVR.coef_)\n",
    "print(model_SVR.score(x_test, y_test))\n",
    "print(model_SVR.score(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HfHOkQ3Bg9t",
    "outputId": "0b30422f-308b-4e57-ae56-07d6b5a500f1"
   },
   "outputs": [],
   "source": [
    "# use model to predict\n",
    "y_test_pred_SVR = model_SVR.predict(x_test)\n",
    "y_train_pred_SVR = model_SVR.predict(x_train)\n",
    "\n",
    "r2_SVR = r2_score(y_test,y_test_pred_SVR).round(3)\n",
    "mse_SVR=mean_squared_error(y_test,y_test_pred_SVR).round(3)\n",
    "mae_SVR=mean_absolute_error(y_test,y_test_pred_SVR).round(3)\n",
    "rmse_SVR=np.sqrt(mean_squared_error(y_test,y_test_pred_SVR)).round(3)\n",
    "CV_value_SVR= explained_variance_score(y_test,y_test_pred_SVR).round(3)\n",
    "\n",
    "train_r2_SVR = r2_score(y_train,y_train_pred_SVR).round(3)\n",
    "train_mse_SVR=mean_squared_error(y_train,y_train_pred_SVR).round(3)\n",
    "train_mae_SVR=mean_absolute_error(y_train,y_train_pred_SVR).round(3)\n",
    "train_rmse_SVR=np.sqrt(mean_squared_error(y_train,y_train_pred_SVR)).round(3)\n",
    "train_CV_value_SVR= explained_variance_score(y_train,y_train_pred_SVR).round(3)\n",
    "\n",
    "print('Support Vector Regression Performance')\n",
    "print('R2 : ', r2_SVR)\n",
    "print('MSE : ', mse_SVR)\n",
    "print('MAE : ', mae_SVR)\n",
    "print('RMSE : ', rmse_SVR)\n",
    "print('CV : ', CV_value_SVR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHq9lMAZHJbo"
   },
   "source": [
    "### KNN Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwIJUgIIHOoR",
    "outputId": "fffd9bdd-fee5-4a24-9958-df74dccc658b"
   },
   "outputs": [],
   "source": [
    "lr_list = [1,2,3,4,5,8,10,13,21,34] #,55,89,100, 120,150,180,144,200,500\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  KNR_optimum = KNeighborsRegressor(n_neighbors=find_optimum)\n",
    "#find the most optimum n_neighbors value for KNN Regressor\n",
    "  KNR_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, KNR_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnL49BwDHd2q"
   },
   "outputs": [],
   "source": [
    "n_neighbors_use =4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7ybRr8bHdKs",
    "outputId": "4df95a7f-6572-4423-e231-6001ac12e423"
   },
   "outputs": [],
   "source": [
    "lr_list = ['uniform', 'distance']\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  KNR_optimum = KNeighborsRegressor(n_neighbors=n_neighbors_use, weights= find_optimum)\n",
    "#find the most optimum weights value for KNN Regressor\n",
    "  KNR_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, KNR_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdBXjY9UHnB8"
   },
   "outputs": [],
   "source": [
    "weights_use = 'distance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vs8eBu1hHmay",
    "outputId": "9661c5bc-a101-4d45-9e02-1123b2563282"
   },
   "outputs": [],
   "source": [
    "lr_list = [ 'ball_tree', 'kd_tree', 'brute']\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  KNR_optimum = KNeighborsRegressor(n_neighbors=n_neighbors_use, weights= weights_use, algorithm = find_optimum )\n",
    "#find the most optimum algorithm for KNN Regressor\n",
    "  KNR_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, KNR_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_gp5cW_HuRf"
   },
   "outputs": [],
   "source": [
    "algorithm_use = 'kd_tree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77KCmZbtHy1E"
   },
   "outputs": [],
   "source": [
    "lr_list = [1,5,10,20,30,50,80,130,210,440]\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  KNR_optimum = KNeighborsRegressor(n_neighbors=n_neighbors_use, weights= weights_use, algorithm = algorithm_use , leaf_size= find_optimum )\n",
    "#find the most optimum leaf_size value for KNN Regressor\n",
    "  KNR_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, KNR_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CGaPH9NH-eI"
   },
   "outputs": [],
   "source": [
    "leaf_size_use = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1711381592264,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "nQOLS-JhH-CH",
    "outputId": "5da3b58b-864d-4947-9d91-c0245c827c54"
   },
   "outputs": [],
   "source": [
    "# Fit KNN model\n",
    "KNN_reg = KNeighborsRegressor(n_neighbors=n_neighbors_use, weights= weights_use, algorithm = algorithm_use , leaf_size= leaf_size_use )\n",
    "KNN_reg.fit(x_train, y_train)\n",
    "\n",
    "# Fit decision tree input-output\n",
    "model_KNR = KNN_reg.fit(x_train, y_train)\n",
    "print(model_KNR.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1711381595358,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "6jMOG-4DIZQg",
    "outputId": "f09b5a95-8796-4024-9afa-a00276dc6390"
   },
   "outputs": [],
   "source": [
    "y_train_pred_KNR = model_KNR.predict(x_train)\n",
    "y_test_pred_KNR = model_KNR.predict(x_test)\n",
    "\n",
    "r2_KNR = r2_score(y_test,y_test_pred_KNR).round(3)\n",
    "mse_KNR=mean_squared_error(y_test,y_test_pred_KNR).round(3)\n",
    "mae_KNR=mean_absolute_error(y_test,y_test_pred_KNR).round(3)\n",
    "rmse_KNR=np.sqrt(mean_squared_error(y_test,y_test_pred_KNR)).round(3)\n",
    "CV_value_KNR= explained_variance_score(y_test,y_test_pred_KNR).round(3)\n",
    "\n",
    "train_r2_KNR = r2_score(y_train,y_train_pred_KNR).round(3)\n",
    "train_mse_KNR=mean_squared_error(y_train,y_train_pred_KNR).round(3)\n",
    "train_mae_KNR=mean_absolute_error(y_train,y_train_pred_KNR).round(3)\n",
    "train_rmse_KNR=np.sqrt(mean_squared_error(y_train,y_train_pred_KNR)).round(3)\n",
    "train_CV_value_KNR= explained_variance_score(y_train,y_train_pred_KNR).round(3)\n",
    "\n",
    "# Compute permutation importance\n",
    "KNN_importance = permutation_importance(model_KNR, x_train, y_train, n_repeats=10, random_state=42)\n",
    "\n",
    "print('K-Neighbors Regression Performance')\n",
    "print('target :', target_col )\n",
    "print('R2 : ', r2_KNR)\n",
    "print('MSE : ', mse_KNR)\n",
    "print('MAE : ', mae_KNR)\n",
    "print('RMSE : ', rmse_KNR)\n",
    "print('CV : ', CV_value_KNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MoXaTZq-xyLu",
    "outputId": "de52f574-52f7-42e9-accd-c680e9858c96"
   },
   "outputs": [],
   "source": [
    "f_importance_KNR = pd.DataFrame({'features':xb.columns,\n",
    "                                'permutation_importances KNR':KNN_importance['importances_mean']}).sort_values(by = 'permutation_importances KNR',ascending = False)\n",
    "f_importance_KNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlFn_CEsBg9t"
   },
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLFXvNgpyAP5",
    "outputId": "f3cc0d52-0dc3-4440-ee58-201eb6853db4"
   },
   "outputs": [],
   "source": [
    "lr_list = [2, 3, 4, 5, 6, 7,8,9,10, 12,15,20,25,50,100]\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  DTR_optimum = DecisionTreeRegressor(max_depth=find_optimum)\n",
    "#find the most optimum C value for DTC Regressor\n",
    "  DTR_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, DTR_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diXYVUyz09iD"
   },
   "outputs": [],
   "source": [
    "max_depth_use = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k54PT0Qmyy2y",
    "outputId": "dbfbc030-b713-4838-fdfe-6f0053ffe223"
   },
   "outputs": [],
   "source": [
    "lr_list = [1,2, 3, 4, 5, 6, 7,8,10, 15, 25,50, 100]\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  DTR_optimum = DecisionTreeRegressor(max_depth= max_depth_use , random_state= find_optimum)\n",
    "#find the most optimum C value for DTC Regressor\n",
    "\n",
    "  DTR_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, DTR_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7gI6ySe1Dlx"
   },
   "outputs": [],
   "source": [
    "random_use = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1711376991416,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "KrS0Tn7sBg9t",
    "outputId": "31de06d7-816a-4eea-9a6f-144444e5bbb9"
   },
   "outputs": [],
   "source": [
    "# Fit decision tree model\n",
    "DT_Reg = DecisionTreeRegressor(max_depth= max_depth_use, random_state= random_use  )\n",
    "DT_Reg.fit(x_train, y_train)\n",
    "\n",
    "# Fit decision tree input-output\n",
    "model_DTR = DT_Reg.fit(x_train, y_train)\n",
    "print(model_DTR.score(x_train, y_train))\n",
    "print(model_DTR.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5NJg48lBg9t",
    "outputId": "92a13199-4ebf-4fd7-da66-966ca410954c"
   },
   "outputs": [],
   "source": [
    "y_train_pred_DTR = model_DTR.predict(x_train)\n",
    "y_test_pred_DTR = model_DTR.predict(x_test)\n",
    "\n",
    "r2_DTR = r2_score(y_test,y_test_pred_DTR).round(3)\n",
    "mse_DTR=mean_squared_error(y_test,y_test_pred_DTR).round(3)\n",
    "mae_DTR=mean_absolute_error(y_test,y_test_pred_DTR).round(3)\n",
    "rmse_DTR=np.sqrt(mean_squared_error(y_test,y_test_pred_DTR)).round(3)\n",
    "CV_value_DTR= explained_variance_score(y_test,y_test_pred_DTR).round(3)\n",
    "\n",
    "train_r2_DTR = r2_score(y_train,y_train_pred_DTR).round(3)\n",
    "train_mse_DTR=mean_squared_error(y_train,y_train_pred_DTR).round(3)\n",
    "train_mae_DTR=mean_absolute_error(y_train,y_train_pred_DTR).round(3)\n",
    "train_rmse_DTR=np.sqrt(mean_squared_error(y_train,y_train_pred_DTR)).round(3)\n",
    "train_CV_value_DTR= explained_variance_score(y_train,y_train_pred_DTR).round(3)\n",
    "\n",
    "print('Decision Tree Regression Performance')\n",
    "print('target :', target_col )\n",
    "print('R2 : ', r2_DTR)\n",
    "print('MSE : ', mse_DTR)\n",
    "print('MAE : ', mae_DTR)\n",
    "print('RMSE : ', rmse_DTR)\n",
    "print('CV : ', CV_value_DTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3K4MeLpSBg9t",
    "outputId": "4c99a2e4-0b13-4683-d4df-8f5fb283a2d8"
   },
   "outputs": [],
   "source": [
    "f_importance_DTR = pd.DataFrame({'features':xb.columns,\n",
    "                                'feature_importances DTR':DT_Reg.feature_importances_}).sort_values(by = 'feature_importances DTR',ascending = False)\n",
    "f_importance_DTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szCwJIc7Bg9t"
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(25, 12))\n",
    "#plot_tree(DT_Reg)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_wrQVvNyj9U"
   },
   "source": [
    "### Extra Trees Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akszMb8Syj9a",
    "outputId": "efbc9e1d-12f9-4300-ffd4-d0c239dad221"
   },
   "outputs": [],
   "source": [
    "#optimum MAX_DEPTH\n",
    "lr_list = [1,2,3,4,5,8,10,13,21,34,38,42,55,89,100, 120,150,180,144,200,500,700]\n",
    "for find_optimum in lr_list:\n",
    "  ETR_optimum = ExtraTreesRegressor(max_depth=find_optimum, random_state=10)\n",
    "  ETR_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, ETR_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3T0aA31Nyj9b"
   },
   "outputs": [],
   "source": [
    "max_depth_use = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B02SaXqPyj9b",
    "outputId": "b33a0294-40a7-4531-ded7-08a34d9097a4"
   },
   "outputs": [],
   "source": [
    "#optimum RANDOM_STATE\n",
    "lr_list = [1,2,5,10,50,100,200,400]\n",
    "for find_optimum in lr_list:\n",
    "  ETR_optimum = ExtraTreesRegressor(max_depth=max_depth_use, random_state=find_optimum)\n",
    "  ETR_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, ETR_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcB-l519yj9b"
   },
   "outputs": [],
   "source": [
    "random_use = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcX21jWoyj9b",
    "outputId": "7a506846-fcef-4ac4-eeb1-46dbe0f4ccf8"
   },
   "outputs": [],
   "source": [
    "#optimum CRITERION\n",
    "lr_list = ['squared_error', 'friedman_mse', 'absolute_error', 'poisson']\n",
    "for find_optimum in lr_list:\n",
    "  ETR_optimum = ExtraTreesRegressor(max_depth=max_depth_use, random_state=random_use, criterion= find_optimum)\n",
    "  ETR_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, ETR_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmpDwCuCyj9b"
   },
   "outputs": [],
   "source": [
    "criterion_use = 'absolute_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYfm8UoRyj9b",
    "outputId": "643681a3-3b3f-4ff3-a073-b5c538c13a39"
   },
   "outputs": [],
   "source": [
    "#optimum SPLITTER\n",
    "lr_list = [2,5,10,20,50,100,110,120, 130, 150,250]\n",
    "for find_optimum in lr_list:\n",
    "  ETR_optimum = ExtraTreesRegressor(max_depth=max_depth_use, random_state=random_use,\n",
    "                                       criterion= criterion_use, n_estimators= find_optimum )\n",
    "  ETR_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, ETR_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5r_4Lpjyj9b"
   },
   "outputs": [],
   "source": [
    "n_estimator_use = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1711377063273,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "cUJ3A6aeyj9b",
    "outputId": "ee21a260-98d6-4e12-f569-1e4679e51755"
   },
   "outputs": [],
   "source": [
    "# Fit decision tree model\n",
    "ETR = ExtraTreesRegressor(max_depth=max_depth_use, random_state=random_use,\n",
    "                                       criterion= criterion_use, n_estimators=n_estimator_use,\n",
    "                                    #    max_features= max_features_use\n",
    "                                       )\n",
    "ETR.fit(x_train, y_train)\n",
    "\n",
    "# Fit decision tree input-output\n",
    "model_ETR = ETR.fit(x_train, y_train)\n",
    "print(model_ETR.score(x_train,y_train))\n",
    "print(model_ETR.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghEu37d9yj9b",
    "outputId": "2741ec47-eebf-469a-e1f7-d24eab6edefd"
   },
   "outputs": [],
   "source": [
    "# Predict train set labels\n",
    "y_train_pred_ETR = model_ETR.predict(x_train)\n",
    "y_test_pred_ETR = model_ETR.predict(x_test)\n",
    "\n",
    "r2_ETR = r2_score(y_test,y_test_pred_ETR).round(3)\n",
    "mse_ETR= mean_squared_error(y_test,y_test_pred_ETR).round(3)\n",
    "mae_ETR= mean_absolute_error(y_test,y_test_pred_ETR).round(3)\n",
    "rmse_ETR= np.sqrt(mean_squared_error(y_test,y_test_pred_ETR)).round(3)\n",
    "CV_value_ETR = explained_variance_score(y_test,y_test_pred_ETR).round(3)\n",
    "\n",
    "train_r2_ETR = r2_score(y_train,y_train_pred_ETR).round(3)\n",
    "train_mse_ETR= mean_squared_error(y_train,y_train_pred_ETR).round(3)\n",
    "train_mae_ETR= mean_absolute_error(y_train,y_train_pred_ETR).round(3)\n",
    "train_rmse_ETR= np.sqrt(mean_squared_error(y_train,y_train_pred_ETR)).round(3)\n",
    "train_CV_value_ETR = explained_variance_score(y_train,y_train_pred_ETR).round(3)\n",
    "\n",
    "print('Extra.Trees Performance')\n",
    "print('target :', target_col )\n",
    "print('R2 : ', r2_ETR)\n",
    "print('MSE : ', mse_ETR)\n",
    "print('MAE : ', mae_ETR)\n",
    "print('RMSE : ', rmse_ETR)\n",
    "print('CV : ', CV_value_ETR)\n",
    "\n",
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                \"Extra.Trees.Class\":model_ETR.feature_importances_.round(4),\n",
    "                                }).sort_values( by=\"Extra.Trees.Class\" ,ascending = False)\n",
    "data_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBZxpclVyj9c"
   },
   "outputs": [],
   "source": [
    "# fn = model_ETR.feature_names_in_\n",
    "# cn= model_ETR.feature_names_in_\n",
    "# figsize = (n_estimator_use*5,5)\n",
    "\n",
    "# fig, axes = plt.subplots(nrows = 1,ncols = n_estimator_use,figsize = figsize, dpi=900)\n",
    "# for index in range(0, n_estimator_use):\n",
    "#     tree.plot_tree(model_ETR.estimators_[index],\n",
    "#                    feature_names = fn,\n",
    "#                    class_names=target_col,\n",
    "#                    filled = True,\n",
    "#                    ax = axes[index]);\n",
    "\n",
    "#     axes[index].set_title('Estimator: ' + str(index), fontsize = 11)\n",
    "# fig.savefig(directory_path + '/'+ folder_ML + '/' + 'model_ETR_' + str(target_col) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rtXV33ZBg9u"
   },
   "source": [
    "### Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [1,2,4,5,6,7,8,12,15,20,25,50]\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  RFR_optimum = RandomForestRegressor(max_depth=find_optimum)\n",
    "#find the most optimum max_depth for RFR Regressor\n",
    "\n",
    "  RFR_optimum.fit(x_train, y_train)\n",
    "\n",
    "  ML_optimization_process (find_optimum, RFR_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_use = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [1,2, 4, 6,8,10,15,20,25,50, 100]\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  RFR_optimum = RandomForestRegressor(max_depth= max_depth_use , random_state=find_optimum)\n",
    "#find the most optimum C value for RFR Regressor\n",
    "\n",
    "  RFR_optimum.fit(x_train, y_train)\n",
    "\n",
    "  ML_optimization_process (find_optimum, RFR_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_use = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RFR = RandomForestRegressor(max_depth = max_depth_use , random_state = random_use )\n",
    "model_RFR.fit(x_train, y_train)\n",
    "# use model to predict\n",
    "y_test_pred_RFR = (model_RFR.predict(x_test)).reshape(-1,1)\n",
    "y_train_pred_RFR = (model_RFR.predict(x_train)).reshape(-1,1)\n",
    "y_pred_RFR = (model_RFR.predict(xb)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_RFR = r2_score(y_test,y_test_pred_RFR).round(3)\n",
    "mse_RFR = mean_squared_error(y_test,y_test_pred_RFR).round(3)\n",
    "mae_RFR = mean_absolute_error(y_test,y_test_pred_RFR).round(3)\n",
    "rmse_RFR = np.sqrt(mean_squared_error(y_test,y_test_pred_RFR)).round(3)\n",
    "CV_value_RFR = (rmse_RFR/np.array(y_test).flatten().mean(axis=0)).round(3)\n",
    "\n",
    "train_r2_RFR = r2_score(y_train,y_train_pred_RFR).round(3)\n",
    "train_mse_RFR = mean_squared_error(y_train,y_train_pred_RFR).round(3)\n",
    "train_mae_RFR = mean_absolute_error(y_train,y_train_pred_RFR).round(3)\n",
    "train_rmse_RFR = np.sqrt(mean_squared_error(y_train,y_train_pred_RFR)).round(3)\n",
    "train_CV_value_RFR = (train_rmse_RFR/np.array(y_train).flatten().mean(axis=0)).round(3)\n",
    "\n",
    "print('Random Forest Regression Performance')\n",
    "print('R2 : ', r2_RFR)\n",
    "print('MSE : ', mse_RFR)\n",
    "print('MAE : ', mae_RFR)\n",
    "print('RMSE : ', rmse_RFR)\n",
    "print('CV : ', CV_value_RFR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_importance_RFR = pd.DataFrame({'features':xb.columns,\n",
    "                                'feature_importances':model_RFR.feature_importances_}).sort_values(by = 'feature_importances',ascending = False)\n",
    "f_importance_RFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7XWtQkpvye6H",
    "outputId": "49ca5df2-244b-446f-8f72-f29cf67cbc06"
   },
   "outputs": [],
   "source": [
    "lr_list = [2, 3,4, 6,8,10,15,20,25,50, 100]\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  RFR_optimum = RandomForestRegressor(max_depth=find_optimum, random_state=1)\n",
    "#fin the most optimum max_depth for RFR Regressor\n",
    "\n",
    "  RFR_optimum.fit(x_train, y_train)\n",
    "\n",
    "  ML_optimization_process (find_optimum, RFR_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBekLXrV1KtS"
   },
   "outputs": [],
   "source": [
    "max_depth_use = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "herXKBpq1Hoo",
    "outputId": "5aac7b61-027a-420c-a7c0-f495618798a8"
   },
   "outputs": [],
   "source": [
    "lr_list = [2, 4, 6,8,10,15,20,25,50, 100]\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  RFR_optimum = RandomForestRegressor(max_depth= max_depth_use , random_state=find_optimum)\n",
    "#find the most optimum C value for RFR Regressor\n",
    "\n",
    "  RFR_optimum.fit(x_train, y_train)\n",
    "\n",
    "  ML_optimization_process (find_optimum, RFR_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq4cYFNX1PTc"
   },
   "outputs": [],
   "source": [
    "random_state_use = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1711381726348,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "wNYpVDwnBg9u",
    "outputId": "3f9e6e97-a3be-44c0-b4a2-0f02dccc3e0e"
   },
   "outputs": [],
   "source": [
    "model_RFR = RandomForestRegressor(max_depth = max_depth_use , random_state = random_state_use )\n",
    "model_RFR.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCL1kGJNBg9u",
    "outputId": "1ce02b34-c228-4625-d178-3a38c27f55dc"
   },
   "outputs": [],
   "source": [
    "# use model to predict\n",
    "y_test_pred_RFR = (model_RFR.predict(x_test)).reshape(-1,1)\n",
    "y_train_pred_RFR = (model_RFR.predict(x_train)).reshape(-1,1)\n",
    "\n",
    "r2_RFR = r2_score(y_test,y_test_pred_RFR).round(3)\n",
    "mse_RFR = mean_squared_error(y_test,y_test_pred_RFR).round(3)\n",
    "mae_RFR = mean_absolute_error(y_test,y_test_pred_RFR).round(3)\n",
    "rmse_RFR = np.sqrt(mean_squared_error(y_test,y_test_pred_RFR)).round(3)\n",
    "CV_value_RFR = explained_variance_score(y_test,y_test_pred_RFR).round(3)\n",
    "\n",
    "train_r2_RFR = r2_score(y_train,y_train_pred_RFR).round(3)\n",
    "train_mse_RFR = mean_squared_error(y_train,y_train_pred_RFR).round(3)\n",
    "train_mae_RFR = mean_absolute_error(y_train,y_train_pred_RFR).round(3)\n",
    "train_rmse_RFR = np.sqrt(mean_squared_error(y_train,y_train_pred_RFR)).round(3)\n",
    "train_CV_value_RFR = explained_variance_score(y_train,y_train_pred_RFR).round(3)\n",
    "\n",
    "print('Random Forest Regression Performance')\n",
    "print('R2 : ', r2_RFR)\n",
    "print('MSE : ', mse_RFR)\n",
    "print('MAE : ', mae_RFR)\n",
    "print('RMSE : ', rmse_RFR)\n",
    "print('CV : ', CV_value_RFR)\n",
    "\n",
    "f_importance_RFR = pd.DataFrame({'features':xb.columns,\n",
    "                                'feature_importances':model_RFR.feature_importances_}).sort_values(by = 'feature_importances',ascending = False)\n",
    "f_importance_RFR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhDidKtuSqSx"
   },
   "source": [
    "### Stochastic Gaussian Descent Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39DKJ9wvk9qx"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90ali8GJlyEE",
    "outputId": "440a75d1-fd0e-47d9-8e03-65e605cfd9e2"
   },
   "outputs": [],
   "source": [
    "lr_list = [0.001,0.005,0.01,0.05,0.1, 0.15, 0.2, 0.25,0.35,0.5]\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  SGD_optimum = SGDRegressor( alpha=0.0001, l1_ratio= find_optimum,  max_iter=3000, tol=0.001, validation_fraction=0.176, eta0=0.01, power_t=0.25  )\n",
    "#find the most optimum l1_ratio for SGDRegression\n",
    "\n",
    "  SGD_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, SGD_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w08LK2XJ1XMo"
   },
   "outputs": [],
   "source": [
    "l1_ratio_use = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1711378433119,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "mIKWOvXxSwJA",
    "outputId": "53e6e9ef-de3b-4403-c646-03300a8bdfd4"
   },
   "outputs": [],
   "source": [
    "#SGD_optimum = SGDRegressor( alpha=0.0001, l1_ratio= 0.25,  max_iter=3000, tol=0.005, validation_fraction=0.176, eta0=0.15, power_t=0.2 )\n",
    "model_SGR = SGDRegressor(l1_ratio = l1_ratio_use )\n",
    "model_SGR.fit(x_train, y_train)\n",
    "print(model_SGR.score(x_train, y_train))\n",
    "print(model_SGR.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvTOSuPXTxjM",
    "outputId": "c09352d4-5a04-4766-cc2a-aeb61cc09f1c"
   },
   "outputs": [],
   "source": [
    "y_train_pred_SGR = model_SGR.predict(x_train)\n",
    "y_test_pred_SGR = model_SGR.predict(x_test)\n",
    "\n",
    "r2_SGR = r2_score(y_test,y_test_pred_SGR).round(3)\n",
    "mse_SGR =mean_squared_error(y_test,y_test_pred_SGR).round(3)\n",
    "mae_SGR =mean_absolute_error(y_test,y_test_pred_SGR).round(3)\n",
    "rmse_SGR =np.sqrt(mean_squared_error(y_test,y_test_pred_SGR)).round(3)\n",
    "CV_value_SGR = explained_variance_score(y_test,y_test_pred_SGR).round(3)\n",
    "\n",
    "train_r2_SGR = r2_score(y_train,y_train_pred_SGR).round(3)\n",
    "train_mse_SGR =mean_squared_error(y_train,y_train_pred_SGR).round(3)\n",
    "train_mae_SGR =mean_absolute_error(y_train,y_train_pred_SGR).round(3)\n",
    "train_rmse_SGR =np.sqrt(mean_squared_error(y_train,y_train_pred_SGR)).round(3)\n",
    "train_CV_value_SGR = explained_variance_score(y_train,y_train_pred_SGR).round(3)\n",
    "\n",
    "print('Stoc.Gaussian Desc. Regression Performance')\n",
    "print('target :', target_col )\n",
    "print('R2 : ', r2_SGR)\n",
    "print('MSE : ', mse_SGR)\n",
    "print('MAE : ', mae_SGR)\n",
    "print('RMSE : ', rmse_SGR)\n",
    "print('CV : ', CV_value_SGR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ovm8xv10biih"
   },
   "source": [
    "### XG Boost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list =  [0.1,0.25,0.5,1,2,3,5,10,20,30,40,70,85,100]\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  XGB_optimum = xg.XGBRegressor( reg_lambda=find_optimum)\n",
    "#find the most optimum max_depth for XGBoost Regression\n",
    "\n",
    "  XGB_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, XGB_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lambda_use = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list =  [0.005,0.01, 0.025,0.05,0.075, 0.09,0.1,0.15,0.2, 0.25,0.35,0.5,0.6,0.75]\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  XGB_optimum = xg.XGBRegressor( reg_alpha=find_optimum,reg_lambda= reg_lambda_use)\n",
    "#find the most optimum max_depth for XGBoost Regression\n",
    "\n",
    "  XGB_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, XGB_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_alpha_use = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [1,2,3,4,5,6,7,8,13,21,34,55,89,144]\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  XGB_optimum = xg.XGBRegressor( max_depth=find_optimum,reg_alpha=reg_alpha_use,reg_lambda= reg_lambda_use)\n",
    "#find the most optimum max_depth for XGBoost Regression\n",
    "\n",
    "  XGB_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, XGB_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_use = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [#0.001,0.005,\n",
    "  0.01, 0.05,0.075, 0.09,0.1,0.15,0.2, 0.25,0.35,0.5,0.75]\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  XGB_optimum = xg.XGBRegressor( max_depth=max_depth_use, learning_rate = find_optimum, reg_alpha= reg_alpha_use, \n",
    "                                reg_lambda= reg_lambda_use, n_estimators=100)\n",
    "#find the most optimum learning_rate for XGBoost Regression\n",
    "\n",
    "  XGB_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, XGB_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_use = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [1,2,3,4,6,7,10,20,35,50,90,95,100,120]\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  XGB_optimum = xg.XGBRegressor( max_depth=max_depth_use, learning_rate = learning_rate_use, reg_alpha= reg_alpha_use, \n",
    "                                reg_lambda= reg_lambda_use, n_estimators=find_optimum)\n",
    "#find the most optimum n_estimator for XGBoost Regression\n",
    "\n",
    "  XGB_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, XGB_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_use = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_XGBR = xg.XGBRegressor( max_depth=max_depth_use, learning_rate = learning_rate_use, reg_alpha= reg_alpha_use,\n",
    "                                 n_estimators=find_optimum,reg_lambda= reg_lambda_use, random_state= random_use)\n",
    "# model_XGBR = xg.XGBRegressor(max_depth= max_depth_use, random_state=random_state_use)\n",
    "model_XGBR.fit(x_train,y_train)\n",
    "\n",
    "# use model to predict\n",
    "y_test_pred_XGBR = model_XGBR.predict(x_test)\n",
    "y_train_pred_XGBR = model_XGBR.predict(x_train)\n",
    "y_test_pred_XGBR = y_test_pred_XGBR.reshape(-1,1)\n",
    "y_pred_XGBR = model_XGBR.predict(xb).reshape(-1,1)\n",
    "print(model_XGBR.score(x_test, y_test))\n",
    "print(np.sqrt(mean_squared_error(y_test,y_test_pred_XGBR)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhdKDk2Ybiij",
    "outputId": "356db69c-94a1-4ac2-bb5d-d10e01432ba8"
   },
   "outputs": [],
   "source": [
    "# use model to predict\n",
    "y_test_pred_XGBR = model_XGBR.predict(x_test)\n",
    "y_train_pred_XGBR = model_XGBR.predict(x_train)\n",
    "y_test_pred_XGBR = y_test_pred_XGBR.reshape(-1,1)\n",
    "print(model_XGBR.score(x_test, y_test))\n",
    "print(np.sqrt(mean_squared_error(y_test,y_test_pred_XGBR)).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDbKIEGJbiik",
    "outputId": "bcf7f327-2df6-4b0c-eb4c-375210fbec0b"
   },
   "outputs": [],
   "source": [
    "r2_XGBR = r2_score(y_test,y_test_pred_XGBR).round(3)\n",
    "mse_XGBR =mean_squared_error(y_test,y_test_pred_XGBR).round(3)\n",
    "mae_XGBR =mean_absolute_error(y_test,y_test_pred_XGBR).round(3)\n",
    "rmse_XGBR =np.sqrt(mean_squared_error(y_test,y_test_pred_XGBR)).round(3)\n",
    "CV_value_XGBR = explained_variance_score(y_test,y_test_pred_XGBR).round(3)\n",
    "\n",
    "train_r2_XGBR = r2_score(y_train,y_train_pred_XGBR).round(3)\n",
    "train_mse_XGBR =mean_squared_error(y_train,y_train_pred_XGBR).round(3)\n",
    "train_mae_XGBR =mean_absolute_error(y_train,y_train_pred_XGBR).round(3)\n",
    "train_rmse_XGBR =np.sqrt(mean_squared_error(y_train,y_train_pred_XGBR)).round(3)\n",
    "train_CV_value_XGBR = explained_variance_score(y_train,y_train_pred_XGBR).round(3)\n",
    "\n",
    "print('X-Gradient Boost Performance')\n",
    "print('R2 : ', r2_XGBR)\n",
    "print('MSE : ', mse_XGBR)\n",
    "print('MAE : ', mae_XGBR)\n",
    "print('RMSE : ', rmse_XGBR)\n",
    "print('CV : ', CV_value_XGBR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1oF6eh1biik",
    "outputId": "ad7d3ffd-f8c7-409e-db95-8b6b2cd41eb3"
   },
   "outputs": [],
   "source": [
    "f_importance_XGBR = pd.DataFrame({'features':xb.columns,\n",
    "                                'feature_importances XGBoost':model_XGBR.feature_importances_}).sort_values(by = 'feature_importances XGBoost',ascending = False)\n",
    "f_importance_XGBR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IE-8mXhmXdb5"
   },
   "source": [
    "### Gradient Boost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9Z5SZCoYmr-",
    "outputId": "c2d3e826-1d77-42a2-880d-9a225e7bc411"
   },
   "outputs": [],
   "source": [
    "lr_list = [1,2,4,5,7,8,10,12,15,20,50,100]\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  GBR_optimum = GradientBoostingRegressor( n_estimators = find_optimum,validation_fraction = 0.176 )\n",
    "#find the most optimum n_estimator for GBRegression\n",
    "\n",
    "  GBR_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, GBR_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8a5Pu7xI12Cv"
   },
   "outputs": [],
   "source": [
    "n_estimators_use = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZwPgNTrZZHK",
    "outputId": "73273743-f393-45f9-d711-e7a6687b6139"
   },
   "outputs": [],
   "source": [
    "lr_list = [0.001,0.005,0.01,0.05,0.075,0.09,0.1,0.2,0.25,0.35,0.5,0.75,1]\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  GBR_optimum = GradientBoostingRegressor( n_estimators = n_estimators_use, learning_rate=find_optimum, validation_fraction = 0.176 )\n",
    "#find the most optimum learning_rate for GBRegression\n",
    "\n",
    "  GBR_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, GBR_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bxwxUMQ161J"
   },
   "outputs": [],
   "source": [
    "learning_rate_use = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1711377118156,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "tBw2nLL6XsGi",
    "outputId": "4f4a2fb0-3a64-4269-ff15-4fedd6f6bc7c"
   },
   "outputs": [],
   "source": [
    "model_GBR = GradientBoostingRegressor( n_estimators=n_estimators_use, learning_rate= learning_rate_use , validation_fraction = 0.176 )\n",
    "model_GBR.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFU1c9CYXgu9",
    "outputId": "0f75799b-d95b-4a1b-cf2e-77fa615893a3"
   },
   "outputs": [],
   "source": [
    "# use model to predict\n",
    "y_test_pred_GBR = model_GBR.predict(x_test)\n",
    "y_train_pred_GBR = model_GBR.predict(x_train).reshape(-1,1)\n",
    "\n",
    "y_test_pred_GBR = y_test_pred_GBR.reshape(-1,1)\n",
    "print(model_GBR.score(x_train, y_train))\n",
    "print(model_GBR.score(x_test, y_test))\n",
    "print(np.sqrt(mean_squared_error(y_test,y_test_pred_GBR)).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYp-myEramJ2",
    "outputId": "8d81f714-0575-4aaf-c811-4b00e49eb342"
   },
   "outputs": [],
   "source": [
    "r2_GBR = r2_score(y_test,y_test_pred_GBR).round(3)\n",
    "mse_GBR =mean_squared_error(y_test,y_test_pred_GBR).round(3)\n",
    "mae_GBR =mean_absolute_error(y_test,y_test_pred_GBR).round(3)\n",
    "rmse_GBR =np.sqrt(mean_squared_error(y_test,y_test_pred_GBR)).round(3)\n",
    "CV_value_GBR = explained_variance_score(y_test,y_test_pred_GBR).round(3)\n",
    "\n",
    "train_r2_GBR = r2_score(y_train,y_train_pred_GBR).round(3)\n",
    "train_mse_GBR =mean_squared_error(y_train,y_train_pred_GBR).round(3)\n",
    "train_mae_GBR =mean_absolute_error(y_train,y_train_pred_GBR).round(3)\n",
    "train_rmse_GBR =np.sqrt(mean_squared_error(y_train,y_train_pred_GBR)).round(3)\n",
    "train_CV_value_GBR = explained_variance_score(y_train,y_train_pred_GBR).round(3)\n",
    "\n",
    "print('Gradient Boost Regression Performance')\n",
    "print('R2 : ', r2_GBR)\n",
    "print('MSE : ', mse_GBR)\n",
    "print('MAE : ', mae_GBR)\n",
    "print('RMSE : ', rmse_GBR)\n",
    "print('CV : ', CV_value_GBR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8j3LL2aaF5I",
    "outputId": "ff9f24a4-c689-4400-e7fa-fd4478ac39dd"
   },
   "outputs": [],
   "source": [
    "f_importance_GBR = pd.DataFrame({'features':xb.columns,\n",
    "                                'feature_importances GBR':model_GBR.feature_importances_}).sort_values(by = 'feature_importances GBR',ascending = False)\n",
    "f_importance_GBR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbawg1VBBg9v"
   },
   "source": [
    "### Multi-layer Perceptron Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzPpSe1_xyLx",
    "outputId": "c1aabb82-422c-4514-c6fb-ea183900b14f"
   },
   "outputs": [],
   "source": [
    "# TRAIN TEST SPLIT _  LinearRegression\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x, yb, train_size = (train_percent/100), shuffle = True)\n",
    "\n",
    "test_reg = LinearRegression()\n",
    "test_reg.fit(x_train2, y_train2)\n",
    "\n",
    "print(\"Linear \", test_reg.score(x_train2, y_train2))\n",
    "print(\"Linear \", test_reg.score(x_test2, y_test2))\n",
    "\n",
    "test_reg = DecisionTreeRegressor()\n",
    "test_reg.fit(x_train2, y_train2)\n",
    "\n",
    "print(\"DTR   \", test_reg.score(x_train2, y_train2))\n",
    "print(\"DTR   \", test_reg.score(x_test2, y_test2))\n",
    "\n",
    "Test_reg = RandomForestRegressor()\n",
    "Test_reg.fit(x_train2, y_train2)\n",
    "\n",
    "print(\"RFR   \", Test_reg.score(x_train2, y_train2))\n",
    "print(\"RFR   \", Test_reg.score(x_test2, y_test2))\n",
    "\n",
    "Test_reg = XGBRegressor()\n",
    "Test_reg.fit(x_train2, y_train2)\n",
    "\n",
    "print(\"XGB   \", Test_reg.score(x_train2, y_train2))\n",
    "print(\"XGB   \", Test_reg.score(x_test2, y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZ66ICSRxfSb",
    "outputId": "23ff5398-ac0a-475d-e50b-be6bb2824f69"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "lr_list = [6,12,24,50,75,100,150,200,300,400,600,800,1200]\n",
    "lr_list2 = [6,12,24,50,75,100,200,300,400]\n",
    "for find_optimum1  in lr_list :\n",
    "  for find_optimum2 in lr_list2 :\n",
    "    MLP_optimum = MLPRegressor(hidden_layer_sizes=(find_optimum1, find_optimum2), max_iter=3000, random_state=2, validation_fraction=0.17)\n",
    "#find the most optimum number of neuron in first layer\n",
    "\n",
    "    MLP_optimum.fit(x_train, y_train)\n",
    "    ML_NN_optimization_process (find_optimum1, find_optimum2, MLP_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAvVpGKK2N_G"
   },
   "outputs": [],
   "source": [
    "first_layer_size_use = 800\n",
    "second_layer_size_use = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6Wh5vvE3rq9",
    "outputId": "6d5e1c05-b77a-4366-aa8d-066bece2ebed"
   },
   "outputs": [],
   "source": [
    "lr_list = [1,2,4,6,8,10,12,16,48]\n",
    "\n",
    "for find_optimum in lr_list:\n",
    "  MLP_optimum = MLPRegressor(hidden_layer_sizes=(first_layer_size_use, second_layer_size_use ), max_iter=3000, random_state= find_optimum)\n",
    "#find the most optimum number of neuron in first layer\n",
    "  MLP_optimum.fit(x_train, y_train)\n",
    "  ML_optimization_process (find_optimum, MLP_optimum, x_train, y_train, x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MA3a7Iky2SYu"
   },
   "outputs": [],
   "source": [
    "random_state_use = 2\n",
    "max_iter_use = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 3216,
     "status": "ok",
     "timestamp": 1711381033435,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "pz7HBVMqBg9w",
    "outputId": "b772b75e-d2db-4784-c124-61bb47b09fbf"
   },
   "outputs": [],
   "source": [
    "# Create MLPRegressor model with much neurons in the first layer, fewer neurons in the second layer, and single output layer\n",
    "model_MLPR = MLPRegressor(hidden_layer_sizes=(first_layer_size_use, second_layer_size_use), max_iter= max_iter_use , random_state= random_state_use )\n",
    "\n",
    "# Fit the model to the training data and validate with the validation data\n",
    "model_MLPR.fit(x_train, y_train)\n",
    "print(model_MLPR.score(x_train, y_train))\n",
    "print(model_MLPR.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1711381033968,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "oi15mjH8Bg9w",
    "outputId": "4635a3a5-bf3f-4d53-b3d1-a73e1bdabb44"
   },
   "outputs": [],
   "source": [
    "y_train_pred_MLPR = model_MLPR.predict(x_train)\n",
    "y_test_pred_MLPR = model_MLPR.predict(x_test)\n",
    "y_all_pred_MLPR = model_MLPR.predict(xb)\n",
    "\n",
    "r2_MLPR = r2_score(y_test,y_test_pred_MLPR).round(3)\n",
    "mse_MLPR = mean_squared_error(y_test,y_test_pred_MLPR).round(3)\n",
    "mae_MLPR = mean_absolute_error(y_test,y_test_pred_MLPR).round(3)\n",
    "rmse_MLPR = np.sqrt(mean_squared_error(y_test,y_test_pred_MLPR)).round(3)\n",
    "CV_value_MLPR = (mean_squared_error(y_test,y_test_pred_MLPR)**0.5/np.array(y_test).mean()).round(3)\n",
    "\n",
    "train_r2_MLPR = r2_score(y_train,y_train_pred_MLPR).round(3)\n",
    "train_mse_MLPR = mean_squared_error(y_train,y_train_pred_MLPR).round(3)\n",
    "train_mae_MLPR = mean_absolute_error(y_train,y_train_pred_MLPR).round(3)\n",
    "train_rmse_MLPR = np.sqrt(mean_squared_error(y_train,y_train_pred_MLPR)).round(3)\n",
    "train_CV_value_MLPR = (mean_squared_error(y_train,y_train_pred_MLPR)**0.5/np.array(y_test).mean()).round(3)\n",
    "\n",
    "all_r2_MLPR = r2_score(yb,y_all_pred_MLPR).round(3)\n",
    "all_mse_MLPR = mean_squared_error(yb,y_all_pred_MLPR).round(3)\n",
    "all_mae_MLPR = mean_absolute_error(yb,y_all_pred_MLPR).round(3)\n",
    "all_rmse_MLPR = np.sqrt(mean_squared_error(yb,y_all_pred_MLPR)).round(3)\n",
    "all_CV_value_MLPR = (mean_squared_error(yb,y_all_pred_MLPR)**0.5/np.array(yb).mean()).round(3)\n",
    "\n",
    "print('Multi-layer Perceptron Regression Performance')\n",
    "print('target : ', target_col )\n",
    "print('R2 : ', r2_MLPR)\n",
    "print('MSE : ', mse_MLPR)\n",
    "print('MAE : ', mae_MLPR)\n",
    "print('RMSE : ', rmse_MLPR)\n",
    "print('CV : ', CV_value_MLPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8lJXWcBxyLy"
   },
   "outputs": [],
   "source": [
    "# Extract coefs_ and intercepts_\n",
    "coefs_show = model_MLPR.coefs_\n",
    "intercepts_show = model_MLPR.intercepts_\n",
    "\n",
    "# Create DataFrame for coefficients\n",
    "coef_df_list = []\n",
    "for i, coef_matrix in enumerate(coefs_show):\n",
    "    layer_df = pd.DataFrame(coef_matrix, columns=[f'Layer_{i+1}_Neuron_{j+1}' for j in range(coef_matrix.shape[1])])\n",
    "    layer_df['Input_Variable'] = xb.columns if i == 0 else [f'Layer_{i}_Neuron_{j+1}' for j in range(coefs_show[i-1].shape[1])]\n",
    "    coef_df_list.append(layer_df.set_index('Input_Variable'))\n",
    "\n",
    "# Combine coefficients into a single DataFrame\n",
    "MLPR_coef_df = pd.concat(coef_df_list, axis=1)\n",
    "\n",
    "# Create DataFrame for intercepts\n",
    "intercept_df_list = []\n",
    "for i, intercept_vector in enumerate(intercepts_show):\n",
    "    intercept_df = pd.DataFrame(intercept_vector, columns=[f'Intercept_Layer_{i+1}'])\n",
    "    intercept_df['Neuron'] = [f'Layer_{i+1}_Neuron_{j+1}' for j in range(len(intercept_vector))]\n",
    "    intercept_df_list.append(intercept_df.set_index('Neuron'))\n",
    "\n",
    "# Combine intercepts into a single DataFrame\n",
    "MLPR_intercept_df = pd.concat(intercept_df_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrcBklnHxyLy"
   },
   "source": [
    "### Decision Tree Regression (Multitarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxScjvsKxyLy"
   },
   "outputs": [],
   "source": [
    "all_targets = target_col.copy()\n",
    "all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYA-scRBxyLy"
   },
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "\n",
    "# Splitting data into features and targets\n",
    "X = xb\n",
    "targets = data_input.loc[:, target_col]\n",
    "\n",
    "# Placeholder for results\n",
    "results = []\n",
    "y_train_pred_row = []\n",
    "y_test_pred_row = []\n",
    "y_train_row = []\n",
    "y_test_row = []\n",
    "\n",
    "all_targets\n",
    "\n",
    "# Training and evaluating for each target\n",
    "for target_col in targets.columns:\n",
    "    data_input = data_ML_R.sort_values(target_col)\n",
    "    X  = data_input.drop(columns=all_targets, axis=1).copy()\n",
    "    y   = data_input.loc[:, target_col]\n",
    "\n",
    "    # Oversample the training data\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = (train_percent/100), shuffle = True)\n",
    "\n",
    "    # Train the DecisionTreeClassifier\n",
    "    model =DecisionTreeRegressor(   )\n",
    "    param_grid = {\n",
    "        'max_depth'    : [1,2,3,4,5,8,10,13,21,34,42,55,89,100, 120,150,180,144,200,500,700],\n",
    "        'criterion'    : ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'splitter'     : ['best', 'random'],\n",
    "        'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "        'random_state' : [1,2,5,10,50,100,200]\n",
    "        }\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=6)\n",
    "    grid_search.fit(X_train, y_train )\n",
    "\n",
    "    clf = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train,y_train_pred))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test,y_test_pred))\n",
    "\n",
    "    # Save the model\n",
    "    model_filename = f\"DT_regression_{target_col}.joblib\"\n",
    "    dump(clf, directory_path + '/'+ folder_ML + '/' + folder_model + '_' + model_filename)\n",
    "\n",
    "    # Store results\n",
    "    feature_importances = clf.feature_importances_\n",
    "    result_row = [target_col] + list(feature_importances) + [r2_train, r2_test, rmse_train, rmse_test]\n",
    "    results.append(result_row)\n",
    "\n",
    "# Creating the results DataFrame\n",
    "columns = ['Feature_imp:'] + [f'{i}' for i in xb.columns] + ['r2_train', 'r2_test', 'rmse_train', 'rmse_test']\n",
    "results_table = pd.DataFrame(results, columns=columns)\n",
    "results_table.to_excel(directory_path + '/'+ folder_ML + '/' + folder_model + '_' + what_estimate + ' regression results.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMrpyv-4CAGY"
   },
   "source": [
    "## Regression ML : Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcMTovgfCAGY"
   },
   "outputs": [],
   "source": [
    "coef_Lin_reg = Lin_reg.coef_.flatten()\n",
    "coef_Ridge = Ridge_reg.coef_.flatten()\n",
    "coef_Lasso = Lasso_reg.coef_.flatten()\n",
    "# coef_SGR = model_SGR.coef_\n",
    "# coef_SVR = (model_SVR.coef_).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OzCapuxCAGY",
    "outputId": "54fe7089-8bd0-4abf-a43c-752e25db936e"
   },
   "outputs": [],
   "source": [
    "numpy_importances = [[coef_Lin_reg],[coef_Lasso],[coef_Ridge]    #,[coef_SVR]\n",
    "                     ]\n",
    "numpy_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CA4yfGgcxyLz"
   },
   "outputs": [],
   "source": [
    "#see result of prediction result by using Linear Regression\n",
    "plt.scatter(pd.DataFrame(y_test_pred_LinReg),pd.DataFrame(y_test).reset_index(level=0, drop=True), label = 'Prediction : Linear Reg' , linewidth= 0.9 )\n",
    "plt.scatter(pd.DataFrame(y_test_pred_Lasso),pd.DataFrame(y_test).reset_index(level=0, drop=True), label = 'Prediction : Lasso Reg' , linewidth= 0.9 )\n",
    "plt.legend()\n",
    "plt.title('Price Estimation using Regressions Model #1')\n",
    "plt.xlabel('estimation')\n",
    "plt.ylabel('target')\n",
    "plt.show(  )\n",
    "#plt.savefig(\"predict test 10.jpg\")\n",
    "\n",
    "#see result of prediction result by using Linear Regression\n",
    "# plt.scatter(pd.DataFrame(y_test_pred_Ridge),pd.DataFrame(y_test).reset_index(level=0, drop=True), label = 'Prediction : Ridge Reg', linewidth= 0.9 )\n",
    "# plt.scatter(pd.DataFrame(y_test_pred_SVR),pd.DataFrame(y_test).reset_index(level=0, drop=True), label = 'Prediction : Support Vector Reg' , linewidth= 0.9 )\n",
    "# plt.legend()\n",
    "# plt.title('Price Estimation using Regressions Model #2')\n",
    "# plt.xlabel('estimation')\n",
    "# plt.ylabel('target')\n",
    "# plt.show()\n",
    "#plt.savefig(\"predict test 10.jpg\")\n",
    "\n",
    "#see result of prediction result by using Linear Regression )\n",
    "plt.scatter(pd.DataFrame(y_test_pred_DTR),pd.DataFrame(y_test).reset_index(level=0, drop=True), label = 'Prediction : Decision Tree Reg' , linewidth= 0.9 )\n",
    "plt.scatter(pd.DataFrame(y_test_pred_RFR),pd.DataFrame(y_test).reset_index(level=0, drop=True), label = 'Prediction : Random Forest Reg' , linewidth= 0.9 )\n",
    "plt.legend()\n",
    "plt.title('Price Estimation using Regressions Model #3')\n",
    "plt.xlabel('estimation')\n",
    "plt.ylabel('target')\n",
    "plt.show()\n",
    "#plt.savefig(\"predict test 10.jpg\")\n",
    "\n",
    "#see result of prediction result by using Linear Regression )\n",
    "plt.scatter(pd.DataFrame(y_test_pred_ETR),pd.DataFrame(y_test).reset_index(level=0, drop=True), label = 'Prediction : Extra Trees Reg' , linewidth= 0.9 )\n",
    "plt.scatter(pd.DataFrame(y_test_pred_XGBR),pd.DataFrame(y_test).reset_index(level=0, drop=True), label = 'Prediction : XGboost Reg' , linewidth= 0.9 )\n",
    "plt.legend()\n",
    "plt.title('Price Estimation using Regressions Model #4')\n",
    "plt.xlabel('estimation')\n",
    "plt.ylabel('target')\n",
    "plt.show()\n",
    "#plt.savefig(\"predict test 10.jpg\")\n",
    "\n",
    "#see result of prediction result by using Linear Regression )\n",
    "plt.scatter(pd.DataFrame(y_test_pred_GBR),pd.DataFrame(y_test).reset_index(level=0, drop=True), label = 'Prediction : GradBoost Reg', linewidth= 0.9  )\n",
    "plt.scatter(pd.DataFrame(y_test_pred_MLPR),pd.DataFrame(y_test).reset_index(level=0, drop=True), label = 'Prediction : MLP Reg' , linewidth= 0.9 )\n",
    "plt.legend()\n",
    "plt.title('Price Estimation using Regressions Model #5')\n",
    "plt.xlabel('estimation')\n",
    "plt.ylabel('target')\n",
    "plt.show()\n",
    "#plt.savefig(\"predict test 10.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CO7EexQXxyL0"
   },
   "outputs": [],
   "source": [
    "plt.plot(pd.DataFrame(model_DTR.predict(x)), label = 'Prediction : Lasso Reg' , linewidth= 0.9 )\n",
    "plt.plot(pd.DataFrame(yb).reset_index(level=0, drop=True), label = 'Real Price', linewidth= 0.7 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwmvsB1lCAGY"
   },
   "outputs": [],
   "source": [
    "#see result of prediction result by using Linear Regression\n",
    "# plt.plot(pd.DataFrame(y_test_pred_LinReg), label = 'Prediction : Linear Reg' , linewidth= 0.9 )\n",
    "plt.plot(pd.DataFrame(y_test_pred_Lasso), label = 'Prediction : Lasso Reg' , linewidth= 0.9 )\n",
    "plt.plot(pd.DataFrame(y_test).reset_index(level=0, drop=True), label = 'Real Price', linewidth= 0.7 )\n",
    "plt.legend()\n",
    "plt.title('Price Estimation using Regressions Model #1')\n",
    "plt.xlabel('Index Data')\n",
    "plt.ylabel('price')\n",
    "plt.show(  )\n",
    "#plt.savefig(\"predict test 10.jpg\")\n",
    "\n",
    "#see result of prediction result by using Linear Regression\n",
    "plt.plot(pd.DataFrame(y_test_pred_Ridge), label = 'Prediction : Ridge Reg', linewidth= 0.9 )\n",
    "plt.plot(pd.DataFrame(y_test_pred_SVR), label = 'Prediction : Support Vector Reg' , linewidth= 0.9 )\n",
    "plt.plot(pd.DataFrame(y_test).reset_index(level=0, drop=True), label = 'Real Price', linewidth= 0.7 )\n",
    "plt.legend()\n",
    "plt.title('Price Estimation using Regressions Model #2')\n",
    "plt.xlabel('Index Data')\n",
    "plt.ylabel('price')\n",
    "plt.show()\n",
    "#plt.savefig(\"predict test 10.jpg\")\n",
    "\n",
    "#see result of prediction result by using Linear Regression )\n",
    "plt.plot(pd.DataFrame(y_test_pred_DTR), label = 'Prediction : Decision Tree Reg' , linewidth= 0.9 )\n",
    "plt.plot(pd.DataFrame(y_test_pred_RFR), label = 'Prediction : Random Forest Reg' , linewidth= 0.9 )\n",
    "plt.plot(pd.DataFrame(y_test).reset_index(level=0, drop=True), label = 'Real Price', linewidth= 0.7 )\n",
    "plt.legend()\n",
    "plt.title('Price Estimation using Regressions Model #3')\n",
    "plt.xlabel('Index Data')\n",
    "plt.ylabel('price')\n",
    "plt.show()\n",
    "#plt.savefig(\"predict test 10.jpg\")\n",
    "\n",
    "#see result of prediction result by using Linear Regression )\n",
    "plt.plot(pd.DataFrame(y_test_pred_ETR), label = 'Prediction : Extra Trees' , linewidth= 0.9 )\n",
    "plt.plot(pd.DataFrame(y_test_pred_XGBR), label = 'Prediction : XGboost Reg' , linewidth= 0.9 )\n",
    "plt.plot(pd.DataFrame(y_test).reset_index(level=0, drop=True), label = 'Real Price', linewidth= 0.7 )\n",
    "plt.legend()\n",
    "plt.title('Price Estimation using Regressions Model #4')\n",
    "plt.xlabel('Index Data')\n",
    "plt.ylabel('price')\n",
    "plt.show()\n",
    "#plt.savefig(\"predict test 10.jpg\")\n",
    "\n",
    "#see result of prediction result by using Linear Regression )\n",
    "plt.plot(pd.DataFrame(y_test_pred_GBR), label = 'Prediction : GradBoost Reg', linewidth= 0.9  )\n",
    "plt.plot(pd.DataFrame(y_test_pred_MLPR), label = 'Prediction : MLP Reg' , linewidth= 0.9 )\n",
    "plt.plot(pd.DataFrame(y_test).reset_index(level=0, drop=True), label = 'Real Price', linewidth= 0.7 )\n",
    "plt.legend()\n",
    "plt.title('Price Estimation using Regressions Model #5')\n",
    "plt.xlabel('Index Data')\n",
    "plt.ylabel('price')\n",
    "plt.show()\n",
    "#plt.savefig(\"predict test 10.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 201,
     "status": "ok",
     "timestamp": 1711381048734,
     "user": {
      "displayName": "Azimil Alam",
      "userId": "11823504813412490778"
     },
     "user_tz": -60
    },
    "id": "-8ixLSi3CAGZ",
    "outputId": "f81a5087-53d0-40e1-dbf7-b5a3c52ef6bd"
   },
   "outputs": [],
   "source": [
    "data_test = np.array([['Algorithm', 'R2', 'MSE', 'MAE', 'RMSE', 'CV Value'],\n",
    "                ['Linear Reg.', r2_LinReg, mse_LinReg, mae_LinReg, rmse_LinReg, CV_value_LinReg],\n",
    "                ['Lasso Reg.', r2_Lasso, mse_Lasso, mae_Lasso, rmse_Lasso, CV_value_Lasso],\n",
    "                ['Ridge Reg.', r2_Ridge, mse_Ridge, mae_Ridge, rmse_Ridge, CV_value_Ridge],\n",
    "                # ['Gauss.Pro. Reg.', r2_GPR, mse_GPR, mae_GPR, rmse_GPR, CV_value_GPR],\n",
    "                ['KNN Regression', r2_KNR, mse_KNR, mae_KNR, rmse_KNR, CV_value_KNR],\n",
    "                # ['Support Vector Reg.', r2_SVR, mse_SVR, mae_SVR, rmse_SVR, CV_value_SVR],\n",
    "                ['Decision Tree Reg.', r2_DTR, mse_DTR, mae_DTR, rmse_DTR, CV_value_DTR],\n",
    "                ['Extra Trees Reg.', r2_ETR, mse_ETR, mae_ETR, rmse_ETR, CV_value_ETR],\n",
    "                ['Random Forest Reg.', r2_RFR, mse_RFR, mae_RFR, rmse_RFR, CV_value_RFR],\n",
    "                # ['SGD Reg.', r2_SGR, mse_SGR, mae_SGR, rmse_SGR, CV_value_SGR],\n",
    "                ['XG-Boost Reg.', r2_XGBR, mse_XGBR, mae_XGBR, rmse_XGBR, CV_value_XGBR],\n",
    "                # ['GradBoost Reg.', r2_GBR, mse_GBR, mae_GBR, rmse_GBR, CV_value_GBR],\n",
    "                ['MLP Reg.', r2_MLPR, mse_MLPR, mae_MLPR, rmse_MLPR, CV_value_MLPR],\n",
    "                 ])\n",
    "\n",
    "table_regression_test = pd.DataFrame(data=data_test[1:, 1:],\n",
    "                     index = data_test[1:,0],\n",
    "                     columns=(data_test[0,1:])).sort_values('R2', ascending = False)\n",
    "table_regression_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7DpehL_xyL1",
    "outputId": "955e34ac-cc92-4de1-b8ba-4f017978fd9f"
   },
   "outputs": [],
   "source": [
    "data_train = np.array([['Algorithm', 'R2', 'MSE', 'MAE', 'RMSE', 'CV Value'],\n",
    "                ['Linear Reg.', train_r2_LinReg, train_mse_LinReg, train_mae_LinReg, train_rmse_LinReg, train_CV_value_LinReg],\n",
    "                ['Lasso Reg.', train_r2_Lasso, train_mse_Lasso, train_mae_Lasso, train_rmse_Lasso, train_CV_value_Lasso],\n",
    "                ['Ridge Reg.', train_r2_Ridge, train_mse_Ridge, train_mae_Ridge, train_rmse_Ridge, train_CV_value_Ridge],\n",
    "                # ['Gauss.Pro. Reg.', train_r2_GPR, train_mse_GPR, train_mae_GPR, train_rmse_GPR, train_CV_value_GPR],\n",
    "                ['KNN Regression', train_r2_KNR, train_mse_KNR, train_mae_KNR, train_rmse_KNR, train_CV_value_KNR],\n",
    "                # ['Support Vector Reg.', train_r2_SVR, train_mse_SVR, train_mae_SVR, train_rmse_SVR, train_CV_value_SVR],\n",
    "                ['Decision Tree Reg.', train_r2_DTR, train_mse_DTR, train_mae_DTR, train_rmse_DTR, train_CV_value_DTR],\n",
    "                ['Extra Trees Reg.', train_r2_ETR, train_mse_ETR, train_mae_ETR, train_rmse_ETR, train_CV_value_ETR],\n",
    "                ['Random Forest Reg.', train_r2_RFR, train_mse_RFR, train_mae_RFR, train_rmse_RFR, train_CV_value_RFR],\n",
    "                # ['SGD Reg.', train_r2_SGR, train_mse_SGR, train_mae_SGR, train_rmse_SGR, train_CV_value_SGR],\n",
    "                ['XG-Boost Reg.', train_r2_XGBR, train_mse_XGBR, train_mae_XGBR, train_rmse_XGBR, train_CV_value_XGBR],\n",
    "                # ['GradBoost Reg.', train_r2_GBR, train_mse_GBR, train_mae_GBR, train_rmse_GBR, train_CV_value_GBR],\n",
    "                ['MLP Reg.', train_r2_MLPR, train_mse_MLPR, train_mae_MLPR, train_rmse_MLPR, train_CV_value_MLPR],\n",
    "                 ])\n",
    "\n",
    "table_regression_train = pd.DataFrame(data=data_train[1:, 1:],\n",
    "                     index = data_train[1:,0],\n",
    "                     columns=(data_train[0,1:])).sort_values('R2', ascending = False)\n",
    "table_regression_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVdWhHeExyL1"
   },
   "outputs": [],
   "source": [
    "train_result = pd.DataFrame({\"y_train\" : y_train,\n",
    "                             \"Linear Reg.\":y_train_pred_LinReg.flatten(),\n",
    "                                # \"Lasso Reg.\" : y_train_pred_Lasso.flatten(),\n",
    "                                # \"Ridge Reg.\":y_train_pred_Ridge.flatten(),\n",
    "                                # \"SGD Reg.\":y_train_pred_SGR.flatten(),\n",
    "                                # \"SV Reg.\":y_train_pred_SVR.flatten(),\n",
    "                                \"Dec.Tree Reg.\" : y_train_pred_DTR.flatten(),\n",
    "                                \"RandForest Reg.\": y_train_pred_RFR.flatten(),\n",
    "                                \"Extra Trees Reg.\": y_train_pred_ETR.flatten(),\n",
    "                                # \"GradBoost Reg.\":y_train_pred_GBR.flatten(),\n",
    "                                \"XGBoost Reg.\":y_train_pred_XGBR.flatten(),\n",
    "                                # \"MLP Reg.\": y_train_pred_MLPR.flatten()\n",
    "                                }\n",
    "                               )\n",
    "\n",
    "test_result = pd.DataFrame({\"y_test\" : y_test,\n",
    "                            \"Linear Reg.\":y_test_pred_LinReg.flatten(),\n",
    "                            # \"Lasso Reg.\" : y_test_pred_Lasso.flatten(),\n",
    "                            # \"Ridge Reg.\":y_test_pred_Ridge.flatten(),\n",
    "                            # \"SGD Reg.\":y_test_pred_SGR.flatten(),\n",
    "                            # \"SV Reg.\":y_test_pred_SVR.flatten(),\n",
    "                            \"Dec.Tree Reg.\" : y_test_pred_DTR.flatten(),\n",
    "                            \"RandForest Reg.\": y_test_pred_RFR.flatten(),\n",
    "                            \"Extra Trees Reg.\": y_test_pred_ETR.flatten(),\n",
    "                            # \"GradBoost Reg.\":y_test_pred_GBR.flatten(),\n",
    "                            \"XGBoost Reg.\":y_test_pred_XGBR.flatten(),\n",
    "                            # \"MLP Reg.\": y_test_pred_MLPR.flatten()\n",
    "                                }\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZtgCcjwCAGZ",
    "outputId": "6e0fae80-d2f6-4cb4-e2c0-e2745f12a16c"
   },
   "outputs": [],
   "source": [
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "                                # \"Linear Reg.\":coef_Lin_reg,\n",
    "                                # \"Lasso Reg.\" : coef_Lasso,\n",
    "                                # \"Ridge Reg.\":coef_Ridge,\n",
    "                                # \"SGD Reg.\":coef_SGDR,\n",
    "                                # \"SV Reg.\":coef_SVR,\n",
    "                                \"Dec.Tree Reg.\" : model_DTR.feature_importances_,\n",
    "                                \"RandForest Reg.\": model_RFR.feature_importances_,\n",
    "                                \"Extra Trees Reg.\": model_RFR.feature_importances_,\n",
    "                                # \"GradBoost Reg.\":model_GBR.feature_importances_,\n",
    "                                \"XGBoost Reg.\":model_XGBR.feature_importances_}\n",
    "                               ).sort_values('Extra Trees Reg.', ascending = False)\n",
    "data_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSf9wsZ9xyL2"
   },
   "source": [
    "## ML Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1dNxULtxyL2"
   },
   "outputs": [],
   "source": [
    "def evaluate_models(models, model_names, X_data, y_data, for_what):\n",
    "    \"\"\"\n",
    "    Evaluate machine learning models and return a DataFrame with performance metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - models (list): List of trained model instances.\n",
    "    - model_names (list): List of names corresponding to the models.\n",
    "    - X_data (pd.DataFrame): Feature data for evaluation.\n",
    "    - y_data (pd.Series or np.array): True target values for evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing R-squared, MSE, MAE, and RMSE for each model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize lists to store results\n",
    "    r2_values = []\n",
    "    mse_values = []\n",
    "    mae_values = []\n",
    "    rmse_values = []\n",
    "    cv_values = []\n",
    "\n",
    "    # Calculate metrics for each model\n",
    "    for model in models:\n",
    "        y_pred = model.predict(X_data)\n",
    "\n",
    "        r2 = r2_score(y_data, y_pred).round(3)\n",
    "        mse = mean_squared_error(y_data, y_pred).round(3)\n",
    "        mae = mean_absolute_error(y_data, y_pred).round(3)\n",
    "        rmse = np.sqrt(mse).round(3)\n",
    "        cv_value = (rmse/(np.array(y_data).flatten().mean(axis=0))).round(3)\n",
    "\n",
    "        r2_values.append(r2)\n",
    "        mse_values.append(mse)\n",
    "        mae_values.append(mae)\n",
    "        rmse_values.append(rmse)\n",
    "        cv_values.append(cv_value)\n",
    "\n",
    "    # Create a DataFrame with the results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Model '+for_what : model_names,\n",
    "        'R2': r2_values,\n",
    "    #    'MSE': mse_values,\n",
    "     #   'MAE': mae_values,\n",
    "        'RMSE': rmse_values,\n",
    "        'CV' : cv_values\n",
    "    })\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "coLj2H-fxyL2"
   },
   "outputs": [],
   "source": [
    "def estimate_predictions(models, model_names, x_train, y_train,names):\n",
    "    # if len(models) != len(model_names):\n",
    "    #     raise ValueError(\"Length of models and model_names must be the same\")\n",
    "\n",
    "    # Initialize the result DataFrame with the actual y_train values\n",
    "    result_df = pd.DataFrame(np.array(y_train).flatten(), columns=['y_' + names])\n",
    "\n",
    "    # Add the predicted values for each model\n",
    "    for model, name in zip(models, model_names):\n",
    "        result_df[name] = model.predict(x_train).reshape(-1,1)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhCdf_X-xyL2"
   },
   "outputs": [],
   "source": [
    "# Function to compute R², RMSE, and MSE for given true and predicted values\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    cv_value = rmse/y_true.mean(axis=0)\n",
    "    return r2, rmse, cv_value\n",
    "\n",
    "def evaluate_predictions(df, y_true_name, condition_name ):\n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "\n",
    "    # Loop through each y_pred column and calculate the metrics\n",
    "    for col in df.columns:\n",
    "        if col != y_true_name:\n",
    "            r2, rmse, cv_value = calculate_metrics(df[y_true_name], df[col])\n",
    "            results.append({condition_name: col, 'R2': r2,'RMSE': rmse,  'CV': cv_value})\n",
    "\n",
    "    # Convert the results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfcLoVvQxyL2"
   },
   "outputs": [],
   "source": [
    "# EVALUATE MODELS AND ESTIMATE PREDICTIONS\n",
    "# Define your models list and model names list\n",
    "models = [Lin_reg, Lasso_reg, Ridge_reg, model_KNR, model_DTR, model_RFR, model_ETR, model_XGBR , model_GBR#, model_MLPR  , model_SVR\n",
    "          ]\n",
    "model_names = ['Linear.Reg', 'Lasso.Reg'  , 'Ridge.Reg' ,'K-Neighbors.Reg'  ,'Dec.Tree.Reg',\n",
    "               'Ran.For.Reg', 'Ext.Trees.Reg', 'XGBoost.Reg', 'GB.Reg' #, 'MLP.Reg'  , 'SV.Reg'\n",
    "               ]\n",
    "\n",
    "# Evaluate the models\n",
    "test_regression_result  = evaluate_models(models, model_names, x_test, y_test, 'test')\n",
    "train_regression_result = evaluate_models(models, model_names, x_train, y_train, 'train')\n",
    "all_regression_result   = evaluate_models(models, model_names, xb, yb, 'All')\n",
    "\n",
    "# Evaluate the models\n",
    "test_prediction_result  = estimate_predictions(models, model_names, x_test, y_test, 'test')\n",
    "train_prediction_result = estimate_predictions(models, model_names, x_train, y_train, 'train')\n",
    "all_prediction_result   = estimate_predictions(models, model_names, xb, yb, 'All')\n",
    "# all_prediction_result2   = data_ML[index_of_data_ML+target_col].sort_values(target_col).drop(target_col, axis=1).reset_index(drop=True).join(all_prediction_result).join(pd.DataFrame(y_all_pred_MLPR, columns=['MLP.Reg'])).sort_values(['room_id', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUpjgZW6xyL2",
    "outputId": "55b70709-8ffb-409b-dcf7-bbad1f13f05a"
   },
   "outputs": [],
   "source": [
    "all_regression_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dj9FNdwtxyL2",
    "outputId": "8c72295f-1e34-4ace-decf-1839e78cea8b"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=all_prediction_result, x='y_All', y='XGBoost.Reg' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzV_D3SrxyL2",
    "outputId": "2493a72b-c2cb-42d7-8f55-e815d9dee31e"
   },
   "outputs": [],
   "source": [
    "data_importance = pd.DataFrame({\"features\" : xb.columns,\n",
    "\n",
    "                                # \"SGD Reg.\":coef_SGDR,\n",
    "                                # \"SV Reg.\":coef_SVR,\n",
    "                                \"Dec.Tree Reg.\" : model_DTR.feature_importances_,\n",
    "                                \"RandForest Reg.\": model_RFR.feature_importances_,\n",
    "                                \"Extra Trees Reg.\": model_ETR.feature_importances_,\n",
    "                                \"GradBoost Reg.\":model_GBR.feature_importances_,\n",
    "                                \"XGBoost Reg.\":model_XGBR.feature_importances_,\n",
    "                                'KNeighbors Reg.' : KNN_importance['importances_mean'],\n",
    "                                \"Linear Reg.\":Lin_reg.coef_.flatten(),\n",
    "                                \"Lasso Reg.\" : Lasso_reg.coef_.flatten(),\n",
    "                                \"Ridge Reg.\": Ridge_reg.coef_.flatten(),\n",
    "                                 }).sort_values('XGBoost Reg.', ascending = False)\n",
    "data_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECBdsHiwxyL2"
   },
   "outputs": [],
   "source": [
    "#saving the model for the app\n",
    "joblib.dump(Lin_reg,     ML_directory + '_Lin_reg' + '.joblib')\n",
    "joblib.dump(Lasso_reg,     ML_directory +'_Lasso' + '.joblib')\n",
    "joblib.dump(Ridge_reg,     ML_directory + '_Ridge' + '.joblib')\n",
    "# joblib.dump(model_GPR,  ML_directory + '_model_GPC' + '.joblib')\n",
    "joblib.dump(model_KNR,  ML_directory +  '_model_KNR' + '.joblib')\n",
    "joblib.dump(model_ETR,  ML_directory +  '_model_ETR' + '.joblib')\n",
    "joblib.dump(model_DTR,  ML_directory +  '_model_DTR' + '.joblib')\n",
    "joblib.dump(model_RFR,  ML_directory +  '_model_RFR' + '.joblib')\n",
    "joblib.dump(model_GBR,  ML_directory +   '_model_GBR' + '.joblib')\n",
    "joblib.dump(model_XGBR, ML_directory +  '_model_XGBR' + '.joblib')\n",
    "\n",
    "joblib.dump(model_SVR,  ML_directory + '_model_SVR' + '.joblib')\n",
    "joblib.dump(model_MLPR, ML_directory +  '_model_MLPR' + '.joblib')\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(ML_directory+ '_perform data' + '.xlsx') as writer: # make one file with severel sheet\n",
    "\n",
    "    # use to_excel function and specify the sheet_name and index to store the dataframe in specified sheet\n",
    "    data_ML.reset_index(drop=True).to_excel(writer, sheet_name = 'main_dataset', index=True)\n",
    "    data_ML.describe(percentiles=[0.1,.25, .5, .75, .9]).to_excel(writer, sheet_name = 'origin_desc', index=True)\n",
    "    data_ML_R.reset_index(drop=True).to_excel(writer, sheet_name = 'data_traintest', index=True)\n",
    "    data_ML_R.describe(percentiles=[0.1,.25, .5, .75, .9]).to_excel(writer, sheet_name = 'traintest_desc', index=True)\n",
    "    pd.DataFrame([xb.columns],[[school_train],target_col]).to_excel(writer, sheet_name = 'output_input', index=True)\n",
    "    train_regression_result.to_excel(writer, sheet_name = 'ACC_train', index=True)\n",
    "    test_regression_result.to_excel(writer, sheet_name = 'Acc_test', index=True)\n",
    "    all_regression_result.to_excel(writer, sheet_name = 'ACC_all', index=True)\n",
    "\n",
    "    train_prediction_result.to_excel(writer, sheet_name = 'train_result', index=True)\n",
    "    test_prediction_result.to_excel(writer, sheet_name = 'test_result', index=True)\n",
    "    all_prediction_result.to_excel(writer, sheet_name = 'all_result', index=True)\n",
    "\n",
    "    data_importance.to_excel(writer, sheet_name = 'data_imp', index=True)      #dont forget the index , becaus coordinate -x and -y are the index of the table\n",
    "    pca_var_ratio.to_excel(writer, sheet_name = 'pca_variat', index=True)\n",
    "    pca_regression.to_excel(writer, sheet_name = 'pca_regress', index=True)\n",
    "\n",
    "    # MLPR_coef_df.to_excel(writer, sheet_name = 'MLPR_coef', index=True)\n",
    "    # MLPR_intercept_df.to_excel(writer, sheet_name = 'MLPR_intercept', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlBv-fUDQqcV"
   },
   "source": [
    "# ML Model - Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfrX0pyFxyL2"
   },
   "outputs": [],
   "source": [
    "todays_date = '_20240611'\n",
    "import joblib\n",
    "#saving the model for the app\n",
    "joblib.dump(Lin_reg,     ML_directory + todays_date + '_Lin_reg' + '.joblib')\n",
    "joblib.dump(Lasso,     ML_directory + todays_date + '_Lasso' + '.joblib')\n",
    "joblib.dump(Ridge,     ML_directory + todays_date + '_Ridge' + '.joblib')\n",
    "# joblib.dump(model_GPR,  ML_directory +todays_date +  '_model_GPC' + '.joblib')\n",
    "joblib.dump(model_KNR,  ML_directory + todays_date + '_model_KNR' + '.joblib')\n",
    "joblib.dump(model_SVR,  ML_directory + todays_date + '_model_SVR' + '.joblib')\n",
    "joblib.dump(model_ETR,  ML_directory + todays_date + '_model_ETR' + '.joblib')\n",
    "joblib.dump(model_DTR,  ML_directory + todays_date + '_model_DTR' + '.joblib')\n",
    "joblib.dump(model_RFR,  ML_directory +  todays_date + '_model_RFR' + '.joblib')\n",
    "joblib.dump(model_GBR,  ML_directory +  todays_date + '_model_GBR' + '.joblib')\n",
    "joblib.dump(model_MLPR, ML_directory +  todays_date + '_model_MLPR' + '.joblib')\n",
    "joblib.dump(model_XGBR, ML_directory + todays_date + '_model_XGBR' + '.joblib')\n",
    "\n",
    "with pd.ExcelWriter(ML_directory+ '_perform data' +  todays_date +'.xlsx') as writer: # make one file with severel sheet\n",
    "\n",
    "    # use to_excel function and specify the sheet_name and index to store the dataframe in specified sheet\n",
    "    data_ML.to_excel(writer, sheet_name = 'main_dataset', index=True)\n",
    "    data_ML.describe().to_excel(writer, sheet_name = 'df_description', index=True)\n",
    "\n",
    "    train_result.to_excel(writer, sheet_name = 'train_result', index=True)\n",
    "    test_result.to_excel(writer, sheet_name = 'test_result', index=True)\n",
    "\n",
    "    table_classification_performance.to_excel(writer, sheet_name = 'ML_perform', index=True)\n",
    "\n",
    "    data_importance.to_excel(writer, sheet_name = 'data_imp', index=True)      #dont forget the index , becaus coordinate -x and -y are the index of the table\n",
    "\n",
    "    MLPR_coef_df.to_excel(writer, sheet_name = 'MLPR_coef', index=True)\n",
    "    MLPR_intercept_df.to_excel(writer, sheet_name = 'MLPR_intercept', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsWB-NKJQqcV"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "#saving the model for the app\n",
    "joblib.dump(logreg,     ML_directory +todays_date + '_LogReg' + '.joblib')\n",
    "# joblib.dump(model_GPC,  ML_directory +todays_date + '_model_GPC' + '.joblib')\n",
    "# joblib.dump(model_KNN,  ML_directory +todays_date + '_model_KNN' + '.joblib')\n",
    "# joblib.dump(model_SVC,  ML_directory +todays_date + '_model_SVC' + '.joblib')\n",
    "joblib.dump(model_ETC,  ML_directory +todays_date + '_model_ETC' + '.joblib')\n",
    "joblib.dump(model_DTC,  ML_directory +todays_date + '_model_DTC' + '.joblib')\n",
    "joblib.dump(model_RFC,  ML_directory +todays_date +  '_model_RFC' + '.joblib')\n",
    "# joblib.dump(model_GBC,  ML_directory +todays_date +  '_model_GBC' + '.joblib')\n",
    "# joblib.dump(model_MLPC, ML_directory + todays_date + '_model_MLPC' + '.joblib')\n",
    "# joblib.dump(model_XGBC, ML_directory +todays_date + '_model_XGBC' + '.joblib')\n",
    "\n",
    "with pd.ExcelWriter(ML_directory+ '.xlsx') as writer: # make one file with severel sheet\n",
    "\n",
    "    # use to_excel function and specify the sheet_name and index\n",
    "    # to store the dataframe in specified sheet\n",
    "    train_result.to_excel(writer, sheet_name = 'train_result', index=True)\n",
    "    test_result.to_excel(writer, sheet_name = 'test_result', index=True)\n",
    "    data_ML_C.describe().to_excel(writer, sheet_name = 'df_description', index=True)\n",
    "    table_classification.to_excel(writer, sheet_name = 'ML_perform', index=True)\n",
    "    data_importance.to_excel(writer, sheet_name = 'data_imp', index=True)      #dont forget the index , becaus coordinate -x and -y are the index of the table\n",
    "    MLPC_coef_df.to_excel(writer, sheet_name = 'MLPR_coef', index=True)\n",
    "    MLPC_intercept_df.to_excel(writer, sheet_name = 'MLPR_intercept', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQSYfGebQqcV"
   },
   "outputs": [],
   "source": [
    "print(udah dulu ya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zalSVrgKxyL3",
    "outputId": "a5bab638-5867-4f94-eed8-73e54b88f5b1"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "#Loading back the model for the app#saving the model for the app\n",
    "joblib.dump(Lin_reg,     ML_directory + '_Lin_reg' + '.joblib')\n",
    "joblib.dump(Lasso_reg,     ML_directory +'_Lasso' + '.joblib')\n",
    "joblib.dump(Ridge_reg,     ML_directory + '_Ridge' + '.joblib')\n",
    "# joblib.dump(model_GPR,  ML_directory + '_model_GPC' + '.joblib')\n",
    "joblib.dump(model_KNR,  ML_directory +  '_model_KNR' + '.joblib')\n",
    "joblib.dump(model_ETR,  ML_directory +  '_model_ETR' + '.joblib')\n",
    "joblib.dump(model_DTR,  ML_directory +  '_model_DTR' + '.joblib')\n",
    "joblib.dump(model_RFR,  ML_directory +  '_model_RFR' + '.joblib')\n",
    "joblib.dump(model_GBR,  ML_directory +   '_model_GBR' + '.joblib')\n",
    "joblib.dump(model_XGBR, ML_directory +  '_model_XGBR' + '.joblib')\n",
    "\n",
    "joblib.dump(model_SVR,  ML_directory + '_model_SVR' + '.joblib')\n",
    "joblib.dump(model_MLPR, ML_directory +  '_model_MLPR' + '.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QbhHAkyQqcV"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "#Loading back the model for the app\n",
    "logreg      = joblib.load( directory_path + '/' + folder_ML + '/' + folder_model + target_col + '_LogReg' + '.joblib')\n",
    "model_DTC   = joblib.load( directory_path + '/' + folder_ML + '/' + folder_model + target_col + '_model_DTC' + '.joblib')\n",
    "RanForC     = joblib.load( directory_path + '/' + folder_ML + '/' + folder_model + target_col + '_model_RFC' + '.joblib')\n",
    "# model_XGBC  = joblib.load( directory_path + '/' + folder_ML + '/' + folder_model + target_col + '_model_XGBC' + '.joblib')\n",
    "# model_GBC   = joblib.load( directory_path + '/' + folder_ML + '/' + folder_model + target_col + '_model_GBC' + '.joblib')\n",
    "# model_MLPC  = joblib.load( directory_path + '/' + folder_ML + '/' + folder_model + target_col + '_model_MLPC' + '.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hf-cV23kQqcV"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#saving the model for the app\n",
    "# pickle.dump(logreg,    open( ML_directory + '_LogReg' + '.pkl', 'wb'))\n",
    "# # pickle.dump(model_GPC,  open( ML_directory + '_model_GPC' + '.pkl', 'wb'))\n",
    "# # pickle.dump(model_KNN,  open( ML_directory + '_model_KNN' + '.pkl', 'wb'))\n",
    "# pickle.dump(model_SVC,  open( ML_directory + '_model_SVC' + '.pkl', 'wb'))\n",
    "pickle.dump(model_DTC, open( ML_directory + '_model_DTC' + '_test.pkl', 'wb'))\n",
    "# pickle.dump(RanForC, open( ML_directory +  '_model_RFC' + '_test.pkl', 'wb'))\n",
    "# pickle.dump(model_GBC, open( ML_directory +  '_model_GBC' + '.pkl', 'wb'))\n",
    "# pickle.dump(model_MLPC, open(ML_directory +  '_model_MLPC' + '.pkl', 'wb'))\n",
    "# pickle.dump(model_XGBC, open(ML_directory + '_model_XGBC'  + '.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hob3QgqyxyL3"
   },
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJttPOz1xyL3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vciycjUtxyL3"
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFepZdQkxyL3",
    "outputId": "d1caabd7-e7db-44cc-f039-bedcbeeced6b"
   },
   "outputs": [],
   "source": [
    "\n",
    "X = combined2[combined2['school_id']=='bratteberg'][is_col_main].drop('feel_health', axis=1)\n",
    "y = combined2[combined2['school_id']=='bratteberg'] ['feel_health'].replace({-2:0, -1:1, 1:2, 2:3 }) # Target variable with categorical values (1 to 4)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "print(\"before oversampling train: \", Counter(y_train ))\n",
    "print(\"before oversampling test: \" , Counter(y_test ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEQmfiL-xyL3",
    "outputId": "ffcf2386-2866-474b-d415-213e8dc101fb"
   },
   "outputs": [],
   "source": [
    "\n",
    "oversample_over = SMOTE(sampling_strategy='all')\n",
    "# fit and apply the transform\n",
    "x_new, y_new = oversample_over.fit_resample(X,y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_new, y_new, train_size = (75/100), shuffle = True)\n",
    "print(\"After oversampling train: \", Counter(y_train ))\n",
    "print(\"After oversampling test: \" , Counter(y_test ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIHT6_cqxyL3",
    "outputId": "8daaf5e7-419d-4c11-cde6-4866e602e92c"
   },
   "outputs": [],
   "source": [
    "# Initialize and train the Random Forest Classifier\n",
    "rf_model = XGBClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "prec_score = precision_score(y_test, y_pred,average='weighted')\n",
    "print(f'precision_score: {prec_score:.2f}')\n",
    "\n",
    "recall_sc = recall_score(y_test, y_pred,average='weighted')\n",
    "print(f'recall_score: {recall_sc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnJOTvOGxyL3"
   },
   "outputs": [],
   "source": [
    "conf_matrix_ML( y_test, y_pred, rf_model, 'RF model' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfceHgsJxyL3"
   },
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qP80t8_xyL3",
    "outputId": "f76438af-98ea-4de9-b296-d395b8740298"
   },
   "outputs": [],
   "source": [
    "ML_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b10Z5XvXxyL3"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "#Loading back the model for the app#saving the model for the app\n",
    "Lin_reg = joblib.load(ML_directory + '_Lin_reg' + '.joblib')\n",
    "Lasso_reg=     joblib.load(ML_directory +'_Lasso' + '.joblib')\n",
    "Ridge_reg=     joblib.load(ML_directory + '_Ridge' + '.joblib')\n",
    "# model_GPR=  joblib.load(ML_directory + '_model_GPC' + '.joblib')\n",
    "model_KNR= joblib.load(ML_directory +  '_model_KNR' + '.joblib')\n",
    "model_ETR = joblib.load(ML_directory +  '_model_ETR' + '.joblib')\n",
    "model_DTR = joblib.load( ML_directory +  '_model_DTR' + '.joblib')\n",
    "model_RFR = joblib.load(  ML_directory +  '_model_RFR' + '.joblib')\n",
    "model_GBR = joblib.load( ML_directory +   '_model_GBR' + '.joblib')\n",
    "model_XGBR =joblib.load( ML_directory +  '_model_XGBR' + '.joblib')\n",
    "\n",
    "model_SVR = joblib.load(  ML_directory + '_model_SVR' + '.joblib')\n",
    "model_MLPR = joblib.load( ML_directory +  '_model_MLPR' + '.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_5pXihTxyL3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize and train the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(x_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0SupxRsxyL4",
    "outputId": "9fac510d-7589-4454-da92-21fb304534f4"
   },
   "outputs": [],
   "source": [
    "# Initialize the SHAP explainer\n",
    "explainer = shap.TreeExplainer(model_XGBR)\n",
    "\n",
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer.shap_values(xb.reset_index(drop=True))\n",
    "\n",
    "# Summary plot of SHAP values for all classes\n",
    "# shap_values[i] corresponds to the SHAP values for class i  IF MULTICLASS TARGET\n",
    "# for i in range(len(shap_values)):\n",
    "#     shap.summary_plot(shap_values[i], x_test, plot_type=\"dot\", show=False)\n",
    "#     plt.title(f'SHAP Summary Plot for Class {i}')\n",
    "#     plt.xlim(-2,2)\n",
    "#     plt.show()\n",
    "\n",
    "# # IF ONLY 2 CATEGORIES\n",
    "shap.summary_plot(shap_values, xb, plot_type=\"dot\", show=False, feature_names=xb.columns)\n",
    "plt.title(f'SHAP Summary Plot')\n",
    "plt.show()\n",
    "\n",
    "# Bar plot to show the mean absolute SHAP value for each feature across all predictions\n",
    "shap.summary_plot(shap_values, xb, plot_type=\"bar\", show=False, feature_names=xb.columns)\n",
    "plt.title('SHAP Feature Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3zfU8VNxyL4",
    "outputId": "854bdfbc-00a7-4183-b88a-85b1c3be45e2"
   },
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(Lin_reg, xb, feature_names=xb.columns)\n",
    "shap_values = explainer(xb)\n",
    "\n",
    "shap.summary_plot(shap_values, xb, plot_type=\"dot\", show=False, feature_names=xb.columns)\n",
    "plt.title(f'SHAP Summary Plot')\n",
    "plt.show()\n",
    "\n",
    "# Bar plot to show the mean absolute SHAP value for each feature across all predictions\n",
    "shap.summary_plot(shap_values, xb, plot_type=\"bar\", show=False, feature_names=xb.columns)\n",
    "plt.title('SHAP Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "shap.plots.scatter(shap_values)\n",
    "shap.plots.waterfall(shap_values[0], max_display=7)\n",
    "shap.plots.waterfall(shap_values[1], max_display=7)\n",
    "shap.plots.waterfall(shap_values[int(xb.count().mean()/4)], max_display=7)\n",
    "shap.plots.waterfall(shap_values[int(xb.count().mean()/2)], max_display=7)\n",
    "shap.plots.waterfall(shap_values[int(xb.count().mean()*3/4)], max_display=7)\n",
    "shap.plots.waterfall(shap_values[-1], max_display=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDZw91ngxyL4",
    "outputId": "c48ce657-5504-4504-f9fd-150a3f8ec057"
   },
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model_XGBR, xb, feature_names=xb.columns)\n",
    "shap_values = explainer(xb)\n",
    "\n",
    "shap.summary_plot(shap_values, xb, plot_type=\"dot\", show=False, feature_names=xb.columns)\n",
    "plt.title(f'SHAP Summary Plot')\n",
    "plt.show()\n",
    "\n",
    "# Bar plot to show the mean absolute SHAP value for each feature across all predictions\n",
    "shap.summary_plot(shap_values, xb, plot_type=\"bar\", show=False, feature_names=xb.columns)\n",
    "plt.title('SHAP Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "shap.plots.scatter(shap_values)\n",
    "shap.plots.waterfall(shap_values[0], max_display=7)\n",
    "shap.plots.waterfall(shap_values[1], max_display=7)\n",
    "shap.plots.waterfall(shap_values[int(xb.count().mean()/4)], max_display=7)\n",
    "shap.plots.waterfall(shap_values[int(xb.count().mean()/2)], max_display=7)\n",
    "shap.plots.waterfall(shap_values[int(xb.count().mean()*3/4)], max_display=7)\n",
    "shap.plots.waterfall(shap_values[-1], max_display=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91jgcTB1xyL4"
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer[1], shap_values[1[0,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXFif5MLxyL4",
    "outputId": "275e1015-c18e-4f48-9bbb-cec7616359d2"
   },
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:, \"rd_temp\"], shap_values[:, \"rd_air\"] , dot_size=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYsuWJ1mxyL4"
   },
   "source": [
    "# Learn PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCE9FEedxyL4",
    "outputId": "ebd9ef7f-4b6c-42cb-d2ca-fa42ff2996c4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "data = np.random.normal(loc=0, scale=1, size=(100, 3))\n",
    "data = np.vstack([data, [8, 0, 0], [0, 8, 0]])  # Add outliers\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "df = pd.DataFrame(data, columns=['Feature1', 'Feature2', 'Feature3'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqHk4L8kxyL4",
    "outputId": "2d805964-e253-4a70-99a5-9fd159c19590"
   },
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Reduce to two principal components\n",
    "principal_components = pca.fit_transform(data_scaled)\n",
    "pc_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "pc_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9sbokXNExyL4",
    "outputId": "da7e04e6-02de-49da-c580-0717b3a7d351"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Visualization of PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pc_df['PC1'], pc_df['PC2'], alpha=0.7, edgecolors='w')\n",
    "plt.title('PCA Plot')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3cQsB8WxyL4",
    "outputId": "c1f97355-4013-42e6-8af5-101ad484ce3e"
   },
   "outputs": [],
   "source": [
    "# Calculate the Euclidean distance from the origin\n",
    "pc_df['Distance'] = np.sqrt(pc_df['PC1']**2 + pc_df['PC2']**2)\n",
    "\n",
    "# Determine a threshold for outliers\n",
    "threshold = np.percentile(pc_df['Distance'], 90)  # 95th percentile as threshold\n",
    "\n",
    "# Identify outliers\n",
    "pc_df['Outlier'] = pc_df['Distance'] > threshold\n",
    "\n",
    "# Visualization with outliers marked\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = np.where(pc_df['Outlier'], 'r', 'b')\n",
    "plt.scatter(pc_df['PC1'], pc_df['PC2'], c=colors, alpha=0.7, edgecolors='w')\n",
    "plt.title('PCA Plot with Outliers')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjUVoZagxyL4",
    "outputId": "cc56527f-e2da-4de1-87cb-bb1c274408ea"
   },
   "outputs": [],
   "source": [
    "pc_df['Distance'].describe(percentiles=[0.1,0.25,0.5,0.75,0.9,0.95]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "5k8zB7YMWQH2",
    "MwekofXHQvdi",
    "qIvmuqzJQqb_",
    "suMwBIEAQqcA",
    "o6GqRa7_DCqI",
    "C3C6q0fJXWLf",
    "sXS_4OCSGrQ1",
    "MCU-7FfhhG2b",
    "2kJOr58KQqcS",
    "gMO3DtTaQqcS",
    "x61xYj5nGrRH",
    "cJ_Z3FGkGrRJ",
    "sW5ZDGcRxeBb",
    "5aBBlM2xQqcU",
    "WvPSzeliFA6u",
    "hVI1OJaku9_A",
    "cYVPpnuMu9_B",
    "4x01_u04u9_C",
    "44Splvsqu9_C",
    "XesRU3RHu9_D",
    "f_8py2sQu9_D",
    "Ykdt6hqau9_F",
    "jPjuvlWcu9_G",
    "CG1v5Hhiu9_H",
    "xt8z_famBg9s",
    "oFY8fVTxBg9t"
   ],
   "provenance": [
    {
     "file_id": "1DaVmzULJmgxLTMU-wEdHHu5LVD0Pg5JN",
     "timestamp": 1699215672297
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "DIGGMINSKOLE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
